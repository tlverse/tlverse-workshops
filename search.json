[{"path":"index.html","id":"welcome","chapter":"Welcome!","heading":"Welcome!","text":"open source, reproducible vignette half-day workshop \nTargeted Learning framework statistical causal inference machine\nlearning. Beyond introducing Targeted Learning, workshop focuses \napplying methodology practice using tlverse software\necosystem. materials based working\ndraft book Targeted Learning R: Causal Data Science \ntlverse Software Ecosystem, \nincludes -depth discussion topics much , may serve \nuseful reference accompany workshop materials.\n","code":""},{"path":"index.html","id":"important-links","chapter":"Welcome!","heading":"Important links","text":"Software installation: Please install relevant software \nworkshop using installation\nscript.Software installation: Please install relevant software \nworkshop using installation\nscript.probably exceed GitHub API rate limit installation,\nthrow error. issue solution addressed\n.probably exceed GitHub API rate limit installation,\nthrow error. issue solution addressed\n.Code: R script files section workshop available via\nGitHub repository workshop \nhttps://github.com/tlverse/tlverse-workshops/tree/master/R_codeCode: R script files section workshop available via\nGitHub repository workshop \nhttps://github.com/tlverse/tlverse-workshops/tree/master/R_code","code":""},{"path":"index.html","id":"about-this-workshop","chapter":"Welcome!","heading":"About this workshop","text":"workshop provide comprehensive introduction field \nTargeted Learning causal inference, corresponding tlverse software\necosystem. Emphasis placed targeted\nminimum loss-based estimators causal effects single timepoint\ninterventions, including extensions missing covariate outcome data.\nmultiply robust, efficient plug-estimators use state---art,\nensemble machine learning tools flexibly adjust confounding \nyielding valid statistical inference. particular, discuss targeted\nestimators causal effects static dynamic interventions; time\npermitting, additional topics discussed include estimation \ncausal effects optimal dynamic stochastic interventions.addition discussion, workshop incorporate interactive\nactivities hands-, guided R programming exercises, allow participants\nopportunity familiarize methodology tools \ntranslate real-world data analysis. highly recommended participants\nunderstanding basic statistical concepts confounding,\nprobability distributions, confidence intervals, hypothesis testing, \nregression. Advanced knowledge mathematical statistics useful \nnecessary. Familiarity R programming language essential.","code":""},{"path":"index.html","id":"outline","chapter":"Welcome!","heading":"Outline","text":"Warm-: Roadmap Targeted Learning Need Statistical\nRevolution\nintroductory video lecture Mark van der Laan Alan\nHubbard\n(Please watch hour-long lecture workshop.)09:00-09:30A: Introduction tlverse Software\nEcosystem WASH Benefits\ndata09:30-10:00A: Super learning sl3 R\npackage10:00-11:00A: Programming exercises sl311:00-11:15A: Morning Coffee Break Q&A11:15-12:00P: Targeted Learning causal inference tmle3 R\npackage12:00-12:45P: Programming exercises tmle312:45-01:30P: Lunch Break01:30-02:15P: Optimal treatment regimes tmle3mopttx R\npackage02:15-03:00P: Programming exercises tmle3mopttx03:00-03:15P: Afternoon Coffee Break03:15-04:00P: Stochastic treatment regimes tmle3shift R\npackage04:00-04:30P: Programming exercises tmle3shift04:30-05:00P: Concluding remarks discussion","code":""},{"path":"index.html","id":"about-the-instructors","chapter":"Welcome!","heading":"About the instructors","text":"","code":""},{"path":"index.html","id":"mark-van-der-laan","chapter":"Welcome!","heading":"Mark van der Laan","text":"Mark van der Laan, PhD, Professor Biostatistics Statistics UC\nBerkeley. research interests include statistical methods computational\nbiology, survival analysis, censored data, adaptive designs, targeted maximum\nlikelihood estimation, causal inference, data-adaptive loss-based learning, \nmultiple testing. research group developed loss-based super learning \nsemiparametric models, based cross-validation, generic optimal tool \nestimation infinite-dimensional parameters, nonparametric density\nestimation prediction censored uncensored data. Building \nwork, research group developed targeted maximum likelihood estimation\ntarget parameter data-generating distribution arbitrary\nsemiparametric nonparametric models, generic optimal methodology \nstatistical causal inference. recently, Mark’s group focused \npart development centralized, principled set software tools \ntargeted learning, tlverse.","code":""},{"path":"index.html","id":"alan-hubbard","chapter":"Welcome!","heading":"Alan Hubbard","text":"Alan Hubbard Professor Biostatistics, former head Division \nBiostatistics UC Berkeley, head data analytics core UC Berkeley’s\nSuperFund research program. current research interests include causal\ninference, variable importance analysis, statistical machine learning,\nestimation inference data-adaptive statistical target parameters, \ntargeted minimum loss-based estimation. Research group generally\nmotivated applications problems computational biology, epidemiology,\nprecision medicine.","code":""},{"path":"index.html","id":"jeremy-coyle","chapter":"Welcome!","heading":"Jeremy Coyle","text":"Jeremy Coyle, PhD, consulting data scientist statistical programmer,\ncurrently leading software development effort produced \ntlverse ecosystem R packages related software tools. Jeremy earned \nPhD Biostatistics UC Berkeley 2016, primarily supervision\nAlan Hubbard.","code":""},{"path":"index.html","id":"nima-hejazi","chapter":"Welcome!","heading":"Nima Hejazi","text":"Nima Hejazi PhD candidate biostatistics, working collaborative\ndirection Mark van der Laan Alan Hubbard. Nima affiliated UC\nBerkeley’s Center Computational Biology NIH Biomedical Big Data training\nprogram, well Fred Hutchinson Cancer Research Center. Previously,\nearned MA Biostatistics BA (majors Molecular Cell\nBiology, Psychology, Public Health), UC Berkeley. research\ninterests fall intersection causal inference machine learning,\ndrawing ideas non/semi-parametric estimation large, flexible\nstatistical models develop efficient robust statistical procedures \nevaluating complex target estimands observational randomized studies.\nParticular areas current emphasis include mediation/path analysis,\noutcome-dependent sampling designs, targeted loss-based estimation, vaccine\nefficacy trials. Nima also passionate statistical computing open\nsource software development applied statistics.","code":""},{"path":"index.html","id":"ivana-malenica","chapter":"Welcome!","heading":"Ivana Malenica","text":"Ivana Malenica PhD student biostatistics advised Mark van der Laan.\nIvana currently fellow Berkeley Institute Data Science, \nserving NIH Biomedical Big Data Freeport-McMoRan Genomic Engine fellow.\nearned Master’s Biostatistics Bachelor’s Mathematics, \nspent time Translational Genomics Research Institute. broadly,\nresearch interests span non/semi-parametric theory, probability theory,\nmachine learning, causal inference high-dimensional statistics. \ncurrent work involves complex dependent settings (dependence time \nnetwork) adaptive sequential designs.","code":""},{"path":"index.html","id":"rachael-phillips","chapter":"Welcome!","heading":"Rachael Phillips","text":"Rachael Phillips PhD student biostatistics, advised Alan Hubbard \nMark van der Laan. MA Biostatistics, BS Biology, BA \nMathematics. student targeted learning causal inference; research\nintegrates personalized medicine, human-computer interaction, experimental\ndesign, regulatory policy.","code":""},{"path":"index.html","id":"repro","chapter":"Welcome!","heading":"0.1 Reproduciblity with the tlverse","text":"tlverse software ecosystem growing collection packages, several \nquite early software lifecycle. team best \nmaintain backwards compatibility. work reaches completion, \nspecific versions tlverse packages used archived tagged \nproduce .book written using bookdown, complete\nsource available GitHub.\nversion book built R version 4.1.0 (2021-05-18),\npandoc version 2.7.3, \nfollowing packages:","code":""},{"path":"index.html","id":"setup","chapter":"Welcome!","heading":"0.2 Setup instructions","text":"","code":""},{"path":"index.html","id":"r-and-rstudio","chapter":"Welcome!","heading":"0.2.1 R and RStudio","text":"R RStudio separate downloads installations. R \nunderlying statistical computing environment. RStudio graphical integrated\ndevelopment environment (IDE) makes using R much easier \ninteractive. need install R install RStudio.","code":""},{"path":"index.html","id":"windows","chapter":"Welcome!","heading":"0.2.1.1 Windows","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed","chapter":"Welcome!","heading":"0.2.1.1.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears console indicates version R \nrunning. Alternatively, can type sessionInfo(), also display\nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install . \ncan check \ninformation remove old versions system \nwish .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed","chapter":"Welcome!","heading":"0.2.1.1.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Run .exe file just downloadedGo RStudio download pageUnder Installers select RStudio x.yy.zzz - Windows\nXP/Vista/7/8 (x, y, z represent version numbers)Double click file install itOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"macos-mac-os-x","chapter":"Welcome!","heading":"0.2.1.2 macOS / Mac OS X","text":"","code":""},{"path":"index.html","id":"if-you-already-have-r-and-rstudio-installed-1","chapter":"Welcome!","heading":"0.2.1.2.1 If you already have R and RStudio installed","text":"Open RStudio, click “Help” > “Check updates”. new version \navailable, quit RStudio, download latest version RStudio.check version R using, start RStudio first thing\nappears terminal indicates version R running.\nAlternatively, can type sessionInfo(), also display \nversion R running. Go CRAN\nwebsite check whether \nrecent version available. , please download install .","code":""},{"path":"index.html","id":"if-you-dont-have-r-and-rstudio-installed-1","chapter":"Welcome!","heading":"0.2.1.2.2 If you don’t have R and RStudio installed","text":"Download R \nCRAN website.Select .pkg file latest R versionDouble click downloaded file install RIt also good idea install XQuartz (needed\npackages)Go RStudio download\npageUnder Installers select RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)\n(x, y, z represent version numbers)Double click file install RStudioOnce ’s installed, open RStudio make sure works don’t get \nerror messages.","code":""},{"path":"index.html","id":"linux","chapter":"Welcome!","heading":"0.2.1.3 Linux","text":"Follow instructions distribution\nCRAN, provide information\nget recent version R common distributions. \ndistributions, use package manager (e.g., Debian/Ubuntu run\nsudo apt-get install r-base, Fedora sudo yum install R), \ndon’t recommend approach versions provided \nusually date. case, make sure least R 3.3.1.Go RStudio download\npageUnder Installers select version matches distribution, \ninstall preferred method (e.g., Debian/Ubuntu sudo dpkg -rstudio-x.yy.zzz-amd64.deb terminal).’s installed, open RStudio make sure works don’t get \nerror messages.setup instructions adapted written Data Carpentry: R\nData Analysis Visualization Ecological\nData.","code":""},{"path":"motivation.html","id":"motivation","chapter":"Motivation","heading":"Motivation","text":"“One enemy robust science humanity — appetite \nright, tendency find patterns noise, see supporting\nevidence already believe true, ignore facts \nfit.”— Nature Editorial (2015b)Scientific research unique point history. need improve rigor\nreproducibility field greater ever; corroboration moves\nscience forward, yet growing alarm results \nreproduced report false discoveries (Baker 2016). Consequences \nmeeting need result decline rate scientific\nprogression, reputation sciences, public’s trust \nfindings (Munafò et al. 2017; Nature Editorial 2015a).“key question want answer seeing results scientific\nstudy whether can trust data analysis.”— Peng (2015)Unfortunately, current state culture data analysis statistics\nactually enables human bias improper model selection. hypothesis\ntests estimators derived statistical models, obtain valid\nestimates inference critical statistical model contains \nprocess generated data. Perhaps treatment randomized \ndepended small number baseline covariates; knowledge \ncan incorporated model. Alternatively, maybe data \nobservational, knowledge data-generating process (DGP).\ncase, statistical model contain data\ndistributions. practice; however, models selected based knowledge\nDGP, instead models often selected based (1) p-values \nyield, (2) convenience implementation, /(3) analysts loyalty\nparticular model. practice “cargo-cult statistics — \nritualistic miming statistics rather conscientious practice,”\n(Stark Saltelli 2018) characterized arbitrary modeling choices, even though\nchoices often result different answers research question.\n, “increasingly often, [statistics] used instead aid \nabet weak science, role can perform well used mechanically \nritually,” opposed original purpose safeguarding weak\nscience (Stark Saltelli 2018). presents fundamental drive behind epidemic\nfalse findings scientific research suffering (van der Laan Starmans 2014).“suggest weak statistical understanding probably due \ninadequate”statistics lite\" education. approach build \nappropriate mathematical fundamentals provide scientifically\nrigorous introduction statistics. Hence, students’ knowledge may remain\nimprecise, patchy, prone serious misunderstandings. approach\nachieves, however, providing students false confidence able\nuse inferential tools whereas usually interpret p-value\nprovided black box statistical software. educational problem\nremains unaddressed, poor statistical practices prevail regardless \nprocedures measures may favored /banned editorials.\"— Szucs Ioannidis (2017)team University California, Berkeley, uniquely positioned \nprovide education. Spearheaded Professor Mark van der Laan, \nspreading rapidly many students colleagues greatly\nenriched field, aptly named “Targeted Learning” methodology targets \nscientific question hand counter current culture \n“convenience statistics” opens door biased estimation, misleading\nresults, false discoveries. Targeted Learning restores fundamentals \nformalized field statistics, facts statistical\nmodel represents real knowledge experiment generated data,\ntarget parameter represents seeking learn data \nfeature distribution generated (van der Laan Starmans 2014). way,\nTargeted Learning defines truth establishes principled standard \nestimation, thereby inhibiting --human biases (e.g., hindsight bias,\nconfirmation bias, outcome bias) infiltrating analysis.“key effective classical [statistical] inference \nwell-defined questions analysis plan tests questions.”— Nosek et al. (2018)objective provide training students, researchers, industry professionals, faculty science, public health, statistics, \nfields empower necessary knowledge skills utilize \nsound methodology Targeted Learning — technique provides tailored\npre-specified machines answering queries, data analysis \ncompletely reproducible, estimators efficient, minimally biased, \nprovide formal statistical inference.Just conscientious use modern statistical methodology necessary \nensure scientific practice thrives, remains critical acknowledge \nrole robust software plays allowing practitioners direct access \npublished results. recall “article…scientific publication \nscholarship , merely advertising scholarship. \nactual scholarship complete software development environment \ncomplete set instructions generated figures,” thus making \navailability adoption robust statistical software key enhancing \ntransparency inherent aspect science (Buckheit Donoho 1995).statistical methodology readily accessible practice, \ncrucial accompanied robust user-friendly software\n(Pullenayegum et al. 2016; Stromberg others 2004). tlverse software\necosystem developed fulfill need Targeted Learning\nmethodology. software facilitate computationally reproducible\nefficient analyses, also tool Targeted Learning education since\nworkflow mirrors methodology. particular, tlverse\nparadigm focus implementing specific estimator small set \nrelated estimators. Instead, focus exposing statistical framework\nTargeted Learning — R packages tlverse ecosystem\ndirectly model key objects defined mathematical theoretical\nframework Targeted Learning. ’s , tlverse R packages share \ncore set design principles centered extensibility, allowing \nused conjunction built upon one cohesive\nfashion.workshop, reader embark journey tlverse\necosystem. Guided R programming exercises, case studies, \nintuitive explanation readers build toolbox applying Targeted\nLearning statistical methodology, translate real-world causal\ninference analyses. Participants need fully trained statistician \nbegin understanding applying methods. However, highly\nrecommended participants understanding basic statistical\nconcepts confounding, probability distributions, confidence intervals,\nhypothesis tests, regression. Advanced knowledge mathematical statistics\nmay useful necessary. Familiarity R programming\nlanguage essential. also recommend understanding introductory\ncausal inference.introductory materials learning R programming language recommend following free resources:Software Carpentry’s Programming \nRSoftware Carpentry’s R Reproducible Scientific\nAnalysisGrolemund Wickham’s R Data\nScienceFor causal inference learning materials recommend following resources:Hernán MA, Robins JM (2019). Causal\nInference.Jason . Roy’s coursera Course Crash Course Causality: Inferring\nCausal Effects Observational Data","code":""},{"path":"tlverse.html","id":"tlverse","chapter":"1 Welcome to the tlverse","heading":"1 Welcome to the tlverse","text":"","code":""},{"path":"tlverse.html","id":"learning-objectives","chapter":"1 Welcome to the tlverse","heading":"1.1 Learning Objectives","text":"Understand tlverse ecosystem conceptuallyIdentify core components tlverseInstall tlverse R packagesUnderstand Targeted Learning roadmapLearn WASH Benefits example data","code":""},{"path":"tlverse.html","id":"what-is-the-tlverse","chapter":"1 Welcome to the tlverse","heading":"1.2 What is the tlverse?","text":"tlverse new framework Targeted Learning R, inspired \ntidyverse ecosystem R packages.analogy tidyverse:tidyverse opinionated collection R packages designed data\nscience. packages share underlying design philosophy, grammar, data\nstructures., tlverse isan opinionated collection R packages Targeted Learningsharing underlying philosophy, grammar, set data structures","code":""},{"path":"tlverse.html","id":"tlverse-components","chapter":"1 Welcome to the tlverse","heading":"1.3 tlverse components","text":"main packages represent core tlverse:sl3: Modern Super Learning Pipelines\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.\n? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.\n? modern object-oriented re-implementation Super Learner\nalgorithm, employing recently developed paradigms R programming.? design leverages modern tools fast computation, \nforward-looking, can form one cornerstones tlverse.tmle3: Engine Targeted Learning\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.\n? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.\n? generalized framework simplifies Targeted Learning \nidentifying implementing series common statistical estimation\nprocedures.? common interface engine accommodates current algorithmic\napproaches Targeted Learning still flexible enough remain \nengine even new techniques developed.addition engines drive development tlverse, \nsupporting packages – particular, two…origami: Generalized Framework \nCross-Validation\n? generalized framework flexible cross-validation\n? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.\n? generalized framework flexible cross-validationWhy? Cross-validation key part ensuring error estimates honest\npreventing overfitting. essential part Super\nLearner algorithm Targeted Learning.delayed: Parallelization Framework \nDependent Tasks\n? framework delayed computations (futures) based task\ndependencies.\n? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.\n? framework delayed computations (futures) based task\ndependencies.? Efficient allocation compute resources essential deploying\nlarge-scale, computationally intensive algorithms.key principle tlverse extensibility. , want support\nnew Targeted Learning estimators developed. model \nnew estimators implemented additional packages using core packages\n. currently two featured examples :tmle3mopttx: Optimal Treatments\ntlverse\n? Learn optimal rule estimate mean outcome rule\n? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.\n? Learn optimal rule estimate mean outcome ruleWhy? Optimal Treatment powerful tool precision healthcare \nsettings one-size-fits-treatment approach \nappropriate.tmle3shift: Shift Interventions \ntlverse\n? Shift interventions continuous treatments\n? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.\n? Shift interventions continuous treatmentsWhy? treatment variables discrete. able estimate \neffects continuous treatment represents powerful extension \nTargeted Learning approach.","code":""},{"path":"tlverse.html","id":"installtlverse","chapter":"1 Welcome to the tlverse","heading":"1.4 Installation","text":"tlverse ecosystem packages currently hosted \nhttps://github.com/tlverse, yet CRAN. \ncan use devtools package install :tlverse depends large number packages also hosted\nGitHub. , may see following error:just means R tried install many packages GitHub \nshort window. fix , need tell R use GitHub \nuser (’ll need GitHub user account). Follow two steps:Type usethis::browse_github_pat() R console, direct\nGitHub’s page create New Personal Access Token.Create Personal Access Token simply clicking “Generate token” \nbottom page.Copy Personal Access Token, long string lowercase letters \nnumbers.Type usethis::edit_r_environ() R console, open \n.Renviron file source window RStudio. able \naccess .Renviron file command, try inputting\nSys.setenv(GITHUB_PAT = ) Personal Access Token inserted \nstring equals symbol; error, skip \nstep 8..Renviron file, type GITHUB_PAT= paste Personal\nAccess Token equals symbol space..Renviron file, press enter key ensure .Renviron\nends newline.Save .Renviron file.Restart R changes take effect. can restart R via drop-\nmenu “Session” tab. “Session” tab top RStudio\ninterface.following steps, able successfully install \npackage threw error .","code":"\ninstall.packages(\"devtools\")\ndevtools::install_github(\"tlverse/tlverse\")Error: HTTP error 403.\n  API rate limit exceeded for 71.204.135.82. (But here's the good news:\n  Authenticated requests get a higher rate limit. Check out the documentation\n  for more details.)\n\n  Rate limit remaining: 0/60\n  Rate limit reset at: 2019-03-04 19:39:05 UTC\n\n  To increase your GitHub API rate limit\n  - Use `usethis::browse_github_pat()` to create a Personal Access Token.\n  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`."},{"path":"intro.html","id":"intro","chapter":"2 The Roadmap for Targeted Learning","heading":"2 The Roadmap for Targeted Learning","text":"","code":""},{"path":"intro.html","id":"learning-objectives-1","chapter":"2 The Roadmap for Targeted Learning","heading":"2.1 Learning Objectives","text":"end chapter able :Translate scientific questions statistical questions.Define statistical model based knowledge experiment \ngenerated data.Identify causal parameter function observed data distribution.Explain following causal statistical assumptions \nimplications: ..d., consistency, interference, positivity, SUTVA.","code":""},{"path":"intro.html","id":"introduction","chapter":"2 The Roadmap for Targeted Learning","heading":"2.2 Introduction","text":"roadmap statistical learning concerned translation \nreal-world data applications mathematical statistical formulation \nrelevant estimation problem. involves data random variable \nprobability distribution, scientific knowledge represented statistical\nmodel, statistical target parameter representing answer question \ninterest, notion estimator sampling distribution \nestimator.","code":""},{"path":"intro.html","id":"the-roadmap","chapter":"2 The Roadmap for Targeted Learning","heading":"2.3 The Roadmap","text":"Following roadmap process five stages.Data random variable probability distribution, \\(O \\sim P_0\\).statistical model \\(\\mathcal{M}\\) \\(P_0 \\\\mathcal{M}\\).statistical target parameter \\(\\Psi\\) estimand \\(\\Psi(P_0)\\).estimator \\(\\hat{\\Psi}\\) estimate \\(\\hat{\\Psi}(P_n)\\).measure uncertainty estimate \\(\\hat{\\Psi}(P_n)\\).","code":""},{"path":"intro.html","id":"data-as-a-random-variable-with-a-probability-distribution-o-sim-p_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(1) Data as a random variable with a probability distribution, \\(O \\sim P_0\\)","text":"data set ’re confronted result experiment can\nview data random variable, \\(O\\), repeat experiment\ndifferent realization experiment. particular, \nrepeat experiment many times learn probability distribution,\n\\(P_0\\), data. , observed data \\(O\\) probability distribution\n\\(P_0\\) \\(n\\) independent identically distributed (..d.) observations \nrandom variable \\(O; O_1, \\ldots, O_n\\). Note data ..d.,\nways handle non-..d. data, establishing conditional\nindependence, stratifying data create sets identically distributed data,\netc. crucial researchers absolutely clear actually\nknow data-generating distribution given problem interest.\nUnfortunately, communication statisticians researchers often\nfraught misinterpretation. roadmap provides mechanism \nensure clear communication research statistician – truly helps\ncommunication!","code":""},{"path":"intro.html","id":"the-empirical-probability-measure-p_n","chapter":"2 The Roadmap for Targeted Learning","heading":"The empirical probability measure, \\(P_n\\)","text":"\\(n\\) ..d. observations empirical probability\nmeasure, \\(P_n\\). empirical probability measure approximation \ntrue probability measure \\(P_0\\), allowing us learn data. \nexample, can define empirical probability measure set, \\(\\), \nproportion observations end \\(\\). ,\n\\[\\begin{equation*}\n  P_n() = \\frac{1}{n}\\sum_{=1}^{n} \\mathbb{}(O_i \\)\n\\end{equation*}\\]order start learning something, need ask “know \nprobability distribution data?” brings us Step 2.","code":""},{"path":"intro.html","id":"the-statistical-model-mathcalm-such-that-p_0-in-mathcalm","chapter":"2 The Roadmap for Targeted Learning","heading":"(2) The statistical model \\(\\mathcal{M}\\) such that \\(P_0 \\in \\mathcal{M}\\)","text":"statistical model \\(\\mathcal{M}\\) defined question asked \nend . defined set possible probability\ndistributions observed data. Often \\(\\mathcal{M}\\) large (possibly\ninfinite-dimensional), reflect fact statistical knowledge \nlimited. case \\(\\mathcal{M}\\) infinite-dimensional, deem \nnonparametric statistical model.Alternatively, probability distribution data hand described\nfinite number parameters, statistical model parametric. \ncase, prescribe belief random variable \\(O\\) \nobserved , e.g., normal distribution mean \\(\\mu\\) variance\n\\(\\sigma^2\\). formally, parametric model may defined\n\\[\\begin{equation*}\n  \\mathcal{M} = \\{P_{\\theta} : \\theta \\\\mathcal{R}^d \\}\n\\end{equation*}\\]Sadly, assumption data-generating distribution specific,\nparametric forms --common, even leap faith. \npractice oversimplification current culture data analysis typically\nderails attempt trying answer scientific question hand; alas,\nstatements ever-popular quip Box “models wrong \nuseful,” encourage data analyst make arbitrary choices even \noften force significant differences answers estimation\nproblem. Targeted Learning paradigm suffer bias since \ndefines statistical model representation true\ndata-generating distribution corresponding observed data.Now, Step 3: ``trying learn data?\"","code":""},{"path":"intro.html","id":"the-statistical-target-parameter-psi-and-estimand-psip_0","chapter":"2 The Roadmap for Targeted Learning","heading":"(3) The statistical target parameter \\(\\Psi\\) and estimand \\(\\Psi(P_0)\\)","text":"statistical target parameter, \\(\\Psi\\), defined mapping \nstatistical model, \\(\\mathcal{M}\\), parameter space (.e., real number)\n\\(\\mathcal{R}\\). , \\(\\Psi: \\mathcal{M}\\rightarrow\\mathbb{R}\\). target\nparameter may seen representation \nquantity wish learn data, answer well-specified\n(often causal) question interest. contrast purely statistical target\nparameters, causal target parameters require identification observed\ndata, based causal models include several untestable assumptions,\ndescribed detail section causal target parameters.simple example, consider data set contains observations \nsurvival time every subject, question interest “’s\nprobability someone lives longer five years?” ,\n\\[\\begin{equation*}\n  \\Psi(P_0) = \\mathbb{P}(O > 5)\n\\end{equation*}\\]answer question estimand, \\(\\Psi(P_0)\\), \nquantity ’re trying learn data. defined \\(O\\),\n\\(\\mathcal{M}\\) \\(\\Psi(P_0)\\) formally defined statistical\nestimation problem.","code":""},{"path":"intro.html","id":"the-estimator-hatpsi-and-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(4) The estimator \\(\\hat{\\Psi}\\) and estimate \\(\\hat{\\Psi}(P_n)\\)","text":"obtain good approximation estimand, need estimator, \npriori-specified algorithm defined mapping set possible\nempirical distributions, \\(P_n\\), live non-parametric statistical\nmodel, \\(\\mathcal{M}_{NP}\\) (\\(P_n \\\\mathcal{M}_{NP}\\)), parameter space\nparameter interest. , \\(\\hat{\\Psi} : \\mathcal{M}_{NP} \\rightarrow \\mathbb{R}^d\\). estimator function takes input\nobserved data, realization \\(P_n\\), gives output value \nparameter space, estimate, \\(\\hat{\\Psi}(P_n)\\).estimator may seen operator maps observed data \ncorresponding empirical distribution value parameter space, \nnumerical output produced function estimate. Thus, \nelement parameter space based empirical probability distribution\nobserved data. plug realization \\(P_n\\) (based sample\nsize \\(n\\) random variable \\(O\\)), get back estimate \\(\\hat{\\Psi}(P_n)\\)\ntrue parameter value \\(\\Psi(P_0)\\).order quantify uncertainty estimate target parameter\n(.e., construct statistical inference), understanding sampling\ndistribution estimator necessary. brings us Step 5.","code":""},{"path":"intro.html","id":"a-measure-of-uncertainty-for-the-estimate-hatpsip_n","chapter":"2 The Roadmap for Targeted Learning","heading":"(5) A measure of uncertainty for the estimate \\(\\hat{\\Psi}(P_n)\\)","text":"Since estimator \\(\\hat{\\Psi}\\) function empirical\ndistribution \\(P_n\\), estimator random variable sampling\ndistribution. , repeat experiment drawing \\(n\\) observations \nevery time end different realization estimate \nestimator sampling distribution. sampling distribution estimator\ncan theoretically validated approximately normally distributed \nCentral Limit Theorem (CLT).class Central Limit Theorems (CLTs) statements regarding \nconvergence sampling distribution estimator normal\ndistribution. general, construct estimators whose limit sampling\ndistributions may shown approximately normal distributed sample size\nincreases. large enough \\(n\\) ,\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\sim N \\left(\\Psi(P_0), \\frac{\\sigma^2}{n}\\right),\n\\end{equation*}\\]\npermitting statistical inference. Now, can proceed quantify \nuncertainty chosen estimator construction hypothesis tests \nconfidence intervals. example, may construct confidence interval \nlevel \\((1 - \\alpha)\\) estimand, \\(\\Psi(P_0)\\):\n\\[\\begin{equation*}\n  \\hat{\\Psi}(P_n) \\pm z_{1 - \\frac{\\alpha}{2}}\n    \\left(\\frac{\\sigma}{\\sqrt{n}}\\right),\n\\end{equation*}\\]\n\\(z_{1 - \\frac{\\alpha}{2}}\\) \\((1 - \\frac{\\alpha}{2})^\\text{th}\\)\nquantile standard normal distribution. Often, interested \nconstructing 95% confidence intervals, corresponding mass \\(\\alpha = 0.05\\) \neither tail limit distribution; thus, typically take\n\\(z_{1 - \\frac{\\alpha}{2}} \\approx 1.96\\).Note: typically estimate standard error,\n\\(\\frac{\\sigma}{\\sqrt{n}}\\).95% confidence interval means take 100 different samples\nsize \\(n\\) compute 95% confidence interval sample \napproximately 95 100 confidence intervals contain estimand,\n\\(\\Psi(P_0)\\). practically, means 95% probability\n(95% confidence) confidence interval procedure contain \ntrue estimand. However, single estimated confidence interval either \ncontain true estimand .","code":""},{"path":"intro.html","id":"summary-of-the-roadmap","chapter":"2 The Roadmap for Targeted Learning","heading":"2.4 Summary of the Roadmap","text":"Data, \\(O\\), viewed random variable probability distribution.\noften \\(n\\) units independent identically distributed units \nprobability distribution \\(P_0\\) (\\(O_1, \\ldots, O_n \\sim P_0\\)). \nstatistical knowledge experiment generated data. \nwords, make statement true data distribution \\(P_0\\) falls \ncertain set called statistical model, \\(\\mathcal{M}\\). Often sets \nlarge statistical knowledge limited statistical models\noften infinite dimensional models. statistical query , “\ntrying learn data?” denoted statistical target parameter,\n\\(\\Psi\\), maps \\(P_0\\) estimand, \\(\\Psi(P_0)\\). point \nstatistical estimation problem formally defined now need\nstatistical theory guide us construction estimators. ’s lot\nstatistical theory review course , particular, relies\nCentral Limit Theorem, allowing us come estimators \napproximately normally distributed also allowing us come statistical\ninference (.e., confidence intervals hypothesis tests).","code":""},{"path":"intro.html","id":"causal","chapter":"2 The Roadmap for Targeted Learning","heading":"2.5 Causal Target Parameters","text":"","code":""},{"path":"intro.html","id":"the-causal-model","chapter":"2 The Roadmap for Targeted Learning","heading":"2.5.1 The Causal Model","text":"formalizing data statistical model, can define causal\nmodel express causal parameters interest. Directed acyclic graphs (DAGs)\none useful tool express know causal relations among\nvariables. Ignoring exogenous \\(U\\) terms (explained ), assume \nfollowing ordering variables observed data \\(O\\).directed acyclic graphs (DAGs) like provide convenient means \nvisualize causal relations variables, causal\nrelations among variables can represented via set structural equations,\ndefine non-parametric structural equation model (NPSEM):\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= f_A(W, U_A) \\\\\n  Y &= f_Y(W, , U_Y),\n\\end{align*}\\]\n\\(U_W\\), \\(U_A\\), \\(U_Y\\) represent unmeasured exogenous background\ncharacteristics influence value variable. NPSEM, \\(f_W\\),\n\\(f_A\\) \\(f_Y\\) denote variable (\\(W\\), \\(\\) \\(Y\\), respectively)\nfunction parents unmeasured background characteristics, note\nimposition particular functional constraints(e.g.,\nlinear, logit-linear, one interaction, etc.). reason, \ncalled non-parametric structural equation models (NPSEMs). \nDAG set nonparametric structural equations represent exactly \ninformation may used interchangeably.first hypothetical experiment consider assigning exposure \nwhole population observing outcome, assigning exposure \nwhole population observing outcome. nonparametric structural\nequations, corresponds comparison outcome distribution \npopulation two interventions:\\(\\) set \\(1\\) individuals, \\(\\) set \\(0\\) individuals.interventions imply two new nonparametric structural equation models. \ncase \\(= 1\\), \n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 1 \\\\\n  Y(1) &= f_Y(W, 1, U_Y),\n\\end{align*}\\]\ncase \\(=0\\),\n\\[\\begin{align*}\n  W &= f_W(U_W) \\\\\n  &= 0 \\\\\n  Y(0) &= f_Y(W, 0, U_Y).\n\\end{align*}\\]equations, \\(\\) longer function \\(W\\) \nintervened system, setting \\(\\) deterministically either values\n\\(1\\) \\(0\\). new symbols \\(Y(1)\\) \\(Y(0)\\) indicate outcome variable \npopulation generated respective NPSEMs ; \noften called counterfactuals (since run contrary--fact). difference\nmeans outcome two interventions defines \nparameter often called “average treatment effect” (ATE), denoted\n\\[\\begin{equation}\\label{eqn:ate}\n  ATE = \\mathbb{E}_X(Y(1)-Y(0)),\n\\end{equation}\\]\n\\(\\mathbb{E}_X\\) mean theoretical (unobserved) full data\n\\(X = (W, Y(1), Y(0))\\).Note, can define much complicated interventions NPSEM’s, \ninterventions based upon rules (based upon covariates), stochastic\nrules, etc. results different targeted parameter entails\ndifferent identifiability assumptions discussed .","code":""},{"path":"intro.html","id":"identifiability","chapter":"2 The Roadmap for Targeted Learning","heading":"2.5.2 Identifiability","text":"can never observe \\(Y(0)\\) (counterfactual outcome \\(=0\\))\n\\(Y(1)\\) (similarly, counterfactual outcome \\(=1\\)), \nestimate  directly. Instead, make assumptions \nquantity may estimated observed data \\(O \\sim P_0\\) \ndata-generating distribution \\(P_0\\). Fortunately, given causal model\nspecified NPSEM , can, handful untestable assumptions,\nestimate ATE, even observational data. assumptions may \nsummarized followsThe causal graph implies \\(Y() \\perp \\) \\(\\\\mathcal{}\\), \nrandomization assumption. case observational data, \nanalogous assumption strong ignorability unmeasured confounding\n\\(Y() \\perp \\mid W\\) \\(\\\\mathcal{}\\);Although represented causal graph, also required assumption\ninterference units, , outcome unit \\(\\) \\(Y_i\\) \naffected exposure unit \\(j\\) \\(A_j\\) unless \\(=j\\);Consistency treatment mechanism also required, .e., outcome\nunit \\(\\) \\(Y_i()\\) whenever \\(A_i = \\), assumption also known “\nversions treatment”;also necessary observed units, across strata defined \\(W\\),\nbounded (non-deterministic) probability receiving treatment –\n, \\(0 < \\mathbb{P}(= \\mid W) < 1\\) \\(\\) \\(W\\)). assumption\nreferred positivity overlap.Remark: Together, (2) (3), assumptions interference \nconsistency, respectively, jointly referred stable unit\ntreatment value assumption (SUTVA).Given assumptions, ATE may re-written function \\(P_0\\),\nspecifically\n\\[\\begin{equation}\\label{eqn:estimand}\n  ATE = \\mathbb{E}_0(Y(1) - Y(0)) = \\mathbb{E}_0\n    \\left(\\mathbb{E}_0[Y \\mid = 1, W] - \\mathbb{E}_0[Y \\mid = 0, W]\\right),\n\\end{equation}\\]\ndifference predicted outcome values subject, \ncontrast treatment conditions (\\(= 0\\) vs. \\(= 1\\)), population,\naveraged observations. Thus, parameter theoretical “full” data\ndistribution can represented estimand observed data\ndistribution. Significantly, nothing representation \n requires parameteric assumptions; thus, regressions\nright hand side may estimated freely machine learning. \ndifferent parameters, potentially different identifiability\nassumptions resulting estimands can functions different components\n\\(P_0\\).","code":""},{"path":"intro.html","id":"the-wash-benefits-example-dataset","chapter":"2 The Roadmap for Targeted Learning","heading":"2.6 The WASH Benefits Example Dataset","text":"data come study effect water quality, sanitation, hand\nwashing, nutritional interventions child development rural Bangladesh\n(WASH Benefits Bangladesh): cluster-randomised controlled trial\n(Luby et al. 2018). study enrolled pregnant women first second\ntrimester rural villages Gazipur, Kishoreganj, Mymensingh, \nTangail districts central Bangladesh, average eight women per\ncluster. Groups eight geographically adjacent clusters block-randomised,\nusing random number generator, six intervention groups (\nreceived weekly visits community health promoter first 6 months\nevery 2 weeks next 18 months) double-sized control group (\nintervention health promoter visit). six intervention groups :chlorinated drinking water;improved sanitation;hand-washing soap;combined water, sanitation, hand washing;improved nutrition counseling provision lipid-based nutrient\nsupplements; andcombined water, sanitation, handwashing, nutrition.workshop, concentrate child growth (size age) outcome \ninterest. reference, trial registered ClinicalTrials.gov \nNCT01590095.purposes workshop, start treating data independent\nidentically distributed (..d.) random draws large target\npopulation. , available options, account clustering \ndata (within sampled geographic units), , simplification, avoid \ndetails workshop presentations, although modifications \nmethodology biased samples, repeated measures, etc., available.28 variables measured, 1 variable set outcome \ninterest. outcome, \\(Y\\), weight--height Z-score (whz dat);\ntreatment interest, \\(\\), randomized treatment group (tr \ndat); adjustment set, \\(W\\), consists simply everything else. \nresults observed data structure \\(n\\) ..d. copies \\(O_i = (W_i, A_i, Y_i)\\), \\(= 1, \\ldots, n\\).Using skimr package, can\nquickly summarize variables measured WASH Benefits data set:(#tab:skim_washb_data)Data summaryVariable type: characterVariable type: numericA convenient summary relevant variables given just , complete\nsmall visualization describing marginal characteristics \ncovariate. Note asset variables reflect socio-economic status \nstudy participants. Notice also uniform distribution treatment groups\n(twice many controls); , course, design.","code":"\nlibrary(tidyverse)\n\n# read in data\ndat <- read_csv(\"https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv\")\ndat\n#> # A tibble: 4,695 x 28\n#>     whz tr     fracode month  aged sex    momage momedu momheight hfiacat  Nlt18\n#>   <dbl> <chr>  <chr>   <dbl> <dbl> <chr>   <dbl> <chr>      <dbl> <chr>    <dbl>\n#> 1  0    Contr… N05265      9   268 male       30 Prima…      146. Food Se…     3\n#> 2 -1.16 Contr… N05265      9   286 male       25 Prima…      149. Moderat…     2\n#> 3 -1.05 Contr… N08002      9   264 male       25 Prima…      152. Food Se…     1\n#> 4 -1.26 Contr… N08002      9   252 female     28 Prima…      140. Food Se…     3\n#> 5 -0.59 Contr… N06531      9   336 female     19 Secon…      151. Food Se…     2\n#> # … with 4,690 more rows, and 17 more variables: Ncomp <dbl>, watmin <dbl>,\n#> #   elec <dbl>, floor <dbl>, walls <dbl>, roof <dbl>, asset_wardrobe <dbl>,\n#> #   asset_table <dbl>, asset_chair <dbl>, asset_khat <dbl>, asset_chouki <dbl>,\n#> #   asset_tv <dbl>, asset_refrig <dbl>, asset_bike <dbl>, asset_moto <dbl>,\n#> #   asset_sewmach <dbl>, asset_mobile <dbl>\nlibrary(skimr)\nskim(dat)"},{"path":"sl3.html","id":"sl3","chapter":"3 Super (Machine) Learning","heading":"3 Super (Machine) Learning","text":"Ivana Malenica Rachael PhillipsBased sl3 R package Jeremy\nCoyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, Oleg Sofrygin.Updated: 2021-05-20","code":""},{"path":"sl3.html","id":"learning-objectives-2","chapter":"3 Super (Machine) Learning","heading":"Learning Objectives","text":"end chapter able :Select loss function appropriate functional parameter \nestimated.Assemble ensemble learners based properties identify \nfeatures support.Customize learner hyperparameters incorporate diversity different\nsettings.Select subset available covariates pass variables \nmodeling algorithm.Fit ensemble nested cross-validation obtain estimate \nperformance ensemble .Obtain sl3 variable importance metrics.Interpret discrete continuous Super Learner fits.Rationalize need remove bias Super Learner make \noptimal bias–variance tradeoff parameter interest.","code":""},{"path":"sl3.html","id":"motivation-1","chapter":"3 Super (Machine) Learning","heading":"Motivation","text":"common task data analysis prediction — using observed data \nlearn function, can used map new input variables \npredicted outcome.\ndata, algorithms can model complex function necessary \nadequately model data. data, main terms regression model might\nfit data quite well.Super Learner (SL), ensemble learner, solves issue, allowing \ncombination algorithms simplest (intercept-) complex\n(neural nets, random forests, SVM, etc).works using cross-validation manner guarantees \nresulting fit good possible, given learners provided.","code":""},{"path":"sl3.html","id":"introduction-1","chapter":"3 Super (Machine) Learning","heading":"Introduction","text":"Chapter 1, introduced Roadmap Targeted\nLearning general template translate real-world data\napplications formal statistical estimation problems. first steps \nroadmap define statistical estimation problem, establishData random variable, equivalently, realization \nparticular experiment/study.statistical model set possible probability distributions \ngiven rise observed data. know, example, \nobservations independent identically distributed.translation scientific question, often causal, \ntarget estimand.Note estimand causal, step 3 also requires establishing\nidentifiability estimand observed data, possible\nnon-testable assumptions may necessarily reasonable. Still, \ntarget quantity valid statistical interpretation. See causal target\nparameters detail causal models identifiability.Now defined statistical estimation problem, ready \nconstruct TMLE; asymptotically linear efficient substitution\nestimator estimand. first step estimation procedure\ninitial estimate data-generating distribution, relevant part\ndistribution needed evaluate target parameter. \ninitial estimation, use Super Learner (SL) (van der Laan, Polley, Hubbard 2007).SL provides important step creating robust estimator. loss-\nfunction- based tool uses cross-validation obtain best prediction \ntarget parameter, based weighted average library machine\nlearning algorithms. library machine learning algorithms consists \nfunctions (“learners” sl3 nomenclature) think might \nconsistent true data-generating distribution. “consistent \ntrue data-generating distribution”, mean algorithms selected \nviolate subject-matter knowledge experiment generated \ndata. Also, library contain diversity algorithms range \nparametric regression models multi-step algorithms involving screening\ncovariates, penalizations, optimizing tuning parameters, etc.ensembling collection algorithms weights (“metalearning” \nsl3 nomenclature) shown adaptive robust, even small\nsamples (Polley van der Laan 2010). SL proven perform asymptotically well\nbest possible prediction algorithm library (van der Laan Dudoit 2003; Van der Vaart, Dudoit, Laan 2006).","code":""},{"path":"sl3.html","id":"why-use-the-super-learner","chapter":"3 Super (Machine) Learning","heading":"Why use the Super Learner?","text":"prediction, one can use cross-validated risk empirically determine\nrelative performance SL competing methods.tested different algorithms actual\ndata looked performance (e.g., MSE prediction), never one\nalgorithm always win (see ).shows results study, comparing fits several\ndifferent learners, including SL algorithms.SL performs asymptotically well best possible weighted combination.including competitors library candidate estimators (GLMs,\nneural nets, SVMs, random forest, etc.), SL asymptotically outperform\ncompetitors — even set competitors allowed grow\npolynomial sample size.Motivates name “Super Learner”: provides system combining many\nestimators improved estimator.","code":""},{"path":"sl3.html","id":"general-overview-of-the-algorithm","chapter":"3 Super (Machine) Learning","heading":"General Overview of the Algorithm","text":"","code":""},{"path":"sl3.html","id":"what-is-cross-validation-and-how-does-it-work","chapter":"3 Super (Machine) Learning","heading":"3.0.0.1 What is cross-validation and how does it work?","text":"many different cross-validation schemes, designed \naccommodate different study designs, data structures, prediction\nproblems.figure shows example \\(V\\)-fold cross-validation \\(V=10\\)\nfolds.","code":""},{"path":"sl3.html","id":"general-step-by-step-overview-of-the-super-learner-algorithm","chapter":"3 Super (Machine) Learning","heading":"3.0.0.2 General step-by-step overview of the Super Learner algorithm","text":"Break sample evenly V-folds (say V=10).10 folds, remove portion sample (kept \nvalidation sample) remaining used fit learners (training\nsample).Fit learner training sample (note, learners \ninternal cross-validation procedure methods select tuning\nparameters).observation corresponding validation sample, predict \noutcome using learners, \\(p\\) learners, \n\\(p\\) predictions.Take another validation sample repeat V-sets data\nremoved.Compare cross-validated fit learners across observations based\nspecified loss function (e.g., squared error, negative log-likelihood, etc)\ncalculating corresponding average loss (risk).Either:\nchoose learner smallest risk apply learner entire data\nset (resulting SL fit),\nweighted average learners minimize cross-validated risk\n(construct ensemble learners), \nre-fitting learners original data set, \nuse weights get SL fit.\n\nchoose learner smallest risk apply learner entire data\nset (resulting SL fit),weighted average learners minimize cross-validated risk\n(construct ensemble learners), \nre-fitting learners original data set, \nuse weights get SL fit.\nre-fitting learners original data set, anduse weights get SL fit.Note, entire procedure can cross-validated get consistent\nestimate future performance SL fit.","code":""},{"path":"sl3.html","id":"how-to-pick-a-library-of-candidate-learners","chapter":"3 Super (Machine) Learning","heading":"3.0.0.3 How to pick a library of candidate learners?","text":"library simply collection algorithms.algorithms library come contextual knowledge \nlarge set “default” algorithms.algorithms may range simple linear regression model multi-step\nalgorithms involving screening covariates, penalizations, optimizing tuning\nparameters, etc.","code":""},{"path":"sl3.html","id":"summary-of-super-learners-foundations","chapter":"3 Super (Machine) Learning","heading":"Summary of Super Learner’s Foundations","text":"use loss function \\(L\\) assign measure performance \nlearner \\(\\psi\\) applied data \\(O\\), subsequently compare\nperformance across learners. generally, \\(L\\) maps every\n\\(\\psi \\\\mathbb{R}\\) \\(L(\\psi) : (O) \\mapsto L(\\psi)(O)\\). use terms\n“learner”, “algorithm”, “estimator” interchangeably.\nimportant recall \\(\\psi\\) estimator \\(\\psi_0\\), \nunknown true parameter value \\(P_0\\).\nvalid loss function expectation (risk) minimized \ntrue value parameter \\(\\psi_0\\). Thus, minimizing expected\nloss bring estimator \\(\\psi\\) closer true \\(\\psi_0\\).\nexample, say observe learning data set \\(O_i=(Y_i,X_i)\\), \n\\(=1, \\ldots, n\\) independent identically distributed observations,\n\\(Y_i\\) continuous outcome interest, \\(X_i\\) set \ncovariates. Also, let objective estimate function\n\\(\\psi_0: X \\mapsto \\psi_0(X) = \\mathbb{E}_0(Y \\mid X)\\), \nconditional mean outcome given covariates. function can expressed\nminimizer expected squared error loss:\n\\(\\psi_0 = \\text{argmin}_{\\psi} \\mathbb{E}[L(O,\\psi(X))]\\), \n\\(L(O,\\psi(X)) = (Y − \\psi(X))^2\\).\ncan estimate loss substituting empirical distribution \ndata \\(P_n\\) true unknown distribution observed data\n\\(P_0\\).\nAlso, can use cross-validated risk empirically determine \nrelative performance estimator (.e., candidate learner), \nperhaps ’s performance compares estimators.\ntested different estimators actual data looked \nperformance (e.g., MSE predictions across learners), can see\nalgorithm (weighted combination) lowest risk, thus\nclosest true \\(\\psi_0\\).\nuse loss function \\(L\\) assign measure performance \nlearner \\(\\psi\\) applied data \\(O\\), subsequently compare\nperformance across learners. generally, \\(L\\) maps every\n\\(\\psi \\\\mathbb{R}\\) \\(L(\\psi) : (O) \\mapsto L(\\psi)(O)\\). use terms\n“learner”, “algorithm”, “estimator” interchangeably.important recall \\(\\psi\\) estimator \\(\\psi_0\\), \nunknown true parameter value \\(P_0\\).valid loss function expectation (risk) minimized \ntrue value parameter \\(\\psi_0\\). Thus, minimizing expected\nloss bring estimator \\(\\psi\\) closer true \\(\\psi_0\\).example, say observe learning data set \\(O_i=(Y_i,X_i)\\), \n\\(=1, \\ldots, n\\) independent identically distributed observations,\n\\(Y_i\\) continuous outcome interest, \\(X_i\\) set \ncovariates. Also, let objective estimate function\n\\(\\psi_0: X \\mapsto \\psi_0(X) = \\mathbb{E}_0(Y \\mid X)\\), \nconditional mean outcome given covariates. function can expressed\nminimizer expected squared error loss:\n\\(\\psi_0 = \\text{argmin}_{\\psi} \\mathbb{E}[L(O,\\psi(X))]\\), \n\\(L(O,\\psi(X)) = (Y − \\psi(X))^2\\).can estimate loss substituting empirical distribution \ndata \\(P_n\\) true unknown distribution observed data\n\\(P_0\\).Also, can use cross-validated risk empirically determine \nrelative performance estimator (.e., candidate learner), \nperhaps ’s performance compares estimators.tested different estimators actual data looked \nperformance (e.g., MSE predictions across learners), can see\nalgorithm (weighted combination) lowest risk, thus\nclosest true \\(\\psi_0\\).cross-validated empirical risk algorithm defined \nempirical mean validation sample loss algorithm fitted\ntraining sample, averaged across splits data.cross-validated empirical risk algorithm defined \nempirical mean validation sample loss algorithm fitted\ntraining sample, averaged across splits data.discrete Super Learner, cross-validation selector, algorithm\nlibrary minimizes cross-validated empirical risk.discrete Super Learner, cross-validation selector, algorithm\nlibrary minimizes cross-validated empirical risk.continuous/ensemble Super Learner, often referred Super Learner\nweighted average library algorithms, weights \nchosen minimize cross-validated empirical risk library.\nRestricting weights positive sum one (.e., convex\ncombination) shown improve upon discrete Super Learner\n(Polley van der Laan 2010; van der Laan, Polley, Hubbard 2007). notion weighted combinations \nintroduced Wolpert (1992) neural networks adapted \nregressions Breiman (1996).continuous/ensemble Super Learner, often referred Super Learner\nweighted average library algorithms, weights \nchosen minimize cross-validated empirical risk library.\nRestricting weights positive sum one (.e., convex\ncombination) shown improve upon discrete Super Learner\n(Polley van der Laan 2010; van der Laan, Polley, Hubbard 2007). notion weighted combinations \nintroduced Wolpert (1992) neural networks adapted \nregressions Breiman (1996).Cross-validation proven optimal selection among estimators. \nresult established oracle inequality cross-validation\nselector among collection candidate estimators (van der Laan Dudoit 2003; Van der Vaart, Dudoit, Laan 2006). condition loss function uniformly\nbounded, guaranteed sl3.Cross-validation proven optimal selection among estimators. \nresult established oracle inequality cross-validation\nselector among collection candidate estimators (van der Laan Dudoit 2003; Van der Vaart, Dudoit, Laan 2006). condition loss function uniformly\nbounded, guaranteed sl3.","code":""},{"path":"sl3.html","id":"sl3-microwave-dinner-implementation","chapter":"3 Super (Machine) Learning","heading":"sl3 “Microwave Dinner” Implementation","text":"begin illustrating core functionality SL algorithm \nimplemented sl3.sl3 implementation consists following steps:Load necessary libraries dataDefine machine learning taskMake SL creating library base learners metalearnerTrain SL machine learning taskObtain predicted values","code":""},{"path":"sl3.html","id":"wash-benefits-study-example","chapter":"3 Super (Machine) Learning","heading":"WASH Benefits Study Example","text":"Using WASH Benefits Bangladesh data, interested predicting\nweight--height z-score whz using available covariate data. \ninformation dataset, data work , \ndescribed chapter tlverse\nhandbook. Let’s begin!","code":""},{"path":"sl3.html","id":"load-the-necessary-libraries-and-data","chapter":"3 Super (Machine) Learning","heading":"0. Load the necessary libraries and data","text":"First, load relevant R packages, set seed, load data.","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(SuperLearner)\nlibrary(origami)\nlibrary(sl3)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# load data set and take a peek\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)\nhead(washb_data) %>%\n  kable() %>%\n  kableExtra:::kable_styling(fixed_thead = T) %>%\n  scroll_box(width = \"100%\", height = \"300px\")"},{"path":"sl3.html","id":"define-the-machine-learning-task","chapter":"3 Super (Machine) Learning","heading":"1. Define the machine learning task","text":"define machine learning “task” (predict weight--height Z-score\nwhz using available covariate data), need create sl3_Task\nobject.sl3_Task keeps track roles variables play machine\nlearning problem, data, metadata (e.g., observational-level\nweights, IDs, offset).Also, missing outcomes, need set drop_missing_outcome = TRUE create task. next analysis, IST stroke trial\ndata, missing outcome. following chapter, need \nestimate “missingness mechanism”; conditional probably \noutcome observed, given history (.e., variables measured\nmissingness). Estimating missingness mechanism requires learning\nprediction function outputs predicted probability unit\nmissing, given history.warning important. task just imputed missing covariates us.\nSpecifically, covariate column missing values, sl3 uses \nmedian impute missing continuous covariates, mode impute binary\ncategorical covariates.Also, covariate column missing values, sl3 adds additional\ncolumn indicating whether value imputed, particularly\nhandy missingness data might informative.Also, notice specify number folds, loss function\ntask. default cross-validation scheme V-fold, \\(V=10\\) number\nfolds.Let’s visualize washb_task:can’t see print task, default cross-validation fold\nstructure (\\(V\\)-fold cross-validation \\(V\\)=10 folds) created \ndefined task.R6 Tip: type washb_task$ press “tab” button (\nneed press “tab” twice ’re RStudio), can view \nactive public fields methods can accessed washb_task\nobject.","code":"\n# specify the outcome and covariates\noutcome <- \"whz\"\ncovars <- colnames(washb_data)[-which(names(washb_data) == outcome)]\n\n# create the sl3 task\nwashb_task <- make_sl3_Task(\n  data = washb_data,\n  covariates = covars,\n  outcome = outcome\n)\n#> Warning in process_data(data, nodes, column_names = column_names, flag = flag, :\n#> Missing covariate data detected: imputing covariates.\nwashb_task\n#> A sl3 Task with 4695 obs and these nodes:\n#> $covariates\n#>  [1] \"tr\"              \"fracode\"         \"month\"           \"aged\"           \n#>  [5] \"sex\"             \"momage\"          \"momedu\"          \"momheight\"      \n#>  [9] \"hfiacat\"         \"Nlt18\"           \"Ncomp\"           \"watmin\"         \n#> [13] \"elec\"            \"floor\"           \"walls\"           \"roof\"           \n#> [17] \"asset_wardrobe\"  \"asset_table\"     \"asset_chair\"     \"asset_khat\"     \n#> [21] \"asset_chouki\"    \"asset_tv\"        \"asset_refrig\"    \"asset_bike\"     \n#> [25] \"asset_moto\"      \"asset_sewmach\"   \"asset_mobile\"    \"delta_momage\"   \n#> [29] \"delta_momheight\"\n#> \n#> $outcome\n#> [1] \"whz\"\n#> \n#> $id\n#> NULL\n#> \n#> $weights\n#> NULL\n#> \n#> $offset\n#> NULL\n#> \n#> $time\n#> NULL\nlength(washb_task$folds) # how many folds?\n#> [1] 10\n\nhead(washb_task$folds[[1]]$training_set) # row indexes for fold 1 training\n#> [1] 1 2 3 4 5 6\nhead(washb_task$folds[[1]]$validation_set) # row indexes for fold 1 validation\n#> [1] 12 21 29 41 43 53\n\nany(\n  washb_task$folds[[1]]$training_set %in% washb_task$folds[[1]]$validation_set\n)\n#> [1] FALSE"},{"path":"sl3.html","id":"make-a-super-learner","chapter":"3 Super (Machine) Learning","heading":"2. Make a Super Learner","text":"Now defined machine learning problem sl3_Task, \nready “make” Super Learner (SL). requires specification ofA set candidate machine learning algorithms, also commonly referred \n“library” “learners”. set include diversity algorithms\nbelieved consistent true data-generating distribution.metalearner, ensemble base learners.might also incorporateFeature selection, pass subset predictors algorithm.Hyperparameter specification, tune base learners.Learners properties indicate features support. may use\nsl3_list_properties() get list properties supported least\none learner.Since continuous outcome, may identify learners support\noutcome type sl3_list_learners().Now idea learners, can construct using \nmake_learner function new method.can customize learner hyperparameters incorporate diversity different\nsettings. Documentation learners hyperparameters can found\nsl3 Learners\nReference.can use Lrnr_define_interactions define interaction terms among\ncovariates. interactions supplied list character vectors,\nvector specifies interaction. example, specify\ninteractions (1) tr (whether subject received \nWASH intervention) elec (whether subject electricity); \n(2) tr hfiacat (subject’s level food security).just defined incomplete. order fit learners \ninteractions, need create Pipeline. Pipeline set learners\nfit sequentially, fit one learner used define \ntask next learner. need create Pipeline interaction\ndefining learner another learner incorporate terms fitting\nmodel. Let’s create learner pipeline fit linear model \ncombination main terms interactions terms, specified \nlrn_interaction.can also include learners SuperLearner R package.fun trick create customized learners grid parameters.see Lrnr_caret called sl3_list_learners(c(\"binomial\"))? \nneed specify use popular algorithm candidate SL \nalgorithm want tune, passed method caret::train().\ndefault method parameter selection criterion set “CV”\ninstead caret::train() default boot. summary metric used \nselect optimal model RMSE continuous outcomes Accuracy \ncategorical binomial outcomes.order assemble library learners, need “stack” \ntogether.Stack special learner interface \nlearners. makes stack special combines multiple learners \ntraining simultaneously, predictions can either combined\ncompared.can also stack learners first creating vector, \ninstantiating stack. prefer method, since easily allows us \nmodify names learners.’re jumping ahead bit, let’s check something quickly. ’s\nstraightforward, just one step, set stack \nlearners train cross-validated manner.","code":"\nsl3_list_properties()\n#>  [1] \"binomial\"      \"categorical\"   \"continuous\"    \"cv\"           \n#>  [5] \"density\"       \"h2o\"           \"ids\"           \"importance\"   \n#>  [9] \"offset\"        \"preprocessing\" \"sampling\"      \"screener\"     \n#> [13] \"timeseries\"    \"weights\"       \"wrapper\"\nsl3_list_learners(\"continuous\")\n#>  [1] \"Lrnr_arima\"                     \"Lrnr_bartMachine\"              \n#>  [3] \"Lrnr_bayesglm\"                  \"Lrnr_bilstm\"                   \n#>  [5] \"Lrnr_bound\"                     \"Lrnr_caret\"                    \n#>  [7] \"Lrnr_cv_selector\"               \"Lrnr_dbarts\"                   \n#>  [9] \"Lrnr_earth\"                     \"Lrnr_expSmooth\"                \n#> [11] \"Lrnr_gam\"                       \"Lrnr_gbm\"                      \n#> [13] \"Lrnr_glm\"                       \"Lrnr_glm_fast\"                 \n#> [15] \"Lrnr_glmnet\"                    \"Lrnr_grf\"                      \n#> [17] \"Lrnr_gru_keras\"                 \"Lrnr_gts\"                      \n#> [19] \"Lrnr_h2o_glm\"                   \"Lrnr_h2o_grid\"                 \n#> [21] \"Lrnr_hal9001\"                   \"Lrnr_HarmonicReg\"              \n#> [23] \"Lrnr_hts\"                       \"Lrnr_lightgbm\"                 \n#> [25] \"Lrnr_lstm_keras\"                \"Lrnr_mean\"                     \n#> [27] \"Lrnr_multiple_ts\"               \"Lrnr_nnet\"                     \n#> [29] \"Lrnr_nnls\"                      \"Lrnr_optim\"                    \n#> [31] \"Lrnr_pkg_SuperLearner\"          \"Lrnr_pkg_SuperLearner_method\"  \n#> [33] \"Lrnr_pkg_SuperLearner_screener\" \"Lrnr_polspline\"                \n#> [35] \"Lrnr_randomForest\"              \"Lrnr_ranger\"                   \n#> [37] \"Lrnr_rpart\"                     \"Lrnr_rugarch\"                  \n#> [39] \"Lrnr_screener_correlation\"      \"Lrnr_solnp\"                    \n#> [41] \"Lrnr_stratified\"                \"Lrnr_svm\"                      \n#> [43] \"Lrnr_tsDyn\"                     \"Lrnr_xgboost\"\n# choose base learners\nlrn_glm <- make_learner(Lrnr_glm)\nlrn_mean <- Lrnr_mean$new()\nlrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1\nlrn_ridge <- Lrnr_glmnet$new(alpha = 0)\nlrn_enet.5 <- make_learner(Lrnr_glmnet, alpha = 0.5)\n\nlrn_polspline <- Lrnr_polspline$new()\n\nlrn_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)\n\nlrn_hal_faster <- Lrnr_hal9001$new(max_degree = 2, reduce_basis = 0.05)\n\nxgb_fast <- Lrnr_xgboost$new() # default with nrounds = 20 is pretty fast\nxgb_50 <- Lrnr_xgboost$new(nrounds = 50)\ninteractions <- list(c(\"elec\", \"tr\"), c(\"tr\", \"hfiacat\"))\n# main terms as well as the interactions above will be included\nlrn_interaction <- make_learner(Lrnr_define_interactions, interactions)\n# we already instantiated a linear model learner, no need to do that again\nlrn_glm_interaction <- make_learner(Pipeline, lrn_interaction, lrn_glm)\nlrn_glm_interaction\n#> [1] \"Lrnr_define_interactions_TRUE\"\n#> [1] \"Lrnr_glm_TRUE\"\nlrn_bayesglm <- Lrnr_pkg_SuperLearner$new(\"SL.bayesglm\")\n# I like to crock pot my SLs\ngrid_params <- list(\n  cost = c(0.01, 0.1, 1, 10, 100, 1000),\n  gamma = c(0.001, 0.01, 0.1, 1),\n  kernel = c(\"polynomial\", \"radial\", \"sigmoid\"),\n  degree = c(1, 2, 3)\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\nsvm_learners <- apply(grid, MARGIN = 1, function(tuning_params) {\n  do.call(Lrnr_svm$new, as.list(tuning_params))\n})\ngrid_params <- list(\n  max_depth = c(2, 4, 6),\n  eta = c(0.001, 0.1, 0.3),\n  nrounds = 100\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\ngrid\n#>   max_depth   eta nrounds\n#> 1         2 0.001     100\n#> 2         4 0.001     100\n#> 3         6 0.001     100\n#> 4         2 0.100     100\n#> 5         4 0.100     100\n#> 6         6 0.100     100\n#> 7         2 0.300     100\n#> 8         4 0.300     100\n#> 9         6 0.300     100\n\nxgb_learners <- apply(grid, MARGIN = 1, function(tuning_params) {\n  do.call(Lrnr_xgboost$new, as.list(tuning_params))\n})\nxgb_learners\n#> [[1]]\n#> [1] \"Lrnr_xgboost_100_1_2_0.001\"\n#> \n#> [[2]]\n#> [1] \"Lrnr_xgboost_100_1_4_0.001\"\n#> \n#> [[3]]\n#> [1] \"Lrnr_xgboost_100_1_6_0.001\"\n#> \n#> [[4]]\n#> [1] \"Lrnr_xgboost_100_1_2_0.1\"\n#> \n#> [[5]]\n#> [1] \"Lrnr_xgboost_100_1_4_0.1\"\n#> \n#> [[6]]\n#> [1] \"Lrnr_xgboost_100_1_6_0.1\"\n#> \n#> [[7]]\n#> [1] \"Lrnr_xgboost_100_1_2_0.3\"\n#> \n#> [[8]]\n#> [1] \"Lrnr_xgboost_100_1_4_0.3\"\n#> \n#> [[9]]\n#> [1] \"Lrnr_xgboost_100_1_6_0.3\"\n# Unlike xgboost, I have no idea how to tune a neural net or BART machine, so\n# I let caret take the reins\nlrnr_caret_nnet <- make_learner(Lrnr_caret, algorithm = \"nnet\")\nlrnr_caret_bartMachine <- make_learner(Lrnr_caret,\n  algorithm = \"bartMachine\",\n  method = \"boot\", metric = \"Accuracy\",\n  tuneLength = 10\n)\nstack <- make_learner(\n  Stack, lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50\n)\nstack\n#> [1] \"Lrnr_glm_TRUE\"                                  \n#> [2] \"Lrnr_polspline_5\"                               \n#> [3] \"Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE_FALSE\"\n#> [4] \"Lrnr_glmnet_NULL_deviance_10_0_100_TRUE_FALSE\"  \n#> [5] \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE\"  \n#> [6] \"Lrnr_xgboost_50_1\"\n# named vector of learners first\nlearners <- c(\n  lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50\n)\nnames(learners) <- c(\n  \"glm\", \"polspline\", \"enet.5\", \"ridge\", \"lasso\", \"xgboost50\"\n)\n# next make the stack\nstack <- make_learner(Stack, learners)\n# now the names are pretty\nstack\n#> [1] \"glm\"       \"polspline\" \"enet.5\"    \"ridge\"     \"lasso\"     \"xgboost50\"\ncv_stack <- Lrnr_cv$new(stack)\ncv_stack\n#> [1] \"Lrnr_cv\"\n#> [1] \"glm\"       \"polspline\" \"enet.5\"    \"ridge\"     \"lasso\"     \"xgboost50\""},{"path":"sl3.html","id":"screening-algorithms-for-feature-selection","chapter":"3 Super (Machine) Learning","heading":"Screening Algorithms for Feature Selection","text":"can optionally select subset available covariates pass \nvariables modeling algorithm. current set learners \ncan used prescreening covariates included .Lrnr_screener_importance selects num_screen (default = 5) covariates\nbased variable importance ranking provided learner. \nlearner “importance” method can used Lrnr_screener_importance;\ncurrently includes Lrnr_ranger, Lrnr_randomForest, \nLrnr_xgboost.Lrnr_screener_coefs, provides screening covariates based \nmagnitude estimated coefficients (possibly regularized) GLM.\nthreshold (default = 1e-3) defines minimum absolute size \ncoefficients, thus covariates, kept. Also, max_retain argument\ncan optionally provided restrict number selected covariates \nmax_retain.Lrnr_screener_correlation provides covariate screening procedures \nrunning test correlation (Pearson default), selecting (1)\ntop ranked variables (default), (2) variables pvalue lower \npre-specified threshold.Lrnr_screener_augment augments set screened covariates additional\ncovariates included default, even screener \nselect . example use screener included .Let’s consider screening covariates based randomForest variable\nimportance ranking (ordered mean decrease accuracy). select top\n5 important covariates according ranking, can combine\nLrnr_screener_importance Lrnr_ranger (limiting number trees \nsetting ntree = 20).Hang ! think – confess: Bob Ross know 20\ntrees makes lonely forest, shouldn’t consider , \nsacrifices make chapter build time!example format Lrnr_screener_augment included \nclarity.Selecting covariates non-zero lasso coefficients quite common. Let’s\nconstruct Lrnr_screener_coefs screener just , test \n.“pipe” selected covariates modeling algorithm, need \nmake Pipeline, similar one built regression model \ninteraction terms.Now, learners internal screening preceded screening\nstep.also consider original stack, compare feature selection\nmethods perform comparison methods without feature selection.Analogous seen , stack pipeline \noriginal stack together, may use base learners super\nlearner.use default\nmetalearner,\nuses\nLrnr_solnp \nprovide fitting procedures pairing loss\nfunction \nmetalearner\nfunction. \ndefault metalearner selects loss metalearner pairing based outcome\ntype. Note learner can used metalearner.Now made diverse stack base learners, ready make \nSL. SL algorithm fits metalearner validation set\npredictions/losses across folds.can also use Lrnr_cv build SL, cross-validate stack \nlearners compare performance learners stack, cross-validate\nsingle learner (see “Cross-validation” section sl3\nintroductory tutorial).Furthermore, can Define New sl3\nLearners can used\nplaces otherwise use sl3 learners, including\nPipelines, Stacks, SL.Recall discrete SL, cross-validated selector, metalearner \nassigns weight 1 learner lowest cross-validated empirical\nrisk, weight 0 learners. metalearner specification can\ninvoked Lrnr_cv_selector.","code":"\nminiforest <- Lrnr_ranger$new(\n  num.trees = 20, write.forest = FALSE,\n  importance = \"impurity_corrected\"\n)\n\n# learner must already be instantiated, we did this when we created miniforest\nscreen_rf <- Lrnr_screener_importance$new(learner = miniforest, num_screen = 5)\nscreen_rf\n#> [1] \"Lrnr_screener_importance_5\"\n\n# which covariates are selected on the full data?\nscreen_rf$train(washb_task)\n#> [1] \"Lrnr_screener_importance_5\"\n#> $selected\n#> [1] \"aged\"      \"month\"     \"tr\"        \"momheight\" \"momedu\"\nkeepme <- c(\"aged\", \"momage\")\n# screener must already be instantiated, we did this when we created screen_rf\nscreen_augment_rf <- Lrnr_screener_augment$new(\n  screener = screen_rf, default_covariates = keepme\n)\nscreen_augment_rf\n#> [1] \"Lrnr_screener_augment_c(\\\"aged\\\", \\\"momage\\\")\"\n# we already instantiated a lasso learner above, no need to do it again\nscreen_lasso <- Lrnr_screener_coefs$new(learner = lrn_lasso, threshold = 0)\nscreen_lasso\n#> [1] \"Lrnr_screener_coefs_0_NULL_2\"\nscreen_rf_pipe <- make_learner(Pipeline, screen_rf, stack)\nscreen_lasso_pipe <- make_learner(Pipeline, screen_lasso, stack)\n# pretty names again\nlearners2 <- c(learners, screen_rf_pipe, screen_lasso_pipe)\nnames(learners2) <- c(names(learners), \"randomforest_screen\", \"lasso_screen\")\n\nfancy_stack <- make_learner(Stack, learners2)\nfancy_stack\n#> [1] \"glm\"                 \"polspline\"           \"enet.5\"             \n#> [4] \"ridge\"               \"lasso\"               \"xgboost50\"          \n#> [7] \"randomforest_screen\" \"lasso_screen\"\nsl <- make_learner(Lrnr_sl, learners = fancy_stack)\ndiscrete_sl_metalrn <- Lrnr_cv_selector$new()\ndiscrete_sl <- Lrnr_sl$new(\n  learners = fancy_stack,\n  metalearner = discrete_sl_metalrn\n)"},{"path":"sl3.html","id":"train-the-super-learner-on-the-machine-learning-task","chapter":"3 Super (Machine) Learning","heading":"3. Train the Super Learner on the machine learning task","text":"SL algorithm fits metalearner validation-set predictions \ncross-validated manner, thereby avoiding overfitting.Now ready “train” SL sl3_task object, washb_task.","code":"\nset.seed(4197)\nsl_fit <- sl$train(washb_task)"},{"path":"sl3.html","id":"obtain-predicted-values","chapter":"3 Super (Machine) Learning","heading":"4. Obtain predicted values","text":"Now fit SL, ready calculate predicted outcome\nsubject.can also obtain summary results.table printed SL fit, note SL mean risk r round(sl_fit_summary$risk[length(sl_fit_summary$learner)], 4) \nensemble weighted ranger glmnet learners highest \nweighting mean learner highly.can also see glmnet learner lowest cross-validated mean\nrisk, thus making cross-validated selector (discrete SL). \nmean risk SL calculated using data, separate\nhold-, SL’s mean risk reported underestimation.","code":"\n# we did it! now we have SL predictions\nsl_preds <- sl_fit$predict()\nhead(sl_preds)\n#> [1] -0.65442 -0.77055 -0.67359 -0.65109 -0.65577 -0.65673\nsl_fit_summary <- sl_fit$print()\n#> [1] \"SuperLearner:\"\n#> List of 8\n#>  $ glm                : chr \"Lrnr_glm_TRUE\"\n#>  $ polspline          : chr \"Lrnr_polspline_5\"\n#>  $ enet.5             : chr \"Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE_FALSE\"\n#>  $ ridge              : chr \"Lrnr_glmnet_NULL_deviance_10_0_100_TRUE_FALSE\"\n#>  $ lasso              : chr \"Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE\"\n#>  $ xgboost50          : chr \"Lrnr_xgboost_50_1\"\n#>  $ randomforest_screen: chr \"Pipeline(Lrnr_screener_importance_5->Stack)\"\n#>  $ lasso_screen       : chr \"Pipeline(Lrnr_screener_coefs_0_NULL_2->Stack)\"\n#> [1] \"Lrnr_solnp_TRUE_TRUE_FALSE_1e-05\"\n#> $pars\n#>  [1] 0.055571 0.055556 0.055564 0.055570 0.055564 0.055591 0.055546 0.055561\n#>  [9] 0.055546 0.055546 0.055546 0.055523 0.055559 0.055559 0.055559 0.055559\n#> [17] 0.055559 0.055521\n#> \n#> $convergence\n#> [1] 0\n#> \n#> $values\n#> [1] 1.0135 1.0135\n#> \n#> $lagrange\n#>            [,1]\n#> [1,] -0.0021608\n#> \n#> $hessian\n#>       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]\n#>  [1,]    1    0    0    0    0    0    0    0    0     0     0     0     0\n#>  [2,]    0    1    0    0    0    0    0    0    0     0     0     0     0\n#>  [3,]    0    0    1    0    0    0    0    0    0     0     0     0     0\n#>  [4,]    0    0    0    1    0    0    0    0    0     0     0     0     0\n#>  [5,]    0    0    0    0    1    0    0    0    0     0     0     0     0\n#>  [6,]    0    0    0    0    0    1    0    0    0     0     0     0     0\n#>  [7,]    0    0    0    0    0    0    1    0    0     0     0     0     0\n#>  [8,]    0    0    0    0    0    0    0    1    0     0     0     0     0\n#>  [9,]    0    0    0    0    0    0    0    0    1     0     0     0     0\n#> [10,]    0    0    0    0    0    0    0    0    0     1     0     0     0\n#> [11,]    0    0    0    0    0    0    0    0    0     0     1     0     0\n#> [12,]    0    0    0    0    0    0    0    0    0     0     0     1     0\n#> [13,]    0    0    0    0    0    0    0    0    0     0     0     0     1\n#> [14,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [15,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [16,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [17,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#> [18,]    0    0    0    0    0    0    0    0    0     0     0     0     0\n#>       [,14] [,15] [,16] [,17] [,18]\n#>  [1,]     0     0     0     0     0\n#>  [2,]     0     0     0     0     0\n#>  [3,]     0     0     0     0     0\n#>  [4,]     0     0     0     0     0\n#>  [5,]     0     0     0     0     0\n#>  [6,]     0     0     0     0     0\n#>  [7,]     0     0     0     0     0\n#>  [8,]     0     0     0     0     0\n#>  [9,]     0     0     0     0     0\n#> [10,]     0     0     0     0     0\n#> [11,]     0     0     0     0     0\n#> [12,]     0     0     0     0     0\n#> [13,]     0     0     0     0     0\n#> [14,]     1     0     0     0     0\n#> [15,]     0     1     0     0     0\n#> [16,]     0     0     1     0     0\n#> [17,]     0     0     0     1     0\n#> [18,]     0     0     0     0     1\n#> \n#> $ineqx0\n#> NULL\n#> \n#> $nfuneval\n#> [1] 23\n#> \n#> $outer.iter\n#> [1] 1\n#> \n#> $elapsed\n#> Time difference of 0.09844 secs\n#> \n#> $vscale\n#>  [1] 1.01351 0.00001 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000\n#> [10] 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000 1.00000\n#> [19] 1.00000 1.00000\n#> \n#> $coefficients\n#>                           glm                     polspline \n#>                      0.055571                      0.055556 \n#>                        enet.5                         ridge \n#>                      0.055564                      0.055570 \n#>                         lasso                     xgboost50 \n#>                      0.055564                      0.055591 \n#>       randomforest_screen_glm randomforest_screen_polspline \n#>                      0.055546                      0.055561 \n#>    randomforest_screen_enet.5     randomforest_screen_ridge \n#>                      0.055546                      0.055546 \n#>     randomforest_screen_lasso randomforest_screen_xgboost50 \n#>                      0.055546                      0.055523 \n#>              lasso_screen_glm        lasso_screen_polspline \n#>                      0.055559                      0.055559 \n#>           lasso_screen_enet.5            lasso_screen_ridge \n#>                      0.055559                      0.055559 \n#>            lasso_screen_lasso        lasso_screen_xgboost50 \n#>                      0.055559                      0.055521 \n#> \n#> $training_offset\n#> [1] FALSE\n#> \n#> $name\n#> [1] \"solnp\"\n#> \n#> [1] \"Cross-validated risk:\"\n#>                           learner coefficients   risk       se  fold_sd\n#>  1:                           glm     0.055571 1.0202 0.023955 0.067500\n#>  2:                     polspline     0.055556 1.0208 0.023577 0.067921\n#>  3:                        enet.5     0.055564 1.0131 0.023598 0.065732\n#>  4:                         ridge     0.055570 1.0153 0.023739 0.065299\n#>  5:                         lasso     0.055564 1.0130 0.023592 0.065840\n#>  6:                     xgboost50     0.055591 1.1136 0.025262 0.077580\n#>  7:       randomforest_screen_glm     0.055546 1.0271 0.024119 0.069913\n#>  8: randomforest_screen_polspline     0.055561 1.0236 0.024174 0.068710\n#>  9:    randomforest_screen_enet.5     0.055546 1.0266 0.024101 0.070117\n#> 10:     randomforest_screen_ridge     0.055546 1.0268 0.024120 0.069784\n#> 11:     randomforest_screen_lasso     0.055546 1.0266 0.024101 0.070135\n#> 12: randomforest_screen_xgboost50     0.055523 1.1399 0.026341 0.100112\n#> 13:              lasso_screen_glm     0.055559 1.0164 0.023542 0.065018\n#> 14:        lasso_screen_polspline     0.055559 1.0177 0.023520 0.065566\n#> 15:           lasso_screen_enet.5     0.055559 1.0163 0.023544 0.065017\n#> 16:            lasso_screen_ridge     0.055559 1.0166 0.023553 0.064869\n#> 17:            lasso_screen_lasso     0.055559 1.0163 0.023544 0.065020\n#> 18:        lasso_screen_xgboost50     0.055521 1.1256 0.025939 0.084270\n#> 19:                  SuperLearner           NA 1.0135 0.023615 0.067434\n#>     fold_min_risk fold_max_risk\n#>  1:       0.89442        1.1200\n#>  2:       0.89892        1.1255\n#>  3:       0.88839        1.1058\n#>  4:       0.88559        1.1063\n#>  5:       0.88842        1.1060\n#>  6:       0.96019        1.2337\n#>  7:       0.90251        1.1326\n#>  8:       0.90167        1.1412\n#>  9:       0.90030        1.1319\n#> 10:       0.90068        1.1311\n#> 11:       0.90043        1.1321\n#> 12:       0.92377        1.2549\n#> 13:       0.90204        1.1156\n#> 14:       0.89742        1.1162\n#> 15:       0.90184        1.1154\n#> 16:       0.90120        1.1146\n#> 17:       0.90183        1.1154\n#> 18:       0.96251        1.2327\n#> 19:       0.88685        1.1102"},{"path":"sl3.html","id":"cross-validated-super-learner","chapter":"3 Super (Machine) Learning","heading":"Cross-validated Super Learner","text":"can cross-validate SL see well SL performs unseen data, \nobtain estimate cross-validated risk SL.estimation procedure requires “outer/external” layer \ncross-validation, also called nested cross-validation, involves setting\naside separate holdout sample don’t use fit SL. external\ncross-validation procedure may also incorporate 10 folds, default\nsl3. However, incorporate 2 outer/external folds \ncross-validation computational efficiency.also need specify loss function evaluate SL. Documentation \navailable loss functions can found sl3 Loss Function\nReference.","code":"\nwashb_task_new <- make_sl3_Task(\n  data = washb_data,\n  covariates = covars,\n  outcome = outcome,\n  folds = origami::make_folds(washb_data, fold_fun = folds_vfold, V = 2)\n)\nCVsl <- CV_lrnr_sl(\n  lrnr_sl = sl_fit, task = washb_task_new, loss_fun = loss_squared_error\n)\nCVsl %>%\n  kable(digits = 4) %>%\n  kableExtra:::kable_styling(fixed_thead = T) %>%\n  scroll_box(width = \"100%\", height = \"300px\")"},{"path":"sl3.html","id":"variable-importance-measures-with-sl3","chapter":"3 Super (Machine) Learning","heading":"Variable Importance Measures with sl3","text":"Variable importance can interesting informative. can also \ncontradictory confusing. Nevertheless, like , \ncollaborators, created variable importance function sl3! sl3\nimportance function returns table variables listed decreasing order\nimportance (.e., important first row).measure importance sl3 based risk ratio, risk difference,\nlearner fit removed, permuted, covariate learner\nfit true covariate, across covariates. manner, larger\nrisk difference, important variable prediction.intuition measure calculates risk (terms \naverage loss predictive accuracy) losing one covariate, keeping\neverything else fixed, compares risk covariate \nlost. risk ratio one, risk difference zero, losing \ncovariate impact, thus important measure. \nacross covariates. stated , can remove covariate \nrefit SL without , just permute covariate (faster, risky)\nhope shuffling distort meaningful information \npresent covariate. idea permuting instead removing saves lot\ntime, also incorporated randomForest variable importance\nmeasures. However, permutation approach risky, importance function\ndefault remove refit.Let’s explore sl3 variable importance measurements washb data.","code":"\nwashb_varimp <- importance(sl_fit, loss = loss_squared_error, type = \"permute\")\nwashb_varimp %>%\n  kable(digits = 4) %>%\n  kableExtra:::kable_styling(fixed_thead = TRUE) %>%\n  scroll_box(width = \"100%\", height = \"300px\")\n# plot variable importance\nimportance_plot(\n  washb_varimp,\n  main = \"sl3 Variable Importance for WASH Benefits Example Data\"\n)"},{"path":"sl3.html","id":"sl3-exercises","chapter":"3 Super (Machine) Learning","heading":"3.1 Exercises","text":"","code":""},{"path":"sl3.html","id":"sl3ex1","chapter":"3 Super (Machine) Learning","heading":"3.1.1 Predicting Myocardial Infarction with sl3","text":"Follow steps predict myocardial infarction (mi) using \navailable covariate data. thank Prof. David Benkeser Emory University \nmaking Cardiovascular Health Study (CHS) data accessible.Create sl3 task, setting myocardial infarction mi outcome \nusing available covariate data.Make library seven relatively fast base learning algorithms (.e., \nconsider BART HAL). Customize hyperparameters one \nlearners. Feel free use learners sl3 SuperLearner. may\nuse base learning library presented .Incorporate least one pipeline feature selection. screener \nlearner(s) can used.Fit metalearning step default metalearner.metalearner base learners, make Super Learner (SL) \ntrain task.Print SL fit calling print() $.Cross-validate SL fit see well performs unseen\ndata. Specify valid loss function evaluate SL.Use importance() function identify “important” predictor \nmyocardial infarction, according sl3 importance metrics.","code":"\n# load the data set\ndb_data <- url(\n  paste0(\n    \"https://raw.githubusercontent.com/benkeser/sllecture/master/\",\n    \"chspred.csv\"\n  )\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)\n\n# take a quick peek\nhead(chspred) %>%\n  kable(digits = 4) %>%\n  kableExtra:::kable_styling(fixed_thead = TRUE) %>%\n  scroll_box(width = \"100%\", height = \"300px\")"},{"path":"sl3.html","id":"sl3ex2","chapter":"3 Super (Machine) Learning","heading":"3.1.2 Predicting Recurrent Ischemic Stroke in an RCT with sl3","text":"exercise, work random sample 5,000 patients \nparticipated International Stroke Trial (IST). data described \nChapter 3.2 tlverse\nhandbook.Train SL predict recurrent stroke DRSISC available covariate\ndata (25 variables). course, can consider feature selection\nmachine learning algorithms. data, outcome \noccasionally missing, sure specify drop_missing_outcome = TRUE\ndefining task.Use SL-based predictions calculate area ROC curve (AUC).Calculate cross-validated AUC evaluate performance \nSL unseen data.covariates predictive 14-day recurrent stroke,\naccording sl3 variable importance measures?","code":"\nist_data <- paste0(\n  \"https://raw.githubusercontent.com/tlverse/\",\n  \"tlverse-handbook/master/data/ist_sample.csv\"\n) %>% fread()\n\n# number 3 help\nist_task_CVsl <- make_sl3_Task(\n  data = ist_data,\n  outcome = \"DRSISC\",\n  covariates = colnames(ist_data)[-which(names(ist_data) == \"DRSISC\")],\n  drop_missing_outcome = TRUE,\n  folds = origami::make_folds(\n    n = sum(!is.na(ist_data$DRSISC)),\n    fold_fun = folds_vfold,\n    V = 5\n  )\n)"},{"path":"sl3.html","id":"concluding-remarks","chapter":"3 Super (Machine) Learning","heading":"3.2 Concluding Remarks","text":"Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.Super Learner (SL) general approach can applied diversity \nestimation prediction problems can defined loss function.straightforward plug estimator returned SL \ntarget parameter mapping.\nexample, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = \\mathbb{E}_{0,W}[\\mathbb{E}_0(Y \\mid =1,W) -  \\mathbb{E}_0(Y \\mid =0,W)]\\).\nuse SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.\nConsidering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent \npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i \\mid A_i=1,W_i\\) \\(=1,\\ldots,n\\). \nvector represent predicted outcomes intervention \nsets subjects receive \\(=0\\), \\(Y_i \\mid A_i=0,W_i\\) \n\\(=1,\\ldots,n\\).\nobtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.\nsl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping \nelse ), A0_task$data contain 0’s (level\npertains receiving treatment) treatment column \ndata.\nstraightforward plug estimator returned SL \ntarget parameter mapping.example, suppose average treatment effect (ATE) \nbinary treatment intervention:\n\\(\\Psi_0 = \\mathbb{E}_{0,W}[\\mathbb{E}_0(Y \\mid =1,W) -  \\mathbb{E}_0(Y \\mid =0,W)]\\).use SL trained original data (let’s call\nsl_fit) predict outcome subjects \nintervention. need take average difference\ncounterfactual outcomes intervention interest.Considering \\(\\Psi_0\\) , first need two \\(n\\)-length vectors \npredicted outcomes intervention. One vector represent \npredicted outcomes intervention sets subjects \nreceive \\(=1\\), \\(Y_i \\mid A_i=1,W_i\\) \\(=1,\\ldots,n\\). \nvector represent predicted outcomes intervention \nsets subjects receive \\(=0\\), \\(Y_i \\mid A_i=0,W_i\\) \n\\(=1,\\ldots,n\\).obtaining vectors counterfactual predicted outcomes, \nneed average take difference order \n“plug-” SL estimator target parameter mapping.sl3 current ATE example, achieved \nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task));\nA1_task$data contain 1’s (level pertains \nreceiving treatment) treatment column data (keeping \nelse ), A0_task$data contain 0’s (level\npertains receiving treatment) treatment column \ndata.’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased; however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!’s worthwhile exercise obtain predicted counterfactual outcomes\ncreate counterfactual sl3 tasks. ’s biased; however, \nplug SL fit target parameter mapping, (e.g., calling result\nmean(sl_fit$predict(A1_task)) - mean(sl_fit$predict(A0_task)) \nestimated ATE. end estimator ATE \noptimized estimation prediction function, ATE!end “analysis day”, want estimator optimized \ntarget estimand interest. ultimately care good job\nestimating \\(\\psi_0\\). SL essential step help us get . \nfact, use counterfactual predicted outcomes explained\nlength . However, SL end estimation procedure.\nSpecifically, Super Learner asymptotically linear\nestimator target estimand; efficient substitution\nestimator. begs question, important estimator \npossess properties?\nasymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values).\nSubstitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties.\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient.\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem. Various canonical gradient \nshown chapters follow.\nPractitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.\n\nend “analysis day”, want estimator optimized \ntarget estimand interest. ultimately care good job\nestimating \\(\\psi_0\\). SL essential step help us get . \nfact, use counterfactual predicted outcomes explained\nlength . However, SL end estimation procedure.\nSpecifically, Super Learner asymptotically linear\nestimator target estimand; efficient substitution\nestimator. begs question, important estimator \npossess properties?asymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values).asymptotically linear estimator converges estimand \n\\(\\frac{1}{\\sqrt{n}}\\) rate, thereby permitting formal statistical inference\n(.e., confidence intervals \\(p\\)-values).Substitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties.Substitution, plug-, estimators estimand desirable \nrespect local global constraints statistical model\n(e.g., bounds), better finite-sample properties.efficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient.\ncanonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem. Various canonical gradient \nshown chapters follow.\nPractitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.\nefficient estimator optimal sense lowest\npossible variance, thus precise. estimator efficient\nasymptotically linear influence curve equal \ncanonical gradient.canonical gradient mathematical object specific \ntarget estimand, provides information level \ndifficulty estimation problem. Various canonical gradient \nshown chapters follow.Practitioner’s need know calculate canonical\ngradient order understand efficiency use Targeted Maximum\nLikelihood Estimation (TMLE). Metaphorically, need \nYoda order Jedi.TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators.TMLE general strategy succeeds constructing efficient \nasymptotically linear plug-estimators.SL fantastic pure prediction, obtaining initial\nestimate first step TMLE, need second step TMLE \ndesirable statistical properties mentioned .SL fantastic pure prediction, obtaining initial\nestimate first step TMLE, need second step TMLE \ndesirable statistical properties mentioned .chapters follow, focus targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.chapters follow, focus targeted maximum likelihood\nestimator targeted minimum loss-based estimator, referred \nTMLE.","code":""},{"path":"sl3.html","id":"appendix","chapter":"3 Super (Machine) Learning","heading":"Appendix","text":"","code":""},{"path":"sl3.html","id":"sl3ex1-sol","chapter":"3 Super (Machine) Learning","heading":"3.2.1 Exercise 1 Solution","text":"potential solution sl3 Exercise 1 – Predicting Myocardial\nInfarction sl3.","code":"\ndb_data <- url(\n  \"https://raw.githubusercontent.com/benkeser/sllecture/master/chspred.csv\"\n)\nchspred <- read_csv(file = db_data, col_names = TRUE)\n\n# make task\nchspred_task <- make_sl3_Task(\n  data = chspred,\n  covariates = head(colnames(chspred), -1),\n  outcome = \"mi\"\n)\n\n# make learners\nglm_learner <- Lrnr_glm$new()\nlasso_learner <- Lrnr_glmnet$new(alpha = 1)\nridge_learner <- Lrnr_glmnet$new(alpha = 0)\nenet_learner <- Lrnr_glmnet$new(alpha = 0.5)\n# curated_glm_learner uses formula = \"mi ~ smoke + beta + waist\"\ncurated_glm_learner <- Lrnr_glm_fast$new(covariates = c(\"smoke, beta, waist\"))\nmean_learner <- Lrnr_mean$new() # That is one mean learner!\nglm_fast_learner <- Lrnr_glm_fast$new()\nranger_learner <- Lrnr_ranger$new()\nsvm_learner <- Lrnr_svm$new()\nxgb_learner <- Lrnr_xgboost$new()\n\n# screening\nscreen_cor <- make_learner(Lrnr_screener_correlation)\nglm_pipeline <- make_learner(Pipeline, screen_cor, glm_learner)\n\n# stack learners together\nstack <- make_learner(\n  Stack,\n  glm_pipeline, glm_learner,\n  lasso_learner, ridge_learner, enet_learner,\n  curated_glm_learner, mean_learner, glm_fast_learner,\n  ranger_learner, svm_learner, xgb_learner\n)\n\n# make and train SL\nsl <- Lrnr_sl$new(\n  learners = stack\n)\nsl_fit <- sl$train(chspred_task)\nsl_fit$print()\n\nCVsl <- CV_lrnr_sl(sl_fit, chspred_task, loss_loglik_binomial)\nCVsl\n\nvarimp <- importance(sl_fit, type = \"permute\")\nvarimp %>%\n  importance_plot(\n    main = \"sl3 Variable Importance for Myocardial Infarction Prediction\"\n  )"},{"path":"sl3.html","id":"sl3ex2-sol","chapter":"3 Super (Machine) Learning","heading":"3.2.2 Exercise 2 Solution","text":"potential solution sl3 Exercise 2 – Predicting Recurrent\nIschemic Stroke RCT sl3.","code":"\nlibrary(ROCR) # for AUC calculation\n\nist_data <- paste0(\n  \"https://raw.githubusercontent.com/tlverse/\",\n  \"tlverse-handbook/master/data/ist_sample.csv\"\n) %>% fread()\n\n# stack\nist_task <- make_sl3_Task(\n  data = ist_data,\n  outcome = \"DRSISC\",\n  covariates = colnames(ist_data)[-which(names(ist_data) == \"DRSISC\")],\n  drop_missing_outcome = TRUE\n)\n\n# learner library\nlrn_glm <- Lrnr_glm$new()\nlrn_lasso <- Lrnr_glmnet$new(alpha = 1)\nlrn_ridge <- Lrnr_glmnet$new(alpha = 0)\nlrn_enet <- Lrnr_glmnet$new(alpha = 0.5)\nlrn_mean <- Lrnr_mean$new()\nlrn_ranger <- Lrnr_ranger$new()\nlrn_svm <- Lrnr_svm$new()\n# xgboost grid\ngrid_params <- list(\n  max_depth = c(2, 5, 8),\n  eta = c(0.01, 0.15, 0.3)\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\nparams_default <- list(nthread = getOption(\"sl.cores.learners\", 1))\nxgb_learners <- apply(grid, MARGIN = 1, function(params_tune) {\n  do.call(Lrnr_xgboost$new, c(params_default, as.list(params_tune)))\n})\nlearners <- unlist(list(\n  xgb_learners, lrn_ridge, lrn_mean, lrn_lasso,\n  lrn_glm, lrn_enet, lrn_ranger, lrn_svm\n),\nrecursive = TRUE\n)\n\n# SL\nsl <- Lrnr_sl$new(learners)\nsl_fit <- sl$train(ist_task)\n\n# AUC\npreds <- sl_fit$predict()\nobs <- c(na.omit(ist_data$DRSISC))\nAUC <- performance(prediction(sl_preds, obs), measure = \"auc\")@y.values[[1]]\nplot(performance(prediction(sl_preds, obs), \"tpr\", \"fpr\"))\n\n# CVsl\nist_task_CVsl <- make_sl3_Task(\n  data = ist_data,\n  outcome = \"DRSISC\",\n  covariates = colnames(ist_data)[-which(names(ist_data) == \"DRSISC\")],\n  drop_missing_outcome = TRUE,\n  folds = origami::make_folds(\n    n = sum(!is.na(ist_data$DRSISC)),\n    fold_fun = folds_vfold,\n    V = 5\n  )\n)\nCVsl <- CV_lrnr_sl(sl_fit, ist_task_CVsl, loss_loglik_binomial)\nCVsl\n\n# sl3 variable importance plot\nist_varimp <- importance(sl_fit, type = \"permute\")\nist_varimp %>%\n  importance_plot(\n    main = \"Variable Importance for Predicting Recurrent Ischemic Stroke\"\n  )"},{"path":"tmle3.html","id":"tmle3","chapter":"4 The TMLE Framework","heading":"4 The TMLE Framework","text":"Jeremy Coyle Nima HejaziBased tmle3 R package.","code":""},{"path":"tmle3.html","id":"learn-tmle","chapter":"4 The TMLE Framework","heading":"4.1 Learning Objectives","text":"end chapter, able toUnderstand use TMLE effect estimation.Use tmle3 estimate Average Treatment Effect (ATE).Understand use tmle3 “Specs” objects.Fit tmle3 custom set target parameters.Use delta method estimate transformations target parameters.","code":""},{"path":"tmle3.html","id":"tmle-intro","chapter":"4 The TMLE Framework","heading":"4.2 Introduction","text":"previous chapter sl3 learned estimate regression\nfunction like \\(\\mathbb{E}[Y \\mid X]\\) data. ’s important first step\nlearning data, can use predictive model estimate\nstatistical causal effects?Going back roadmap targeted learning, suppose ’d like \nestimate effect treatment variable \\(\\) outcome \\(Y\\). discussed,\none potential parameter characterizes effect Average Treatment\nEffect (ATE), defined \\(\\psi_0 = \\mathbb{E}_W[\\mathbb{E}[Y \\mid =1,W] - \\mathbb{E}[Y \\mid =0,W]]\\) interpreted difference mean outcome\ntreatment \\(=1\\) \\(=0\\), averaging distribution \ncovariates \\(W\\). ’ll illustrate several potential estimators \nparameter, motivate use TMLE (targeted maximum likelihood\nestimation; targeted minimum loss-based estimation) framework, using \nfollowing example data:small ticks right indicate mean outcomes (averaging \\(W\\))\n\\(=1\\) \\(=0\\) respectively, difference quantity ’d\nlike estimate.hope motivate application TMLE chapter, refer \ninterested reader two Targeted Learning books associated works \nfull technical details.","code":""},{"path":"tmle3.html","id":"substitution-est","chapter":"4 The TMLE Framework","heading":"4.3 Substitution Estimators","text":"can use sl3 fit Super Learner regression model estimate\noutcome regression function \\(\\mathbb{E}_0[Y \\mid ,W]\\), often refer\n\\(\\overline{Q}_0(,W)\\) whose estimate denote \\(\\overline{Q}_n(,W)\\).\nconstruct estimate ATE \\(\\psi_n\\), need “plug-” \nestimates \\(\\overline{Q}_n(,W)\\), evaluated two intervention contrasts,\ncorresponding ATE “plug-” formula:\n\\(\\psi_n = \\frac{1}{n}\\sum(\\overline{Q}_n(1,W)-\\overline{Q}_n(0,W))\\). kind\nestimator called plug-substitution estimator, since accurate\nestimates \\(\\psi_n\\) parameter \\(\\psi_0\\) may obtained substituting\nestimates \\(\\overline{Q}_n(,W)\\) relevant regression functions\n\\(\\overline{Q}_0(,W)\\) .Applying sl3 estimate outcome regression example, can see\nensemble machine learning predictions fit data quite well:solid lines indicate sl3 estimate regression function, \ndotted lines indicating tmle3 updates (described ).substitution estimators intuitive, naively using approach \nSuper Learner estimate \\(\\bar{Q}_0(,W)\\) several limitations. First, Super\nLearner selecting learner weights minimize risk across entire\nregression function, instead “targeting” ATE parameter hope \nestimate, leading biased estimation. , sl3 trying well \nfull regression curve left, instead focusing small ticks \nright. ’s , sampling distribution approach \nasymptotically linear, therefore inference possible.can see limitations illustrated estimates generated \nexample data:see Super Learner, estimates true parameter value (indicated \ndashed vertical line) accurately GLM. However, still less\naccurate TMLE, valid inference possible. contrast, TMLE\nachieves less biased estimator valid inference.","code":""},{"path":"tmle3.html","id":"tmle","chapter":"4 The TMLE Framework","heading":"4.4 Targeted Maximum Likelihood Estimation","text":"TMLE takes initial estimate \\(\\overline{Q}_n(,W)\\) well estimate \npropensity score \\(g_n(\\mid W) = \\mathbb{P}(= 1 \\mid W)\\) produces \nupdated estimate \\(\\overline{Q}^{\\star}_n(,W)\\) “targeted” \nparameter interest. TMLE keeps benefits substitution estimators (\none), augments original, potentially erratic estimates correct \nbias also resulting asymptotically linear (thus normally\ndistributed) estimator accommodates inference via asymptotically consistent\nWald-style confidence intervals.","code":""},{"path":"tmle3.html","id":"tmle-updates","chapter":"4 The TMLE Framework","heading":"4.4.1 TMLE Updates","text":"different types TMLEs (, sometimes, multiple set \ntarget parameters) – , give example algorithm TML\nestimation ATE. \\(\\overline{Q}^{\\star}_n(,W)\\) TMLE-augmented\nestimate \\(f(\\overline{Q}^{\\star}_n(,W)) = f(\\overline{Q}_n(,W)) + \\epsilon \\cdot H_n(,W)\\), \\(f(\\cdot)\\) appropriate link function (e.g.,\n\\(\\text{logit}(x) = \\log\\left(\\frac{x}{1 - x}\\right)\\)), estimate\n\\(\\epsilon_n\\) coefficient \\(\\epsilon\\) “clever covariate” \\(H_n(,W)\\)\ncomputed. form covariate \\(H_n(,W)\\) differs across target\nparameters; case ATE, \\(H_n(,W) = \\frac{}{g_n(\\mid W)} - \\frac{1-}{1-g_n(, W)}\\), \\(g_n(,W) = \\mathbb{P}(=1 \\mid W)\\) \nestimated propensity score, estimator depends initial fit (\nsl3) outcome regression (\\(\\overline{Q}_n\\)) propensity score\n(\\(g_n\\)).several robust augmentations used across tlverse,\nincluding use additional layer cross-validation avoid\n-fitting bias (.e., CV-TMLE) well approaches consistently\nestimating several parameters simultaneously (e.g., points survival\ncurve).","code":""},{"path":"tmle3.html","id":"tmle-infer","chapter":"4 The TMLE Framework","heading":"4.4.2 Statistical Inference","text":"Since TMLE yields asymptotically linear estimator, obtaining statistical\ninference convenient. TML estimator corresponding\n(efficient) influence function (often, “EIF”, short) describes \nasymptotic distribution estimator. using estimated EIF, Wald-style\ninference (asymptotically correct confidence intervals) can constructed\nsimply plugging form EIF initial estimates\n\\(\\overline{Q}^{\\star}_n\\) \\(g_n\\), computing sample standard error.following sections describe simple detailed way \nspecifying estimating TMLE tlverse. designing tmle3, \nsought replicate closely possible general estimation framework\nTMLE, theoretical object relevant TMLE encoded \ncorresponding software object/method. First, present simple\napplication tmle3 WASH Benefits example, go describe\nunderlying objects greater detail.","code":""},{"path":"tmle3.html","id":"easy-bake-example-tmle3-for-ate","chapter":"4 The TMLE Framework","heading":"4.5 Easy-Bake Example: tmle3 for ATE","text":"’ll illustrate basic use TMLE using WASH Benefits data\nintroduced earlier estimating average treatment effect.","code":""},{"path":"tmle3.html","id":"load-the-data","chapter":"4 The TMLE Framework","heading":"4.5.1 Load the Data","text":"’ll use WASH Benefits data earlier chapters:","code":"\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(tmle3)\nlibrary(sl3)\nwashb_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/tlverse-data/master/\",\n    \"wash-benefits/washb_data.csv\"\n  ),\n  stringsAsFactors = TRUE\n)"},{"path":"tmle3.html","id":"define-the-variable-roles","chapter":"4 The TMLE Framework","heading":"4.5.2 Define the variable roles","text":"’ll use common \\(W\\) (covariates), \\(\\) (treatment/intervention), \\(Y\\)\n(outcome) data structure. tmle3 needs know variables dataset\ncorrespond roles. use list character vectors tell\n. call “Node List” corresponds nodes Directed\nAcyclic Graph (DAG), way displaying causal relationships variables.","code":"\nnode_list <- list(\n  W = c(\n    \"month\", \"aged\", \"sex\", \"momage\", \"momedu\",\n    \"momheight\", \"hfiacat\", \"Nlt18\", \"Ncomp\", \"watmin\",\n    \"elec\", \"floor\", \"walls\", \"roof\", \"asset_wardrobe\",\n    \"asset_table\", \"asset_chair\", \"asset_khat\",\n    \"asset_chouki\", \"asset_tv\", \"asset_refrig\",\n    \"asset_bike\", \"asset_moto\", \"asset_sewmach\",\n    \"asset_mobile\"\n  ),\n  A = \"tr\",\n  Y = \"whz\"\n)"},{"path":"tmle3.html","id":"handle-missingness","chapter":"4 The TMLE Framework","heading":"4.5.3 Handle Missingness","text":"Currently, missingness tmle3 handled fairly simple way:Missing covariates median- (continuous) mode- (discrete)\nimputed, additional covariates indicating imputation generated, just\ndescribed sl3 chapter.Missing treatment variables excluded – observations dropped.Missing outcomes efficiently handled automatic calculation (\nincorporation estimators) inverse probability censoring weights\n(IPCW); also known IPCW-TMLE may thought joint\nintervention remove missingness analogous procedure used \nclassical inverse probability weighted estimators.steps implemented process_missing function tmle3:","code":"\nprocessed <- process_missing(washb_data, node_list)\nwashb_data <- processed$data\nnode_list <- processed$node_list"},{"path":"tmle3.html","id":"create-a-spec-object","chapter":"4 The TMLE Framework","heading":"4.5.4 Create a “Spec” Object","text":"tmle3 general, allows components TMLE procedure \nspecified modular way. However, end-users interested \nmanually specifying components. Therefore, tmle3 implements \ntmle3_Spec object bundles set components specification\n(“Spec”) , minimal additional detail, can run end-user.’ll start using one specs, work way \ninternals tmle3.","code":"\nate_spec <- tmle_ATE(\n  treatment_level = \"Nutrition + WSH\",\n  control_level = \"Control\"\n)"},{"path":"tmle3.html","id":"define-the-learners","chapter":"4 The TMLE Framework","heading":"4.5.5 Define the learners","text":"Currently, thing user must define sl3 learners used\nestimate relevant factors likelihood: Q g.takes form list sl3 learners, one likelihood factor\nestimated sl3:, use Super Learner defined previous chapter. future,\nplan include reasonable defaults learners.","code":"\n# choose base learners\nlrnr_mean <- make_learner(Lrnr_mean)\nlrnr_rf <- make_learner(Lrnr_ranger)\n\n# define metalearners appropriate to data types\nls_metalearner <- make_learner(Lrnr_nnls)\nmn_metalearner <- make_learner(\n  Lrnr_solnp, metalearner_linear_multinomial,\n  loss_loglik_multinomial\n)\nsl_Y <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = ls_metalearner\n)\nsl_A <- Lrnr_sl$new(\n  learners = list(lrnr_mean, lrnr_rf),\n  metalearner = mn_metalearner\n)\nlearner_list <- list(A = sl_A, Y = sl_Y)"},{"path":"tmle3.html","id":"fit-the-tmle","chapter":"4 The TMLE Framework","heading":"4.5.6 Fit the TMLE","text":"now everything need fit tmle using tmle3:","code":"\ntmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)\nprint(tmle_fit)\n#> A tmle3_Fit that took 1 step(s)\n#>    type                                    param  init_est tmle_est       se\n#> 1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.005231  0.00812 0.050679\n#>        lower   upper psi_transformed lower_transformed upper_transformed\n#> 1: -0.091208 0.10745         0.00812         -0.091208           0.10745"},{"path":"tmle3.html","id":"evaluate-the-estimates","chapter":"4 The TMLE Framework","heading":"4.5.7 Evaluate the Estimates","text":"can see summary results printing fit object. Alternatively, \ncan extra results summary indexing :","code":"\nestimates <- tmle_fit$summary$psi_transformed\nprint(estimates)\n#> [1] 0.00812"},{"path":"tmle3.html","id":"tmle3-components","chapter":"4 The TMLE Framework","heading":"4.6 tmle3 Components","text":"Now ’ve successfully used spec obtain TML estimate, let’s look\nhood components. spec number functions \ngenerate objects necessary define fit TMLE.","code":""},{"path":"tmle3.html","id":"tmle3_task","chapter":"4 The TMLE Framework","heading":"4.6.1 tmle3_task","text":"First , tmle3_Task, analogous sl3_Task, containing data ’re\nfitting TMLE , well NPSEM generated node_list\ndefined , describing variables relationships.","code":"\ntmle_task <- ate_spec$make_tmle_task(washb_data, node_list)\ntmle_task$npsem\n#> $W\n#> tmle3_Node: W\n#>  Variables: month, aged, sex, momedu, hfiacat, Nlt18, Ncomp, watmin, elec, floor, walls, roof, asset_wardrobe, asset_table, asset_chair, asset_khat, asset_chouki, asset_tv, asset_refrig, asset_bike, asset_moto, asset_sewmach, asset_mobile, momage, momheight, delta_momage, delta_momheight\n#>  Parents: \n#> \n#> $A\n#> tmle3_Node: A\n#>  Variables: tr\n#>  Parents: W\n#> \n#> $Y\n#> tmle3_Node: Y\n#>  Variables: whz\n#>  Parents: A, W"},{"path":"tmle3.html","id":"initial-likelihood","chapter":"4 The TMLE Framework","heading":"4.6.2 Initial Likelihood","text":"Next, object representing likelihood, factorized according \nNPSEM described :components likelihood indicate factors estimated: \nmarginal distribution \\(W\\) estimated using NP-MLE, conditional\ndistributions \\(\\) \\(Y\\) estimated using sl3 fits (defined \nlearner_list) .can use tandem tmle_task object obtain likelihood\nestimates observation:","code":"\ninitial_likelihood <- ate_spec$make_initial_likelihood(\n  tmle_task,\n  learner_list\n)\nprint(initial_likelihood)\n#> W: Lf_emp\n#> A: LF_fit\n#> Y: LF_fit\ninitial_likelihood$get_likelihoods(tmle_task)\n#>                W       A        Y\n#>    1: 0.00021299 0.34925 -0.35834\n#>    2: 0.00021299 0.36117 -0.93261\n#>    3: 0.00021299 0.34740 -0.80873\n#>    4: 0.00021299 0.34248 -0.94020\n#>    5: 0.00021299 0.34134 -0.57866\n#>   ---                            \n#> 4691: 0.00021299 0.23375 -0.58997\n#> 4692: 0.00021299 0.23366 -0.22769\n#> 4693: 0.00021299 0.22660 -0.74235\n#> 4694: 0.00021299 0.28944 -0.91796\n#> 4695: 0.00021299 0.19533 -1.03878"},{"path":"tmle3.html","id":"targeted-likelihood-updater","chapter":"4 The TMLE Framework","heading":"4.6.3 Targeted Likelihood (updater)","text":"also need define “Targeted Likelihood” object. special type\nlikelihood able updated using tmle3_Update object. \nobject defines update strategy (e.g., submodel, loss function, CV-TMLE \n).constructing targeted likelihood, can specify different update\noptions. See documentation tmle3_Update details different\noptions. example, can disable CV-TMLE (default tmle3) \nfollows:","code":"\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\ntargeted_likelihood_no_cv <-\n  Targeted_Likelihood$new(initial_likelihood,\n    updater = list(cvtmle = FALSE)\n  )"},{"path":"tmle3.html","id":"parameter-mapping","chapter":"4 The TMLE Framework","heading":"4.6.4 Parameter Mapping","text":"Finally, need define parameters interest. , spec defines \nsingle parameter, ATE. next section, ’ll see add additional\nparameters.","code":"\ntmle_params <- ate_spec$make_params(tmle_task, targeted_likelihood)\nprint(tmle_params)\n#> [[1]]\n#> Param_ATE: ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}]"},{"path":"tmle3.html","id":"putting-it-all-together","chapter":"4 The TMLE Framework","heading":"4.6.5 Putting it all together","text":"used spec manually generate components, can now\nmanually fit tmle3:result equivalent fitting using tmle3 function .","code":"\ntmle_fit_manual <- fit_tmle3(\n  tmle_task, targeted_likelihood, tmle_params,\n  targeted_likelihood$updater\n)\nprint(tmle_fit_manual)\n#> A tmle3_Fit that took 1 step(s)\n#>    type                                    param   init_est tmle_est       se\n#> 1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0045451  0.01174 0.050807\n#>       lower   upper psi_transformed lower_transformed upper_transformed\n#> 1: -0.08784 0.11132         0.01174          -0.08784           0.11132"},{"path":"tmle3.html","id":"fitting-tmle3-with-multiple-parameters","chapter":"4 The TMLE Framework","heading":"4.7 Fitting tmle3 with multiple parameters","text":", fit tmle3 just one parameter. tmle3 also supports fitting\nmultiple parameters simultaneously. illustrate , ’ll use \ntmle_TSM_all spec:spec generates Treatment Specific Mean (TSM) level \nexposure variable. Note must first generate new targeted likelihood,\nold one targeted ATE. However, can recycle initial\nlikelihood fit , saving us super learner step.","code":"\ntsm_spec <- tmle_TSM_all()\ntargeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)\nall_tsm_params <- tsm_spec$make_params(tmle_task, targeted_likelihood)\nprint(all_tsm_params)\n#> [[1]]\n#> Param_TSM: E[Y_{A=Control}]\n#> \n#> [[2]]\n#> Param_TSM: E[Y_{A=Handwashing}]\n#> \n#> [[3]]\n#> Param_TSM: E[Y_{A=Nutrition}]\n#> \n#> [[4]]\n#> Param_TSM: E[Y_{A=Nutrition + WSH}]\n#> \n#> [[5]]\n#> Param_TSM: E[Y_{A=Sanitation}]\n#> \n#> [[6]]\n#> Param_TSM: E[Y_{A=WSH}]\n#> \n#> [[7]]\n#> Param_TSM: E[Y_{A=Water}]"},{"path":"tmle3.html","id":"delta-method","chapter":"4 The TMLE Framework","heading":"4.7.1 Delta Method","text":"can also define parameters based Delta Method Transformations \nparameters. instance, can estimate ATE using delta method two\nTSM parameters:can similarly used estimate derived parameters like Relative\nRisks, Population Attributable Risks","code":"\nate_param <- define_param(\n  Param_delta, targeted_likelihood,\n  delta_param_ATE,\n  list(all_tsm_params[[1]], all_tsm_params[[4]])\n)\nprint(ate_param)\n#> Param_delta: E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}]"},{"path":"tmle3.html","id":"fit","chapter":"4 The TMLE Framework","heading":"4.7.2 Fit","text":"can now fit TMLE simultaneously TSM parameters, well \ndefined ATE parameter","code":"\nall_params <- c(all_tsm_params, ate_param)\n\ntmle_fit_multiparam <- fit_tmle3(\n  tmle_task, targeted_likelihood, all_params,\n  targeted_likelihood$updater\n)\n\nprint(tmle_fit_multiparam)\n#> A tmle3_Fit that took 1 step(s)\n#>    type                                       param   init_est  tmle_est\n#> 1:  TSM                            E[Y_{A=Control}] -0.5959678 -0.620830\n#> 2:  TSM                        E[Y_{A=Handwashing}] -0.6188184 -0.660230\n#> 3:  TSM                          E[Y_{A=Nutrition}] -0.6111402 -0.606586\n#> 4:  TSM                    E[Y_{A=Nutrition + WSH}] -0.6005128 -0.608949\n#> 5:  TSM                         E[Y_{A=Sanitation}] -0.5857464 -0.578472\n#> 6:  TSM                                E[Y_{A=WSH}] -0.5205610 -0.448252\n#> 7:  TSM                              E[Y_{A=Water}] -0.5657364 -0.537709\n#> 8:  ATE E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}] -0.0045451  0.011881\n#>          se     lower    upper psi_transformed lower_transformed\n#> 1: 0.029901 -0.679435 -0.56223       -0.620830         -0.679435\n#> 2: 0.041719 -0.741998 -0.57846       -0.660230         -0.741998\n#> 3: 0.042047 -0.688996 -0.52418       -0.606586         -0.688996\n#> 4: 0.041285 -0.689867 -0.52803       -0.608949         -0.689867\n#> 5: 0.042396 -0.661566 -0.49538       -0.578472         -0.661566\n#> 6: 0.045506 -0.537442 -0.35906       -0.448252         -0.537442\n#> 7: 0.039253 -0.614644 -0.46077       -0.537709         -0.614644\n#> 8: 0.050801 -0.087688  0.11145        0.011881         -0.087688\n#>    upper_transformed\n#> 1:          -0.56223\n#> 2:          -0.57846\n#> 3:          -0.52418\n#> 4:          -0.52803\n#> 5:          -0.49538\n#> 6:          -0.35906\n#> 7:          -0.46077\n#> 8:           0.11145"},{"path":"tmle3.html","id":"exercises","chapter":"4 The TMLE Framework","heading":"4.8 Exercises","text":"","code":""},{"path":"tmle3.html","id":"tmle3-ex1","chapter":"4 The TMLE Framework","heading":"4.8.1 Estimation of the ATE with tmle3","text":"Follow steps estimate average treatment effect using data \nCollaborative Perinatal Project (CPP), available sl3 package. \nsimplify example, define binary intervention variable, parity01 –\nindicator one children current child \nbinary outcome, haz01 – indicator average height \nage.Define variable roles \\((W,,Y)\\) creating list nodes.\nInclude following baseline covariates \\(W\\): apgar1, apgar5,\ngagebrth, mage, meducyrs, sexn. \\(\\) \\(Y\\) specified\n.Define tmle3_Spec object ATE, tmle_ATE().Using base learning libraries defined , specify sl3 base\nlearners estimation \\(\\overline{Q}_0 = \\mathbb{E}_0(Y \\mid ,Y)\\) \n\\(g_0 = \\mathbb{P}(= 1 \\mid W)\\).Define metalearner like .Define one super learner estimating \\(\\overline{Q}_0\\) another \nestimating \\(g_0\\). Use metalearner super learners.Create list two super learners defined step call\nobject learner_list. list names (defining super\nlearner estimation \\(g_0\\)) Y (defining super learner \nestimation \\(\\overline{Q}_0\\)).Fit TMLE tmle3 function specifying (1) tmle3_Spec,\ndefined Step 2; (2) data; (3) list nodes, \nspecified Step 1; (4) list super learners estimation \n\\(g_0\\) \\(\\overline{Q}_0\\), defined Step 6. Note: Like ,\nneed explicitly make copy data (work around\ndata.table optimizations), e.g., (cpp2 <- data.table::copy(cpp)), \nuse cpp2 data going forward.","code":"\n# load the data set\ndata(cpp)\ncpp <- cpp %>%\n  as_tibble() %>%\n  dplyr::filter(!is.na(haz)) %>%\n  mutate(\n    parity01 = as.numeric(parity > 0),\n    haz01 = as.numeric(haz > 0)\n  )\nmetalearner <- make_learner(\n  Lrnr_solnp,\n  loss_function = loss_loglik_binomial,\n  learner_function = metalearner_logistic_binomial\n)"},{"path":"tmle3.html","id":"tmle3-ex2","chapter":"4 The TMLE Framework","heading":"4.8.2 Estimation of Strata-Specific ATEs with tmle3","text":"exercise, work random sample 5,000 patients \nparticipated International Stroke Trial (IST). data described \nChapter 3.2 tlverse handbook. included data \nsummarized description relevant exercise.outcome, \\(Y\\), indicates recurrent ischemic stroke within 14 days \nrandomization (DRSISC); treatment interest, \\(\\), randomized\naspirin vs. aspirin treatment allocation (RXASP ist); \nadjustment set, \\(W\\), consists simply variables measured baseline. \ndata, outcome occasionally missing, need create \nvariable indicating missingness (\\(\\Delta\\)) analyses \ntlverse, since missingness automatically detected NA present\noutcome. Covariates missing values (RATRIAL, RASP3 RHEP24)\nalready imputed. Additional covariates created\n(MISSING_RATRIAL_RASP3 MISSING_RHEP24), indicate whether \ncovariate imputed. missingness identical RATRIAL \nRASP3, one covariate indicating imputation two\ncovariates created.Estimate average effect randomized asprin treatment (RXASP = 1) \nrecurrent ischemic stroke. Even though missingness mechanism \\(Y\\),\n\\(\\Delta\\), need specified node list, still need\naccounted TMLE. words, estimation problem,\n\\(\\Delta\\) relevant factor likelihood. Thus, defining \nlist sl3 learners likelihood factor, sure include list\nlearners estimation \\(\\Delta\\), say sl_Delta, specify \nlearner list, like \nlearner_list <- list(= sl_A, delta_Y = sl_Delta, Y = sl_Y).Recall RCT conducted internationally. Suposse concern\ndose asprin may varied across geographical regions, \naverage across geographical regions may warranted. Calculate \nstrata specific ATEs according geographical region (REGION).","code":"\nist_data <- fread(\n  paste0(\n    \"https://raw.githubusercontent.com/tlverse/deming2019-workshop/\",\n    \"master/data/ist_sample.csv\"\n  )\n)"},{"path":"tmle3.html","id":"summary","chapter":"4 The TMLE Framework","heading":"4.9 Summary","text":"tmle3 general purpose framework generating TML estimates. easiest\nway use use predefined spec, allowing just fill \nblanks data, variable roles, sl3 learners. However, digging \nhood allows users specify wide range TMLEs. next sections,\n’ll see framework can used estimate advanced parameters \noptimal treatments stochastic shift interventions.","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"optimal-individualized-treatment-regimes-optional","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5 Optimal Individualized Treatment Regimes (optional)","text":"Ivana MalenicaBased tmle3mopttx R package\nIvana Malenica, Jeremy Coyle, Mark van der Laan.Updated: 2021-05-20","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"learning-objectives-3","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.1 Learning Objectives","text":"end lesson able :Differentiate dynamic optimal dynamic treatment interventions static\ninterventions.Explain benefits challenges associated using optimal\nindividualized treatment regimes practice.Contrast impact implementing optimal individualized treatment\nregime population impact implementing static dynamic\ntreatment regimes population.Estimate causal effects optimal individualized treatment regimes \ntmle3mopttx R package.Implement optimal individualized treatment rules based sub-optimal\nrules, “simple” rules, recognize practical benefit rules.Construct “realistic” optimal individualized treatment regimes respect\nreal data subject-matter knowledge limitations interventions \nconsidering interventions supported data.Measure variable importance defined terms optimal individualized\ntreatment interventions.","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"introduction-to-optimal-individualized-interventions","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.2 Introduction to Optimal Individualized Interventions","text":"Identifying intervention effective patient\nbased lifestyle, genetic environmental factors common goal \nprecision medicine. One opts administer intervention individuals\nbenefit , instead assigning treatment population level.aim motivates different type intervention, opposed static\nexposures might used .aim motivates different type intervention, opposed static\nexposures might used .chapter, learn dynamic\n(individualized) interventions tailor treatment decision based \ncollected covariates.chapter, learn dynamic\n(individualized) interventions tailor treatment decision based \ncollected covariates.statistics community, treatment strategy termed\nindividualized treatment regimes (ITR), (counterfactual)\npopulation mean outcome ITR value ITR.statistics community, treatment strategy termed\nindividualized treatment regimes (ITR), (counterfactual)\npopulation mean outcome ITR value ITR.Even , suppose one wishes maximize population mean outcome,\nindividual access set measured covariates.\nITR maximal value referred optimal ITR \noptimal individualized treatment. Consequently, value optimal\nITR termed optimal value, mean optimal\nindividualized treatment.Even , suppose one wishes maximize population mean outcome,\nindividual access set measured covariates.\nITR maximal value referred optimal ITR \noptimal individualized treatment. Consequently, value optimal\nITR termed optimal value, mean optimal\nindividualized treatment.One opts administer intervention individuals profit \n, instead assigning treatment population level. know\nintervention works patient?One opts administer intervention individuals profit \n, instead assigning treatment population level. know\nintervention works patient?example, one might seek improve retention HIV care. randomized\nclinical trial, several interventions show efficacy- including appointment\nreminders text messages, small cash incentives time clinic\nvisits, peer health workers.example, one might seek improve retention HIV care. randomized\nclinical trial, several interventions show efficacy- including appointment\nreminders text messages, small cash incentives time clinic\nvisits, peer health workers.Ideally, want improve effectiveness assigning patient \nintervention likely benefit , well improve\nefficiency allocating resources individuals need ,\nbenefit intervention.Ideally, want improve effectiveness assigning patient \nintervention likely benefit , well improve\nefficiency allocating resources individuals need ,\nbenefit intervention.\nFIGURE 5.1: Illustration Dynamic Treatment Regime Clinical Setting\naim motivates different type intervention, opposed static exposures \nmight used .chapter, examine multiple examples optimal individualized\ntreatment regimes\nestimate mean outcome ITR\ncandidate rules restricted depend user-supplied subset \nbaseline covariates.chapter, examine multiple examples optimal individualized\ntreatment regimes\nestimate mean outcome ITR\ncandidate rules restricted depend user-supplied subset \nbaseline covariates.order accomplish , present tmle3mopttx R\npackage, features \nimplementation recently developed algorithm computing targeted minimum\nloss-based estimates causal effect based optimal ITR \ncategorical treatment.order accomplish , present tmle3mopttx R\npackage, features \nimplementation recently developed algorithm computing targeted minimum\nloss-based estimates causal effect based optimal ITR \ncategorical treatment.particular, use tmle3mopttx estimate\noptimal ITR corresponding population value,\nconstruct realistic optimal ITRs, perform variable importance terms \nmean optimal individualized treatment.particular, use tmle3mopttx estimate\noptimal ITR corresponding population value,\nconstruct realistic optimal ITRs, perform variable importance terms \nmean optimal individualized treatment.","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"data-structure-and-notation","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.3 Data Structure and Notation","text":"Suppose observe \\(n\\) independent identically distributed observations \nform \\(O=(W,,Y) \\sim P_0\\). \\(P_0 \\\\mathcal{M}\\), \\(\\mathcal{M}\\) \nfully nonparametric model.Suppose observe \\(n\\) independent identically distributed observations \nform \\(O=(W,,Y) \\sim P_0\\). \\(P_0 \\\\mathcal{M}\\), \\(\\mathcal{M}\\) \nfully nonparametric model.Denote \\(\\\\mathcal{}\\) categorical treatment, \n\\(\\mathcal{} \\equiv \\{a_1, \\ldots, a_{n_A} \\}\\) \\(n_A = |\\mathcal{}|\\), \n\\(n_A\\) denoting number categories.Denote \\(\\\\mathcal{}\\) categorical treatment, \n\\(\\mathcal{} \\equiv \\{a_1, \\ldots, a_{n_A} \\}\\) \\(n_A = |\\mathcal{}|\\), \n\\(n_A\\) denoting number categories.Denote \\(Y\\) final outcome, \\(W\\) vector-valued collection baseline\ncovariates.Denote \\(Y\\) final outcome, \\(W\\) vector-valued collection baseline\ncovariates.likelihood data admits factorization, implied time ordering \\(O\\).\n\\[\\begin{equation*}\\label{eqn:likelihood_factorization}\np_0(O) = p_{Y,0}(Y|,W) p_{,0}(|W) p_{W,0}(W) = q_{Y,0}(Y|,W) q_{,0}(|W) q_{W,0}(W),\n\\end{equation*}\\]likelihood data admits factorization, implied time ordering \\(O\\).\n\\[\\begin{equation*}\\label{eqn:likelihood_factorization}\np_0(O) = p_{Y,0}(Y|,W) p_{,0}(|W) p_{W,0}(W) = q_{Y,0}(Y|,W) q_{,0}(|W) q_{W,0}(W),\n\\end{equation*}\\]Consequently, define\n\\(P_{Y,0}(Y|,W)=Q_{Y,0}(Y|,W)\\), \\(P_{,0}(|W)=g_0(|W)\\) \\(P_{W,0}(W)=Q_{W,0}(W)\\) \ncorresponding conditional distributions \\(Y\\), \\(\\) \\(W\\).Consequently, define\n\\(P_{Y,0}(Y|,W)=Q_{Y,0}(Y|,W)\\), \\(P_{,0}(|W)=g_0(|W)\\) \\(P_{W,0}(W)=Q_{W,0}(W)\\) \ncorresponding conditional distributions \\(Y\\), \\(\\) \\(W\\).also define \\(\\bar{Q}_{Y,0}(,W) \\equiv E_0[Y|,W]\\).also define \\(\\bar{Q}_{Y,0}(,W) \\equiv E_0[Y|,W]\\).Finally, denote \\(V\\) subset baseline covariates \\(W\\) \noptimal individualized rule depends .Finally, denote \\(V\\) subset baseline covariates \\(W\\) \noptimal individualized rule depends .","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"defining-the-causal-effect-of-an-optimal-individualized-intervention","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.4 Defining the Causal Effect of an Optimal Individualized Intervention","text":"Consider dynamic treatment rules \\(V \\rightarrow d(V) \\\\{a_1, \\ldots, a_{n_A} \\} \\times \\{1\\}\\),\nassigning treatment \\(\\) based \\(V\\).Consider dynamic treatment rules \\(V \\rightarrow d(V) \\\\{a_1, \\ldots, a_{n_A} \\} \\times \\{1\\}\\),\nassigning treatment \\(\\) based \\(V\\).Dynamic treatment regime may viewed intervention \n\\(\\) set equal value based hypothetical regime \\(d(V)\\), \\(Y_{d(V)}\\)\ncorresponding counterfactual outcome \\(d(V)\\).Dynamic treatment regime may viewed intervention \n\\(\\) set equal value based hypothetical regime \\(d(V)\\), \\(Y_{d(V)}\\)\ncorresponding counterfactual outcome \\(d(V)\\).goal causal analysis motivated optimal individualized\nintervention estimate parameter defined counterfactual mean outcome \nrespect modified intervention distribution.goal causal analysis motivated optimal individualized\nintervention estimate parameter defined counterfactual mean outcome \nrespect modified intervention distribution.Recall causal assumptions:Recall causal assumptions:Consistency: \\(Y^{d(v_i)}_i = Y_i\\) event \\(A_i = d(v_i)\\),\n\\(= 1, \\ldots, n\\).Stable unit value treatment assumption (SUTVA): \\(Y^{d(v_i)}_i\\) \ndepend \\(d(v_j)\\) \\(= 1, \\ldots, n\\) \\(j \\neq \\), lack\ninterference.Strong ignorability: \\(\\perp \\!\\!\\! \\perp Y^{d(v)} \\mid W\\), \\(\\\\mathcal{}\\).Positivity (overlap): \\(P_0(\\min_{\\\\mathcal{}} g_0(|W) > 0)=1\\), also assume non-exceptional law effect., also assume non-exceptional law effect.primarily interested value individualized rule,\n\\[E_0[Y_{d(V)}] = E_{0,W}[\\bar{Q}_{Y,0}(=d(V),W)].\\]primarily interested value individualized rule,\n\\[E_0[Y_{d(V)}] = E_{0,W}[\\bar{Q}_{Y,0}(=d(V),W)].\\]optimal rule rule maximal value:\n\\[d_{opt}(V) \\equiv \\text{argmax}_{d(V) \\\\mathcal{D}} E_0[Y_{d(V)}] \\]\n\\(\\mathcal{D}\\) represents set possible rules, \\(d\\), implied \\(V\\).optimal rule rule maximal value:\n\\[d_{opt}(V) \\equiv \\text{argmax}_{d(V) \\\\mathcal{D}} E_0[Y_{d(V)}] \\]\n\\(\\mathcal{D}\\) represents set possible rules, \\(d\\), implied \\(V\\).target causal estimand analysis :\n\\[\\psi_0 := E_0[Y_{d_{opt}(V)}] =  E_{0,W}[\\bar{Q}_{Y,0}(=d_{opt}(V),W)].\\]target causal estimand analysis :\n\\[\\psi_0 := E_0[Y_{d_{opt}(V)}] =  E_{0,W}[\\bar{Q}_{Y,0}(=d_{opt}(V),W)].\\]General, high-level idea:General, high-level idea:Learn optimal ITR using Super Learner.Learn optimal ITR using Super Learner.Estimate value cross-validated Targeted Minimum Loss-based\nEstimator (CV-TMLE).Estimate value cross-validated Targeted Minimum Loss-based\nEstimator (CV-TMLE).","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"why-cv-tmle","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.4.1 Why CV-TMLE?","text":"CV-TMLE necessary non-cross-validated TMLE biased upward \nmean outcome rule, therefore overly optimistic.CV-TMLE necessary non-cross-validated TMLE biased upward \nmean outcome rule, therefore overly optimistic.generally however, using CV-TMLE allows us freedom estimation \ntherefore greater data adaptivity, without sacrificing inference!generally however, using CV-TMLE allows us freedom estimation \ntherefore greater data adaptivity, without sacrificing inference!","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"binary-treatment","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.5 Binary Treatment","text":"estimate optimal individualized treatment regime? case \nbinary treatment, key quantity optimal ITR blip function.estimate optimal individualized treatment regime? case \nbinary treatment, key quantity optimal ITR blip function.Optimal ITR ideally assigns treatment individuals falling strata \nstratum specific average treatment effect, blip function, \npositive assign treatment individuals quantity\nnegative.Optimal ITR ideally assigns treatment individuals falling strata \nstratum specific average treatment effect, blip function, \npositive assign treatment individuals quantity\nnegative.define blip function : \\[\\bar{Q}_0(V) \\equiv E_0[Y_1-Y_0|V] \\equiv\nE_0[\\bar{Q}_{Y,0}(1,W) - \\bar{Q}_{Y,0}(0,W) | V], \\] average treatment\neffect within stratum \\(V\\).define blip function : \\[\\bar{Q}_0(V) \\equiv E_0[Y_1-Y_0|V] \\equiv\nE_0[\\bar{Q}_{Y,0}(1,W) - \\bar{Q}_{Y,0}(0,W) | V], \\] average treatment\neffect within stratum \\(V\\).Optimal individualized rule can now derived \\(d_{opt}(V) = (\\bar{Q}_{0}(V) > 0)\\).Optimal individualized rule can now derived \\(d_{opt}(V) = (\\bar{Q}_{0}(V) > 0)\\).Relying Targeted Maximum Likelihood (TML) estimator Super\nLearner estimate blip function, follow steps order \nobtain value ITR:Relying Targeted Maximum Likelihood (TML) estimator Super\nLearner estimate blip function, follow steps order \nobtain value ITR:Estimate \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\) using sl3. denote \nestimates \\(\\bar{Q}_{Y,n}(,W)\\) \\(g_n(|W)\\).Estimate \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\) using sl3. denote \nestimates \\(\\bar{Q}_{Y,n}(,W)\\) \\(g_n(|W)\\).Apply doubly robust Augmented-Inverse Probability Weighted (-IPW)\ntransform outcome, define:Apply doubly robust Augmented-Inverse Probability Weighted (-IPW)\ntransform outcome, define:\\[D_{\\bar{Q}_Y,g,}(O) \\equiv \\frac{(=)}{g(|W)} (Y-\\bar{Q}_Y(,W)) + \\bar{Q}_Y(=,W)\\]Note randomization positivity assumptions \n\\(E[D_{\\bar{Q}_Y,g,}(O) | V] = E[Y_a |V].\\) emphasize double robust nature\n-IPW transform: consistency \\(E[Y_a |V]\\) depend correct estimation\neither \\(\\bar{Q}_{Y,0}(,W)\\) \\(g_0(|W)\\). , randomized trial, \nguaranteed consistent estimate \\(E[Y_a |V]\\) even get \\(\\bar{Q}_{Y,0}(,W)\\) wrong!Using transform, can define following contrast:\\[D_{\\bar{Q}_Y,g}(O) = D_{\\bar{Q}_Y,g,=1}(O) - D_{\\bar{Q}_Y,g,=0}(O)\\]estimate blip function, \\(\\bar{Q}_{0,}(V)\\), regressing \\(D_{\\bar{Q}_Y,g}(O)\\) \\(V\\) using\nspecified sl3 library learners appropriate loss function.estimated rule \\(d(V) = \\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).estimated rule \\(d(V) = \\text{argmax}_{\\\\mathcal{}} \\bar{Q}_{0,}(V)\\).obtain inference mean outcome estimated optimal rule using CV-TMLE.obtain inference mean outcome estimated optimal rule using CV-TMLE.","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"evaluating-the-causal-effect-of-an-optimal-itr-with-binary-treatment","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.5.1 Evaluating the Causal Effect of an optimal ITR with Binary Treatment","text":"start, let us load packages use set seed simulation:","code":"\nlibrary(here)\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3mopttx)\nlibrary(devtools)\nset.seed(116)"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"simulate-data","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.5.1.1 Simulate Data","text":"data generating distribution following form:\\[W \\sim \\mathcal{N}(\\bf{0},I_{3 \\times 3})\\]\n\\[P(=1|W) = \\frac{1}{1+\\exp^{(-0.8*W_1)}}\\]\n\\[P(Y=1|,W) = 0.5\\text{logit}^{-1}[-5I(=1)(W_1-0.5)+5I(=0)(W_1-0.5)] +\n0.5\\text{logit}^{-1}(W_2W_3)\\]composes observed data structure \\(O = (W, , Y)\\).composes observed data structure \\(O = (W, , Y)\\).Note mean true optimal rule \\(\\psi_0=0.578\\) data\ngenerating distribution.Note mean true optimal rule \\(\\psi_0=0.578\\) data\ngenerating distribution.Next, specify role variable data set plays \nnodes DAG.Next, specify role variable data set plays \nnodes DAG.now observed data structure (data), specification \nrole variable data set plays nodes DAG.","code":"\ndata(\"data_bin\")\n# organize data and nodes for tmle3\ndata <- data_bin\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\"),\n  A = \"A\",\n  Y = \"Y\"\n)"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"constructing-optimal-stacked-regressions-with-sl3","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.5.1.2 Constructing Optimal Stacked Regressions with sl3","text":"generate three different ensemble learners must fit, corresponding\nlearners outcome regression, propensity score, blip\nfunction.make explicit respect standard\nnotation bundling ensemble learners list object :","code":"\n# Define sl3 library and metalearners:\nlrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)\nlrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)\nlrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)\nlrn_mean <- Lrnr_mean$new()\nlrn_glm <- Lrnr_glm_fast$new()\n\n## Define the Q learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(\n    lrn_xgboost_50, lrn_xgboost_100,\n    lrn_xgboost_500, lrn_mean, lrn_glm\n  ),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the g learner:\ng_learner <- Lrnr_sl$new(\n  learners = list(lrn_xgboost_100, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n## Define the B learner:\nb_learner <- Lrnr_sl$new(\n  learners = list(\n    lrn_xgboost_50, lrn_xgboost_100,\n    lrn_xgboost_500, lrn_mean, lrn_glm\n  ),\n  metalearner = Lrnr_nnls$new()\n)\n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.5.1.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"start, initialize specification TMLE parameter \ninterest simply calling tmle3_mopttx_blip_revere.start, initialize specification TMLE parameter \ninterest simply calling tmle3_mopttx_blip_revere.specify argument V = c(\"W1\", \"W2\", \"W3\") initializing \ntmle3_Spec object order communicate ’re interested learning\nrule dependent V covariates.specify argument V = c(\"W1\", \"W2\", \"W3\") initializing \ntmle3_Spec object order communicate ’re interested learning\nrule dependent V covariates.also need specify type blip use estimation\nproblem, list learners used estimate relevant parts \nlikelihood blip function.also need specify type blip use estimation\nproblem, list learners used estimate relevant parts \nlikelihood blip function.addition, need specify whether want maximize minimize \nmean outcome rule (maximize=TRUE).addition, need specify whether want maximize minimize \nmean outcome rule (maximize=TRUE).complex=FALSE, tmle3mopttx consider possible rules \nsmaller set covariates including static rules, optimize mean\noutcome suboptimal rules dependent \\(V\\).complex=FALSE, tmle3mopttx consider possible rules \nsmaller set covariates including static rules, optimize mean\noutcome suboptimal rules dependent \\(V\\).realistic=TRUE, treatments supported data considered,\ntherefore alleviating concerns regarding practical positivity issues.realistic=TRUE, treatments supported data considered,\ntherefore alleviating concerns regarding practical positivity issues.can see confidence interval covers true mean true optimal\nindividualized treatment!","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\"), type = \"blip1\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)\n# fit the TML estimator\nfit <- tmle3(tmle_spec, data, node_list, learner_list)\nfit\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=NULL}]  0.43164  0.55855 0.027047 0.50554 0.61156\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.55855           0.50554           0.61156"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"categorical-treatment","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.6 Categorical Treatment","text":"QUESTION: Can still use blip function treatment \ncategorical?section, consider evaluate mean outcome \noptimal individualized treatment \\(\\) two categories!section, consider evaluate mean outcome \noptimal individualized treatment \\(\\) two categories!define pseudo-blips vector valued entities output \ngiven \\(V\\) vector length equal number treatment categories,\n\\(n_A\\). , define : \\[\\bar{Q}_0^{pblip}(V) =\n\\{\\bar{Q}_{0,}^{pblip}(V): \\\\mathcal{} \\}\\]define pseudo-blips vector valued entities output \ngiven \\(V\\) vector length equal number treatment categories,\n\\(n_A\\). , define : \\[\\bar{Q}_0^{pblip}(V) =\n\\{\\bar{Q}_{0,}^{pblip}(V): \\\\mathcal{} \\}\\]implement three different pseudo-blips tmle3mopttx.implement three different pseudo-blips tmle3mopttx.Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference: \\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv E_0(Y_a-Y_0|V)\\]Blip1 corresponds choosing reference category treatment, \ndefining blip categories relative specified\nreference: \\[\\bar{Q}_{0,}^{pblip-ref}(V) \\equiv E_0(Y_a-Y_0|V)\\]Blip2 approach corresponds defining blip relative average\ncategories:\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\\mathcal{}} Y_a|V)\\]Blip2 approach corresponds defining blip relative average\ncategories:\n\\[\\bar{Q}_{0,}^{pblip-avg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A} \\sum_{\\\\mathcal{}} Y_a|V)\\]Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A}\n\\sum_{\\\\mathcal{}} Y_{} P(=|V)|V)\\]Blip3 reflects extension Blip2, average now weighted\naverage:\n\\[\\bar{Q}_{0,}^{pblip-wavg}(V) \\equiv E_0(Y_a- \\frac{1}{n_A}\n\\sum_{\\\\mathcal{}} Y_{} P(=|V)|V)\\]","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"evaluating-the-causal-effect-of-an-optimal-itr-with-categorical-treatment","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.6.1 Evaluating the Causal Effect of an optimal ITR with Categorical Treatment","text":"procedure analogous previously described binary treatment,\nnow need pay attention type blip define estimation\nstage, well construct learners.","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"simulated-data","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.6.1.1 Simulated Data","text":"First, load simulated data. , data generating distribution \nfollowing form:\\[W \\sim \\mathcal{N}(\\bf{0},I_{4 \\times 4})\\]\n\\[P(|W) = \\frac{1}{1+\\exp^{(-(0.05*(=1)*W_1+0.8*(=2)*W_1+0.8*(=3)*W_1))}}\\]\\[P(Y|,W) = 0.5\\text{logit}^{-1}[15I(=1)(W_1-0.5) - 3I(=2)(2W_1+0.5) \\\\\n+ 3I(=3)(3W_1-0.5)] +\\text{logit}^{-1}(W_2W_1)\\]can just load data available part package follows:composes observed data structure \\(O = (W, , Y)\\). Note \nmean true optimal rule \\(\\psi_0=0.658\\), quantity \naim estimate.","code":"\ndata(\"data_cat_realistic\")\n# organize data and nodes for tmle3\ndata <- data_cat_realistic\nnode_list <- list(\n  W = c(\"W1\", \"W2\", \"W3\", \"W4\"),\n  A = \"A\",\n  Y = \"Y\"\n)"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"constructing-optimal-stacked-regressions-with-sl3-1","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.6.1.2 Constructing Optimal Stacked Regressions with sl3","text":"QUESTION: categorical treatment, dimension blip now?\ngo estimating ?generate three different ensemble learners must fit,\ncorresponding learners outcome regression, propensity score, \nblip function.generate three different ensemble learners must fit,\ncorresponding learners outcome regression, propensity score, \nblip function.Note need estimate \\(g_0(|W)\\) categorical \\(\\)- therefore\nuse multinomial Super Learner option available within sl3 package learners\ncan address multi-class classification problems.Note need estimate \\(g_0(|W)\\) categorical \\(\\)- therefore\nuse multinomial Super Learner option available within sl3 package learners\ncan address multi-class classification problems.order see learners can\nused estimate \\(g_0(|W)\\) sl3, run following:order see learners can\nused estimate \\(g_0(|W)\\) sl3, run following:","code":"\n# Initialize few of the learners:\nlrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)\nlrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)\nlrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)\nlrn_mean <- Lrnr_mean$new()\nlrn_glm <- Lrnr_glm_fast$new()\n\n## Define the Q learner, which is just a regular learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean, lrn_glm),\n  metalearner = Lrnr_nnls$new()\n)\n\n# Define the g learner, which is a multinomial learner:\n# specify the appropriate loss of the multinomial learner:\nmn_metalearner <- make_learner(Lrnr_solnp,\n  loss_function = loss_loglik_multinomial,\n  learner_function = metalearner_linear_multinomial\n)\ng_learner <- make_learner(Lrnr_sl, list(lrn_xgboost_100, lrn_xgboost_500, lrn_mean), mn_metalearner)\n\n# Define the Blip learner, which is a multivariate learner:\nlearners <- list(lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean, lrn_glm)\nb_learner <- create_mv_learners(learners = learners)\n# See which learners support multi-class classification:\nsl3_list_learners(c(\"categorical\"))\n#>  [1] \"Lrnr_bound\"                \"Lrnr_caret\"               \n#>  [3] \"Lrnr_cv_selector\"          \"Lrnr_glmnet\"              \n#>  [5] \"Lrnr_grf\"                  \"Lrnr_gru_keras\"           \n#>  [7] \"Lrnr_h2o_glm\"              \"Lrnr_h2o_grid\"            \n#>  [9] \"Lrnr_independent_binomial\" \"Lrnr_lightgbm\"            \n#> [11] \"Lrnr_lstm_keras\"           \"Lrnr_mean\"                \n#> [13] \"Lrnr_multivariate\"         \"Lrnr_nnet\"                \n#> [15] \"Lrnr_optim\"                \"Lrnr_polspline\"           \n#> [17] \"Lrnr_pooled_hazards\"       \"Lrnr_randomForest\"        \n#> [19] \"Lrnr_ranger\"               \"Lrnr_rpart\"               \n#> [21] \"Lrnr_screener_correlation\" \"Lrnr_solnp\"               \n#> [23] \"Lrnr_svm\"                  \"Lrnr_xgboost\"\n# specify outcome and treatment regressions and create learner list\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.6.1.3 Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects","text":"can see confidence interval covers\ntrue mean true optimal individualized treatment.NOTICE distribution assigned treatment! need shortly.","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"W1\", \"W2\", \"W3\", \"W4\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)\n# fit the TML estimator\nfit <- tmle3(tmle_spec, data, node_list, learner_list)\nfit\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=NULL}]  0.54158  0.65915 0.068054 0.52577 0.79254\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.65915           0.52577           0.79254\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec$return_rule)\n#> \n#>   1   2   3 \n#> 428 394 178"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"extensions-to-causal-effect-of-an-oit","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.7 Extensions to Causal Effect of an OIT","text":"consider two extensions procedure described \nestimating value ITR.consider two extensions procedure described \nestimating value ITR.first one considers setting user\nmight interested grid possible suboptimal rules, corresponding \npotentially limited knowledge potential effect modifiers (Simpler Rules).first one considers setting user\nmight interested grid possible suboptimal rules, corresponding \npotentially limited knowledge potential effect modifiers (Simpler Rules).second extension concerns implementation realistic optimal individual\ninterventions certain regimes might preferred, due practical \nglobal positivity restraints realistic implement (Realistic Interventions).second extension concerns implementation realistic optimal individual\ninterventions certain regimes might preferred, due practical \nglobal positivity restraints realistic implement (Realistic Interventions).","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"simpler-rules","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.7.1 Simpler Rules","text":"order consider ambitious fully \\(V\\)-optimal rule, \ndefine \\(S\\)-optimal rules optimal rule considers possible subsets\n\\(V\\) covariates, card(\\(S\\)) \\(\\leq\\) card(\\(V\\)) \\(\\emptyset \\S\\).order consider ambitious fully \\(V\\)-optimal rule, \ndefine \\(S\\)-optimal rules optimal rule considers possible subsets\n\\(V\\) covariates, card(\\(S\\)) \\(\\leq\\) card(\\(V\\)) \\(\\emptyset \\S\\).allows us consider sub-optimal rules easier estimate \npotentially provide realistic rules- , allow statistical\ninference counterfactual mean outcome sub-optimal rule.allows us consider sub-optimal rules easier estimate \npotentially provide realistic rules- , allow statistical\ninference counterfactual mean outcome sub-optimal rule.Even though user specified baseline covariates basis\nrule estimation, simpler rule sufficient \nmaximize mean optimal individualized treatment!QUESTION: set covariates picked tmle3mopttx\ncompare baseline covariates true rule depends ?","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = FALSE, realistic = FALSE\n)\n# fit the TML estimator\nfit <- tmle3(tmle_spec, data, node_list, learner_list)\nfit\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{d(V=1)}]  0.48614  0.53913 0.093273 0.35632 0.72194\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.53913           0.35632           0.72194"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"realistic-optimal-individual-regimes","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.7.2 Realistic Optimal Individual Regimes","text":"tmle3mopttx also provides option estimate mean \nrealistic, implementable, optimal individualized treatment.tmle3mopttx also provides option estimate mean \nrealistic, implementable, optimal individualized treatment.often case assigning particular regime might ability \nfully maximize (minimize) desired outcome, due \nglobal practical positivity constrains, treatment\ncan never implemented real life (highly unlikely).often case assigning particular regime might ability \nfully maximize (minimize) desired outcome, due \nglobal practical positivity constrains, treatment\ncan never implemented real life (highly unlikely).Specifying realistic=\"TRUE\", consider possibly suboptimal\ntreatments optimize outcome question \nsupported data.Specifying realistic=\"TRUE\", consider possibly suboptimal\ntreatments optimize outcome question \nsupported data.QUESTION: Referring back data-generating distribution, \nthink distribution allocated treatment changed distribution\n“non-realistic” rule?","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"W4\", \"W3\", \"W2\", \"W1\"), type = \"blip2\",\n  learners = learner_list,\n  maximize = TRUE, complex = TRUE, realistic = TRUE\n)\n# fit the TML estimator\nfit <- tmle3(tmle_spec, data, node_list, learner_list)\nfit\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est       se   lower   upper\n#> 1:  TSM E[Y_{A=NULL}]  0.54869  0.58779 0.057258 0.47557 0.70001\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.58779           0.47557           0.70001\n\n# How many individuals got assigned each treatment?\ntable(tmle_spec$return_rule)\n#> \n#>   1   2   3 \n#>   4 502 494"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"variable-importance-analysis","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.7.3 Variable Importance Analysis","text":"previous sections seen obtain contrast \nmean optimal individualized rule mean observed outcome \nsingle covariate. now ready run variable importance analysis \nobserved covariates!previous sections seen obtain contrast \nmean optimal individualized rule mean observed outcome \nsingle covariate. now ready run variable importance analysis \nobserved covariates!order run tmle3mopttx variable importance measure, \nneed considered covariates categorical variables.order run tmle3mopttx variable importance measure, \nneed considered covariates categorical variables.illustration purpose,\nbin baseline covariates corresponding data-generating distribution\ndescribed previous section:illustration purpose,\nbin baseline covariates corresponding data-generating distribution\ndescribed previous section:Note node list now includes \\(W_1\\) treatments well!\nDon’t worry, still properly adjust baseline covariates \nconsidering \\(\\) treatment.","code":"\n# bin baseline covariates to 3 categories:\ndata$W1 <- ifelse(data$W1 < quantile(data$W1)[2], 1, ifelse(data$W1 < quantile(data$W1)[3], 2, 3))\n\nnode_list <- list(\n  W = c(\"W3\", \"W4\", \"W2\"),\n  A = c(\"W1\", \"A\"),\n  Y = \"Y\"\n)"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"variable-importance-using-targeted-estimation-of-the-value-of-the-itr","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.7.3.1 Variable Importance using Targeted Estimation of the value of the ITR","text":"initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle3_mopttx_vim.final result tmle3_vim tmle3mopttx spec ordered list\nmean outcomes optimal individualized treatment categorical\ncovariates dataset.","code":"\n# initialize a tmle specification\ntmle_spec <- tmle3_mopttx_vim(\n  V = c(\"W2\"),\n  type = \"blip2\",\n  learners = learner_list,\n  contrast = \"multiplicative\",\n  maximize = FALSE,\n  method = \"SL\",\n  complex = TRUE,\n  realistic = FALSE\n)\n# fit the TML estimator\nvim_results <- tmle3_vim(tmle_spec, data, node_list, learner_list,\n  adjust_for_other_A = TRUE\n)\n\nprint(vim_results)\n#>    type                  param   init_est  tmle_est       se      lower\n#> 1:   RR RR(E[Y_{A=NULL}]/E[Y])  0.0052065  0.058406 0.034144 -0.0085144\n#> 2:   RR RR(E[Y_{A=NULL}]/E[Y]) -0.0239991 -0.067511 0.051592 -0.1686303\n#>       upper psi_transformed lower_transformed upper_transformed  A           W\n#> 1: 0.125327         1.06015           0.99152            1.1335  A W3,W4,W2,W1\n#> 2: 0.033608         0.93472           0.84482            1.0342 W1  W3,W4,W2,A\n#>     Z_stat     p_nz p_nz_corrected\n#> 1:  1.7106 0.043578       0.087156\n#> 2: -1.3085 0.095344       0.095344"},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"exercise","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.8 Exercise","text":"","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"real-world-data-and-tmle3mopttx","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.8.1 Real World Data and tmle3mopttx","text":"Finally, cement everything learned far real data application.previous sections, using WASH Benefits data,\ncorresponding Effect water quality, sanitation, hand washing, \nnutritional interventions child development rural Bangladesh trial.main aim cluster-randomized controlled trial assess \nimpact six intervention groups, including:ControlControlHandwashing soapHandwashing soapImproved nutrition counselling provision lipid-based nutrient supplementsImproved nutrition counselling provision lipid-based nutrient supplementsCombined water, sanitation, handwashing, nutrition.Combined water, sanitation, handwashing, nutrition.Improved sanitationImproved sanitationCombined water, sanitation, handwashingCombined water, sanitation, handwashingChlorinated drinking waterChlorinated drinking waterWe aim estimate optimal ITR corresponding value optimal ITR\nmain intervention WASH Benefits data!outcome interest weight--height Z-score, whereas treatment \nsix intervention groups aimed improving living conditions.Questions:Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nthink use covariates \\(V\\)?\nwant minimize maximize outcome? blip type use?\nConstruct appropriate sl3 library \\(\\), \\(Y\\) \\(B\\).Define \\(V\\) mother’s education (momedu), current living conditions (floor),\npossession material items including refrigerator (asset_refrig).\nthink use covariates \\(V\\)?\nwant minimize maximize outcome? blip type use?\nConstruct appropriate sl3 library \\(\\), \\(Y\\) \\(B\\).Based \\(V\\) defined previous question, estimate mean ITR \nmain randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value optimal ITR?\nchange initial estimate? intervention dominant?\nthink ?Based \\(V\\) defined previous question, estimate mean ITR \nmain randomized intervention used WASH Benefits trial\nweight--height Z-score outcome. ’s TMLE value optimal ITR?\nchange initial estimate? intervention dominant?\nthink ?Using formulation questions 1 2, estimate realistic optimal ITR\ncorresponding value realistic ITR. results change? intervention\ndominant realistic rules? think ?Using formulation questions 1 2, estimate realistic optimal ITR\ncorresponding value realistic ITR. results change? intervention\ndominant realistic rules? think ?","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"summary-1","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.9 Summary","text":"summary, mean outcome optimal individualized treatment counterfactual\nquantity interest representing mean outcome everybody, contrary\nfact, received treatment optimized outcome.summary, mean outcome optimal individualized treatment counterfactual\nquantity interest representing mean outcome everybody, contrary\nfact, received treatment optimized outcome.tmle3mopttx estimates mean outcome optimal individualized treatment, candidate\nrules restricted respond user-supplied subset baseline intermediate\ncovariates. addition provides options realistic, data-adaptive interventions.tmle3mopttx estimates mean outcome optimal individualized treatment, candidate\nrules restricted respond user-supplied subset baseline intermediate\ncovariates. addition provides options realistic, data-adaptive interventions.essence, target parameter answers key aim precision medicine: allocating\navailable treatment tailoring individual characteristics patient, \ngoal optimizing final outcome.essence, target parameter answers key aim precision medicine: allocating\navailable treatment tailoring individual characteristics patient, \ngoal optimizing final outcome.","code":""},{"path":"optimal-individualized-treatment-regimes-optional.html","id":"solutions","chapter":"5 Optimal Individualized Treatment Regimes (optional)","heading":"5.9.1 Solutions","text":"start, let’s load data, convert columns class numeric,\ntake quick look :, specify NPSEM via node_list object.pick potential effect modifiers, including mother’s education, current\nliving conditions (floor), possession material items including refrigerator.\nconcentrate covariates might indicative socio-economic status\nindividuals involved trial. can explore distribution \\(V\\), \\(\\) \\(Y\\):specify simply library outcome regression, propensity score\nblip function. Since treatment categorical, need define \nmultinomial learner \\(\\) multivariate learner \\(B\\). \ndefine xgboost grid parameters, initialize mean learner.seen , initialize tmle3_mopttx_blip_revere Spec order \nanswer Question 1. want maximize outcome, higher weight--height Z-score\nbetter. also use blip2 blip type, note used blip1\nwell since “Control” good reference category.Using formulation , estimate realistic optimal ITR\ncorresponding value realistic ITR:","code":"\nwashb_data <- fread(\"https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv\", stringsAsFactors = TRUE)\nwashb_data <- washb_data[!is.na(momage), lapply(.SD, as.numeric)]\nhead(washb_data, 3)\nnode_list <- list(\n  W = names(washb_data)[!(names(washb_data) %in% c(\"whz\", \"tr\"))],\n  A = \"tr\", Y = \"whz\"\n)\n# V1, V2 and V3:\ntable(washb_data$momedu)\ntable(washb_data$floor)\ntable(washb_data$asset_refrig)\n\n# A:\ntable(washb_data$tr)\n\n# Y:\nsummary(washb_data$whz)\n# Initialize few of the learners:\ngrid_params <- list(\n  nrounds = c(100, 500),\n  eta = c(0.01, 0.1)\n)\ngrid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)\nxgb_learners <- apply(grid, MARGIN = 1, function(params_tune) {\n  do.call(Lrnr_xgboost$new, c(as.list(params_tune)))\n})\nlrn_mean <- Lrnr_mean$new()\n\n## Define the Q learner, which is just a regular learner:\nQ_learner <- Lrnr_sl$new(\n  learners = list(\n    xgb_learners[[1]], xgb_learners[[2]], xgb_learners[[3]],\n    xgb_learners[[4]], lrn_mean\n  ),\n  metalearner = Lrnr_nnls$new()\n)\n\n# Define the g learner, which is a multinomial learner:\n# specify the appropriate loss of the multinomial learner:\nmn_metalearner <- make_learner(Lrnr_solnp,\n  loss_function = loss_loglik_multinomial,\n  learner_function = metalearner_linear_multinomial\n)\ng_learner <- make_learner(Lrnr_sl, list(xgb_learners[[4]], lrn_mean), mn_metalearner)\n\n# Define the Blip learner, which is a multivariate learner:\nlearners <- list(\n  xgb_learners[[1]], xgb_learners[[2]], xgb_learners[[3]],\n  xgb_learners[[4]], lrn_mean\n)\nb_learner <- create_mv_learners(learners = learners)\n\nlearner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)\n## Question 2:\n\n# Initialize a tmle specification\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"momedu\", \"floor\", \"asset_refrig\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = FALSE\n)\n\n# Fit the TML estimator.\nfit <- tmle3(tmle_spec, data = washb_data, node_list, learner_list)\nfit\n\n# Which intervention is the most dominant?\ntable(tmle_spec$return_rule)\n## Question 3:\n\n# Initialize a tmle specification with \"realistic=TRUE\":\ntmle_spec <- tmle3_mopttx_blip_revere(\n  V = c(\"momedu\", \"floor\", \"asset_refrig\"), type = \"blip2\",\n  learners = learner_list, maximize = TRUE, complex = TRUE,\n  realistic = TRUE\n)\n\n# Fit the TML estimator.\nfit <- tmle3(tmle_spec, data = washb_data, node_list, learner_list)\nfit\n\ntable(tmle_spec$return_rule)"},{"path":"stochastic-treatment-regimes-optional.html","id":"stochastic-treatment-regimes-optional","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6 Stochastic Treatment Regimes (optional)","text":"Nima HejaziBased tmle3shift R package\nNima Hejazi, Jeremy Coyle, Mark van der Laan.Updated: 2021-05-20","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"learning-objectives-4","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.1 Learning Objectives","text":"Differentiate stochastic treatment regimes static, dynamic, optimal\ntreatment regimes.Describe estimating causal effects stochastic interventions informs \nreal-world data analysis.Contrast population level stochastic intervention policy modified\ntreatment policy.Estimate causal effects stochastic treatment regimes \ntmle3shift R package.Specify grid counterfactual shift interventions used defining\nset stochastic intervention policies.Interpret set effect estimates grid counterfactual shift\ninterventions.Construct marginal structural models measure variable importance terms\nstochastic interventions, using grid shift interventions.Implement shift intervention individual level, facilitate\nshifting individual value ’s supported data.Define novel shift intervention functions extend tmle3shift R\npackage.","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"introduction-2","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.2 Introduction","text":"section, examine simple example stochastic treatment regimes \ncontext continuous treatment variable interest, defining \nintuitive causal effect examine stochastic interventions \ngenerally. first step using stochastic\ntreatment regimes practice, present tmle3shift R\npackage, features \nimplementation recently developed algorithm computing targeted minimum\nloss-based estimates causal effect based stochastic treatment regime\nshifts natural value treatment based shifting function\n\\(d(,W)\\). also use tmle3shift construct marginal structural models\nvariable importance measures, implement shift interventions \nindividual level, define novel shift intervention functions.","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"stochastic-interventions","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.3 Stochastic Interventions","text":"Present relatively simple yet extremely flexible manner realistic\ncausal effects (contrasts thereof) may defined.Present relatively simple yet extremely flexible manner realistic\ncausal effects (contrasts thereof) may defined.May applied nearly manner treatment variable – continuous,\nordinal, categorical, binary – allowing rich set causal effects \ndefined formalism.May applied nearly manner treatment variable – continuous,\nordinal, categorical, binary – allowing rich set causal effects \ndefined formalism.Arguably general classes interventions causal\neffects may defined, conceptually simple.Arguably general classes interventions causal\neffects may defined, conceptually simple.may consider stochastic interventions two ways:\nequation \\(f_A\\), produces \\(\\), replaced probabilistic\nmechanism \\(g_{\\delta}(\\mid W)\\) differs original \\(g(\\mid W)\\). stochastically modified value treatment \\(A_{\\delta}\\) \ndrawn user-specified distribution \\(g_\\delta(\\mid W)\\), may\ndepend original distribution \\(g(\\mid W)\\) indexed \nuser-specified parameter \\(\\delta\\). case, stochastically\nmodified value treatment \\(A_{\\delta} \\sim g_{\\delta}(\\cdot \\mid W)\\).\nobserved value \\(\\) replaced new value \\(A_{d(,W)}\\) based \napplying user-defined function \\(d(,W)\\) \\(\\). case, \nstochastic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(, W)\\), \nregime \\(d\\) depends treatment level \\(\\) assigned \nabsence regime well covariates \\(W\\). Stochastic\ninterventions variety may referred depending \nnatural value treatment modified treatment policies\n(Haneuse Rotnitzky 2013; Young, Hernán, Robins 2014).\nmay consider stochastic interventions two ways:equation \\(f_A\\), produces \\(\\), replaced probabilistic\nmechanism \\(g_{\\delta}(\\mid W)\\) differs original \\(g(\\mid W)\\). stochastically modified value treatment \\(A_{\\delta}\\) \ndrawn user-specified distribution \\(g_\\delta(\\mid W)\\), may\ndepend original distribution \\(g(\\mid W)\\) indexed \nuser-specified parameter \\(\\delta\\). case, stochastically\nmodified value treatment \\(A_{\\delta} \\sim g_{\\delta}(\\cdot \\mid W)\\).equation \\(f_A\\), produces \\(\\), replaced probabilistic\nmechanism \\(g_{\\delta}(\\mid W)\\) differs original \\(g(\\mid W)\\). stochastically modified value treatment \\(A_{\\delta}\\) \ndrawn user-specified distribution \\(g_\\delta(\\mid W)\\), may\ndepend original distribution \\(g(\\mid W)\\) indexed \nuser-specified parameter \\(\\delta\\). case, stochastically\nmodified value treatment \\(A_{\\delta} \\sim g_{\\delta}(\\cdot \\mid W)\\).observed value \\(\\) replaced new value \\(A_{d(,W)}\\) based \napplying user-defined function \\(d(,W)\\) \\(\\). case, \nstochastic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(, W)\\), \nregime \\(d\\) depends treatment level \\(\\) assigned \nabsence regime well covariates \\(W\\). Stochastic\ninterventions variety may referred depending \nnatural value treatment modified treatment policies\n(Haneuse Rotnitzky 2013; Young, Hernán, Robins 2014).observed value \\(\\) replaced new value \\(A_{d(,W)}\\) based \napplying user-defined function \\(d(,W)\\) \\(\\). case, \nstochastic treatment regime may viewed intervention \\(\\)\nset equal value based hypothetical regime \\(d(, W)\\), \nregime \\(d\\) depends treatment level \\(\\) assigned \nabsence regime well covariates \\(W\\). Stochastic\ninterventions variety may referred depending \nnatural value treatment modified treatment policies\n(Haneuse Rotnitzky 2013; Young, Hernán, Robins 2014).","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"identifying-the-causal-effect-of-a-stochastic-intervention","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.3.1 Identifying the Causal Effect of a Stochastic Intervention","text":"stochastic intervention generates counterfactual random variable\n\\(Y_{d(,W)} := f_Y(d(,W), W, U_Y) \\equiv Y_{g_{\\delta}} := f_Y(A_{\\delta}, W, U_Y)\\), \\(Y_{d(,W)} \\sim \\mathcal{P}_0^{\\delta}\\).stochastic intervention generates counterfactual random variable\n\\(Y_{d(,W)} := f_Y(d(,W), W, U_Y) \\equiv Y_{g_{\\delta}} := f_Y(A_{\\delta}, W, U_Y)\\), \\(Y_{d(,W)} \\sim \\mathcal{P}_0^{\\delta}\\).target causal estimand analysis \\(\\psi_{0, \\delta} := \\mathbb{E}_{P_0^{\\delta}}\\{Y_{d(,W)}\\}\\), mean counterfactual\noutcome variable \\(Y_{d(, W)}\\). statistical target parameter may also \ndenoted \\(\\Psi(P_0) = \\mathbb{E}_{P_0}{\\overline{Q}(d(, W), W)}\\), \n\\(\\overline{Q}(d(, W), W)\\) counterfactual outcome value given\nindividual stochastic intervention distribution\n(Dı́az van der Laan 2018).target causal estimand analysis \\(\\psi_{0, \\delta} := \\mathbb{E}_{P_0^{\\delta}}\\{Y_{d(,W)}\\}\\), mean counterfactual\noutcome variable \\(Y_{d(, W)}\\). statistical target parameter may also \ndenoted \\(\\Psi(P_0) = \\mathbb{E}_{P_0}{\\overline{Q}(d(, W), W)}\\), \n\\(\\overline{Q}(d(, W), W)\\) counterfactual outcome value given\nindividual stochastic intervention distribution\n(Dı́az van der Laan 2018).prior work, Dı́az van der Laan (2012) showed causal quantity interest\n\\(\\mathbb{E}_0 \\{Y_{d(, W)}\\}\\) identified functional \ndistribution \\(O\\):\n\\[\\begin{align*}\\label{eqn:identification2012}\n  \\psi_{0,d} = \\int_{\\mathcal{W}} \\int_{\\mathcal{}} & \\mathbb{E}_{P_0}\n   \\{Y \\mid = d(, w), W = w\\} \\cdot \\\\ &q_{0, }^O(\\mid W = w) \\cdot\n   q_{0, W}^O(w) d\\mu()d\\nu(w).\n\\end{align*}\\]prior work, Dı́az van der Laan (2012) showed causal quantity interest\n\\(\\mathbb{E}_0 \\{Y_{d(, W)}\\}\\) identified functional \ndistribution \\(O\\):\n\\[\\begin{align*}\\label{eqn:identification2012}\n  \\psi_{0,d} = \\int_{\\mathcal{W}} \\int_{\\mathcal{}} & \\mathbb{E}_{P_0}\n   \\{Y \\mid = d(, w), W = w\\} \\cdot \\\\ &q_{0, }^O(\\mid W = w) \\cdot\n   q_{0, W}^O(w) d\\mu()d\\nu(w).\n\\end{align*}\\]four standard assumptions presented  necessary order\nestablish identifiability causal parameter observed data\nvia statistical functional. \nConsistency: \\(Y^{d(a_i, w_i)}_i = Y_i\\) event \\(A_i = d(a_i, w_i)\\),\n\\(= 1, \\ldots, n\\)\nStable unit value treatment assumption (SUTVA): \\(Y^{d(a_i, w_i)}_i\\) \ndepend \\(d(a_j, w_j)\\) \\(= 1, \\ldots, n\\) \\(j \\neq \\), lack\ninterference (Rubin 1978, 1980).\nStrong ignorability: \\(A_i \\perp \\!\\!\\! \\perp Y^{d(a_i, w_i)}_i \\mid W_i\\),\n\\(= 1, \\ldots, n\\).\nPositivity (overlap): \\(a_i \\\\mathcal{} \\implies d(a_i, w_i) \\\\mathcal{}\\) \\(w \\\\mathcal{W}\\), \\(\\mathcal{}\\) denotes \nsupport \\(\\mid W = w_i \\quad \\forall = 1, \\ldots n\\).\nfour standard assumptions presented  necessary order\nestablish identifiability causal parameter observed data\nvia statistical functional. wereConsistency: \\(Y^{d(a_i, w_i)}_i = Y_i\\) event \\(A_i = d(a_i, w_i)\\),\n\\(= 1, \\ldots, n\\)Stable unit value treatment assumption (SUTVA): \\(Y^{d(a_i, w_i)}_i\\) \ndepend \\(d(a_j, w_j)\\) \\(= 1, \\ldots, n\\) \\(j \\neq \\), lack\ninterference (Rubin 1978, 1980).Strong ignorability: \\(A_i \\perp \\!\\!\\! \\perp Y^{d(a_i, w_i)}_i \\mid W_i\\),\n\\(= 1, \\ldots, n\\).Positivity (overlap): \\(a_i \\\\mathcal{} \\implies d(a_i, w_i) \\\\mathcal{}\\) \\(w \\\\mathcal{W}\\), \\(\\mathcal{}\\) denotes \nsupport \\(\\mid W = w_i \\quad \\forall = 1, \\ldots n\\).identification assumptions satisfied, Dı́az van der Laan (2012) \nDı́az van der Laan (2018) provide efficient influence function respect \nnonparametric model \\(\\mathcal{M}\\) \n\\[\\begin{equation*}\\label{eqn:eif}\n  D(P_0)(x) = H(, w)({y - \\overline{Q}(, w)}) +\n  \\overline{Q}(d(, w), w) - \\Psi(P_0),\n\\end{equation*}\\]\nauxiliary covariate \\(H(,w)\\) may expressed\n\\[\\begin{equation*}\\label{eqn:aux_covar_full}\n  H(,w) = \\mathbb{}(+ \\delta < u(w)) \\frac{g_0(- \\delta \\mid w)} {g_0(\\mid w)}\n    + \\mathbb{}(+ \\delta \\geq u(w)),\n\\end{equation*}\\]\nmay reduced \n\\[\\begin{equation*}\\label{eqn:aux_covar_simple}\n  H(,w) = \\frac{g_0(- \\delta \\mid w)}{g_0(\\mid w)} + 1\n\\end{equation*}\\]\ncase treatment limits arise conditioning\n\\(W\\), .e., \\(A_i \\(u(w) - \\delta, u(w))\\).identification assumptions satisfied, Dı́az van der Laan (2012) \nDı́az van der Laan (2018) provide efficient influence function respect \nnonparametric model \\(\\mathcal{M}\\) \n\\[\\begin{equation*}\\label{eqn:eif}\n  D(P_0)(x) = H(, w)({y - \\overline{Q}(, w)}) +\n  \\overline{Q}(d(, w), w) - \\Psi(P_0),\n\\end{equation*}\\]\nauxiliary covariate \\(H(,w)\\) may expressed\n\\[\\begin{equation*}\\label{eqn:aux_covar_full}\n  H(,w) = \\mathbb{}(+ \\delta < u(w)) \\frac{g_0(- \\delta \\mid w)} {g_0(\\mid w)}\n    + \\mathbb{}(+ \\delta \\geq u(w)),\n\\end{equation*}\\]\nmay reduced \n\\[\\begin{equation*}\\label{eqn:aux_covar_simple}\n  H(,w) = \\frac{g_0(- \\delta \\mid w)}{g_0(\\mid w)} + 1\n\\end{equation*}\\]\ncase treatment limits arise conditioning\n\\(W\\), .e., \\(A_i \\(u(w) - \\delta, u(w))\\).","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"interpreting-the-causal-effect-of-a-stochastic-intervention","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.3.2 Interpreting the Causal Effect of a Stochastic Intervention","text":"\nFIGURE 5.1: Animation counterfactual outcome changes natural treatment distribution subjected simple stochastic intervention\n","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"estimating-the-causal-effect-of-a-stochastic-intervention-with-tmle3shift","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.4 Estimating the Causal Effect of a Stochastic Intervention with tmle3shift","text":"use tmle3shift construct targeted maximum likelihood (TML) estimator \ncausal effect stochastic treatment regime shifts natural\nvalue treatment based shifting function \\(d(,W)\\). follow\nrecipe provided Dı́az van der Laan (2018), tailored tmle3 framework:Construct initial estimators \\(g_n\\) \\(g_0(, W)\\) \\(Q_n\\) \n\\(\\overline{Q}_0(, W)\\), perhaps using data-adaptive regression techniques.observation \\(\\), compute estimate \\(H_n(a_i, w_i)\\) \nauxiliary covariate \\(H(a_i,w_i)\\).Estimate parameter \\(\\epsilon\\) logistic regression model\n\\[ \\text{logit}\\overline{Q}_{\\epsilon, n}(, w) =\n\\text{logit}\\overline{Q}_n(, w) + \\epsilon H_n(, w),\\]\nalternative regression model incorporating weights.Compute TML estimator \\(\\Psi_n\\) target parameter, defining update\n\\(\\overline{Q}_n^{\\star}\\) initial estimate\n\\(\\overline{Q}_{n, \\epsilon_n}\\):\n\\[\\begin{equation*}\\label{eqn:tmle}\n  \\Psi_n = \\Psi(P_n^{\\star}) = \\frac{1}{n} \\sum_{= 1}^n\n  \\overline{Q}_n^{\\star}(d(A_i, W_i), W_i).\n\\end{equation*}\\]start, let’s load packages ’ll use set seed simulation:1. Construct initial estimators \\(g_n\\) \\(g_0(, W)\\) \\(Q_n\\) \n\\(\\overline{Q}_0(, W)\\).need estimate two components likelihood order construct \nTML estimator.outcome regression, \\(\\hat{Q}_n\\), simple regression \nform \\(\\mathbb{E}[Y \\mid ,W]\\).second estimate treatment mechanism, \\(\\hat{g}_n\\),\n.e., propensity score. case continuous intervention node\n\\(\\), quantity takes form \\(p(\\mid W)\\), conditional\ndensity. Generally speaking, conditional density estimation challenging\nproblem received much attention literature. estimate \ntreatment mechanism, must make use learning algorithms specifically\nsuited conditional density estimation; list learners may \nextracted sl3 using sl3_list_learners():proceed, ’ll select two learners, Lrnr_haldensify using\nhighly adaptive lasso conditional density estimation, based \nalgorithm given Dı́az van der Laan (2011) implemented Hejazi, Benkeser, van der Laan (2020), \nsemiparametric location-scale conditional density estimators implemented \nsl3 package. Super Learner may \nconstructed pooling estimates modified conditional\ndensity estimation techniques.Finally, construct learner_list object use constructing TML\nestimator target parameter interest:","code":"\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(sl3)\nlibrary(tmle3)\nlibrary(tmle3shift)\nset.seed(429153)\n# learners used for conditional expectation regression\nmean_learner <- Lrnr_mean$new()\nfglm_learner <- Lrnr_glm_fast$new()\nxgb_learner <- Lrnr_xgboost$new(nrounds = 200)\nsl_regression_learner <- Lrnr_sl$new(\n  learners = list(mean_learner, fglm_learner, xgb_learner)\n)\nsl3_list_learners(\"density\")\n#> [1] \"Lrnr_density_discretize\"     \"Lrnr_density_hse\"           \n#> [3] \"Lrnr_density_semiparametric\" \"Lrnr_haldensify\"            \n#> [5] \"Lrnr_solnp_density\"\n# learners used for conditional densities (i.e., generalized propensity score)\nhaldensify_learner <- Lrnr_haldensify$new(\n  n_bins = c(3, 5),\n  lambda_seq = exp(seq(-1, -10, length = 200))\n)\n# semiparametric density estimator based on homoscedastic errors (HOSE)\nhose_learner_xgb <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = xgb_learner\n)\n# semiparametric density estimator based on heteroscedastic errors (HESE)\nhese_learner_xgb_fglm <- make_learner(Lrnr_density_semiparametric,\n  mean_learner = xgb_learner,\n  var_learner = fglm_learner\n)\n# SL for the conditional treatment density\nsl_density_learner <- Lrnr_sl$new(\n  learners = list(haldensify_learner, hose_learner_xgb,\n                  hese_learner_xgb_fglm),\n  metalearner = Lrnr_solnp_density$new()\n)\nQ_learner <- sl_regression_learner\ng_learner <- sl_density_learner\nlearner_list <- list(Y = Q_learner, A = g_learner)"},{"path":"stochastic-treatment-regimes-optional.html","id":"simulate-data-1","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.4.1 Simulate Data","text":"now observed data structure (data) specification role\nvariable data set plays nodes directed acyclic\ngraph (DAG) via nonparametric structural equation models (NPSEMs).start, initialize specification TMLE parameter \ninterest (tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_val = 0.5 initializing \ntmle3_Spec object communicate ’re interested shift \\(0.5\\) \nscale treatment \\(\\) – , specify \\(\\delta = 0.5\\).seen , tmle_shift specification object (like tmle3_Spec\nobjects) store data specific analysis interest. Later,\n’ll see passing data object directly tmle3 wrapper function,\nalongside instantiated tmle_spec, serve construct tmle3_Task\nobject internally (see tmle3 documentation details).","code":"\n# simulate simple data for tmle-shift sketch\nn_obs <- 1000 # number of observations\ntx_mult <- 2 # multiplier for the effect of W = 1 on the treatment\n\n## baseline covariates -- simple, binary\nW <- replicate(2, rbinom(n_obs, 1, 0.5))\n\n## create treatment based on baseline W\nA <- rnorm(n_obs, mean = tx_mult * W, sd = 1)\n\n## create outcome as a linear function of A, W + white noise\nY <- rbinom(n_obs, 1, prob = plogis(A + W))\n\n# organize data and nodes for tmle3\ndata <- data.table(W, A, Y)\nsetnames(data, c(\"W1\", \"W2\", \"A\", \"Y\"))\nnode_list <- list(W = c(\"W1\", \"W2\"), A = \"A\", Y = \"Y\")\nhead(data)\n#>    W1 W2        A Y\n#> 1:  1  1  3.58065 1\n#> 2:  1  0  3.20718 1\n#> 3:  1  1  1.03584 1\n#> 4:  0  0 -0.65785 1\n#> 5:  1  1  3.01990 1\n#> 6:  1  1  2.78031 1\n# initialize a tmle specification\ntmle_spec <- tmle_shift(\n  shift_val = 0.5,\n  shift_fxn = shift_additive,\n  shift_fxn_inv = shift_additive_inv\n)"},{"path":"stochastic-treatment-regimes-optional.html","id":"targeted-estimation-of-stochastic-interventions-effects","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.4.2 Targeted Estimation of Stochastic Interventions Effects","text":"print method resultant tmle_fit object conveniently displays \nresults computing TML estimator.","code":"\ntmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n#> \n#> Iter: 1 fn: 1384.5336     Pars:  0.22969279 0.00001885 0.77028836\n#> Iter: 2 fn: 1384.5336     Pars:  0.229692876 0.000007582 0.770299542\n#> solnp--> Completed in 2 iterations\ntmle_fit\n#> A tmle3_Fit that took 1 step(s)\n#>    type         param init_est tmle_est      se   lower   upper psi_transformed\n#> 1:  TSM E[Y_{A=NULL}]   0.8008  0.79855 0.01284 0.77339 0.82372         0.79855\n#>    lower_transformed upper_transformed\n#> 1:           0.77339           0.82372"},{"path":"stochastic-treatment-regimes-optional.html","id":"stochastic-interventions-over-a-grid-of-counterfactual-shifts","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5 Stochastic Interventions over a Grid of Counterfactual Shifts","text":"Consider arbitrary scalar \\(\\delta\\) defines counterfactual outcome\n\\(\\psi_n = Q_n(d(, W), W)\\), , simplicity, let \\(d(, W) = + \\delta\\).\nsimplified expression auxiliary covariate TMLE \\(\\psi\\) \n\\(H_n = \\frac{g^{\\star}(\\mid w)}{g(\\mid w)}\\), \\(g^{\\star}(\\mid w)\\)\ndefines treatment mechanism stochastic intervention implemented.\nmanner, can specify grid shifts \\(\\delta\\) define set \nstochastic intervention policies priori manner.Consider arbitrary scalar \\(\\delta\\) defines counterfactual outcome\n\\(\\psi_n = Q_n(d(, W), W)\\), , simplicity, let \\(d(, W) = + \\delta\\).\nsimplified expression auxiliary covariate TMLE \\(\\psi\\) \n\\(H_n = \\frac{g^{\\star}(\\mid w)}{g(\\mid w)}\\), \\(g^{\\star}(\\mid w)\\)\ndefines treatment mechanism stochastic intervention implemented.\nmanner, can specify grid shifts \\(\\delta\\) define set \nstochastic intervention policies priori manner.ascertain whether given choice shift \\(\\delta\\) acceptable, let\nbound \\(C(\\delta) = \\frac{g^{\\star}(\\mid w)}{g(\\mid w)} \\leq M\\),\n\\(g^{\\star}(\\mid w)\\) function \\(\\delta\\) part, \\(M\\) \nuser-specified upper bound \\(C(\\delta)\\). , \\(C(\\delta)\\) measure \ninfluence given observation (bound ratio \nconditional densities), provides way limit maximum influence \ngiven observation choice shift \\(\\delta\\).ascertain whether given choice shift \\(\\delta\\) acceptable, let\nbound \\(C(\\delta) = \\frac{g^{\\star}(\\mid w)}{g(\\mid w)} \\leq M\\),\n\\(g^{\\star}(\\mid w)\\) function \\(\\delta\\) part, \\(M\\) \nuser-specified upper bound \\(C(\\delta)\\). , \\(C(\\delta)\\) measure \ninfluence given observation (bound ratio \nconditional densities), provides way limit maximum influence \ngiven observation choice shift \\(\\delta\\).purpose using shift practice, present software\nprovides functions shift_additive_bounded \nshift_additive_bounded_inv, define variation shift:\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & C(\\delta) \\leq M \\\\\n      0, \\text{otherwise} \\\\\n    \\end{cases},\n\\end{equation}\\]\ncorresponds intervention natural value treatment\ngiven observational unit shifted value \\(\\delta\\) case \nratio intervened density \\(g^{\\star}(\\mid w)\\) natural\ndensity \\(g(\\mid w)\\) (, \\(C(\\delta)\\)) exceed bound \\(M\\). \ncase ratio \\(C(\\delta)\\) exceeds bound \\(M\\), stochastic\nintervention policy apply given unit remain \nnatural value treatment \\(\\).purpose using shift practice, present software\nprovides functions shift_additive_bounded \nshift_additive_bounded_inv, define variation shift:\n\\[\\begin{equation}\n  \\delta(, w) =\n    \\begin{cases}\n      \\delta, & C(\\delta) \\leq M \\\\\n      0, \\text{otherwise} \\\\\n    \\end{cases},\n\\end{equation}\\]\ncorresponds intervention natural value treatment\ngiven observational unit shifted value \\(\\delta\\) case \nratio intervened density \\(g^{\\star}(\\mid w)\\) natural\ndensity \\(g(\\mid w)\\) (, \\(C(\\delta)\\)) exceed bound \\(M\\). \ncase ratio \\(C(\\delta)\\) exceeds bound \\(M\\), stochastic\nintervention policy apply given unit remain \nnatural value treatment \\(\\).","code":""},{"path":"stochastic-treatment-regimes-optional.html","id":"initializing-vimshift-through-its-tmle3_spec","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.1 Initializing vimshift through its tmle3_Spec","text":"start, initialize specification TMLE parameter \ninterest (called tmle3_Spec tlverse nomenclature) simply calling\ntmle_shift. specify argument shift_grid = seq(-1, 1, = 1)\ninitializing tmle3_Spec object communicate ’re interested\nassessing mean counterfactual outcome grid shifts -1, 0, 1 scale treatment \\(\\).","code":"\n# what's the grid of shifts we wish to consider?\ndelta_grid <- seq(from = -1, to = 1, by = 1)\n\n# initialize a tmle specification\ntmle_spec <- tmle_vimshift_delta(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)"},{"path":"stochastic-treatment-regimes-optional.html","id":"targeted-estimation-of-stochastic-intervention-effects","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.2 Targeted Estimation of Stochastic Intervention Effects","text":"One may walk step--step procedure fitting TML estimator\nmean counterfactual outcome shift grid, using \nmachinery exposed tmle3 R package, \nsimply invoke tmle3 wrapper function fit series TML estimators\n(one parameter defined grid delta) single function call.\nconvenience, choose latter:Remark: print method resultant tmle_fit object conveniently\ndisplays results computing TML estimator.","code":"\ntmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)\n#> \n#> Iter: 1 fn: 1385.8594     Pars:  0.25208827 0.00004768 0.74786405\n#> Iter: 2 fn: 1385.8594     Pars:  0.25208843 0.00002901 0.74788255\n#> solnp--> Completed in 2 iterations\ntmle_fit\n#> A tmle3_Fit that took 1 step(s)\n#>          type          param init_est tmle_est        se   lower   upper\n#> 1:        TSM  E[Y_{A=NULL}]  0.61655  0.61546 0.0140007 0.58802 0.64290\n#> 2:        TSM  E[Y_{A=NULL}]  0.74115  0.73899 0.0138954 0.71176 0.76623\n#> 3:        TSM  E[Y_{A=NULL}]  0.84917  0.84374 0.0107174 0.82274 0.86475\n#> 4: MSM_linear MSM(intercept)  0.73563  0.73273 0.0120788 0.70906 0.75641\n#> 5: MSM_linear     MSM(slope)  0.11631  0.11414 0.0053805 0.10360 0.12469\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.61546           0.58802           0.64290\n#> 2:         0.73899           0.71176           0.76623\n#> 3:         0.84374           0.82274           0.86475\n#> 4:         0.73273           0.70906           0.75641\n#> 5:         0.11414           0.10360           0.12469"},{"path":"stochastic-treatment-regimes-optional.html","id":"inference-with-marginal-structural-models","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.3 Inference with Marginal Structural Models","text":"Since consider estimating mean counterfactual outcome \\(\\psi_n\\) \nseveral values intervention \\(\\delta\\), taken aforementioned\n\\(\\delta\\)-grid, one approach obtaining inference single summary measure\nestimated quantities involves leveraging working marginal structural\nmodels (MSMs). Summarizing estimates \\(\\psi_n\\) working MSM allows\ninference trend imposed \\(\\delta\\)-grid evaluated via \nsimple hypothesis test parameter working MSM. Letting\n\\(\\psi_{\\delta}(P_0)\\) mean outcome shift \\(\\delta\\) \ntreatment, \\(\\vec{\\psi}_{\\delta} = (\\psi_{\\delta}: \\delta)\\) \ncorresponding estimators \\(\\vec{\\psi}_{n, \\delta} = (\\psi_{n, \\delta}: \\delta)\\).\n, let \\(\\beta(\\vec{\\psi}_{\\delta}) = \\phi((\\psi_{\\delta}: \\delta))\\). \nstraightforward application delta method (discussed previously), may\nwrite efficient influence function MSM parameter \\(\\beta\\) terms \nEIFs corresponding point estimates. Based , inference\nworking MSM rather straightforward. wit, limiting distribution\n\\(m_{\\beta}(\\delta)\\) may expressed \\[\\sqrt{n}(\\beta_n - \\beta_0) \\N(0,\n\\Sigma),\\] \\(\\Sigma\\) empirical covariance matrix \n\\(\\text{EIF}_{\\beta}(O)\\).","code":"\ntmle_fit$summary[4:5, ]\n#>          type          param init_est tmle_est        se   lower   upper\n#> 1: MSM_linear MSM(intercept)  0.73563  0.73273 0.0120788 0.70906 0.75641\n#> 2: MSM_linear     MSM(slope)  0.11631  0.11414 0.0053805 0.10360 0.12469\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.73273           0.70906           0.75641\n#> 2:         0.11414           0.10360           0.12469"},{"path":"stochastic-treatment-regimes-optional.html","id":"directly-targeting-the-msm-parameter-beta","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.4 Directly Targeting the MSM Parameter \\(\\beta\\)","text":"Note , working MSM fit individual TML estimates \nmean counterfactual outcome given value shift \\(\\delta\\) \nsupplied grid. parameter interest \\(\\beta\\) MSM \nasymptotically linear (, fact, TML estimator) consequence \nconstruction individual TML estimators. smaller samples, may \nprudent perform TML estimation procedure targets parameter\n\\(\\beta\\) directly, opposed constructing several independently\ntargeted TML estimates. approach constructing estimator \nproposed sequel.Suppose simple working MSM \\(\\mathbb{E}Y_{g^0_{\\delta}} = \\beta_0 + \\beta_1 \\delta\\), TML estimator targeting \\(\\beta_0\\) \\(\\beta_1\\) may \nconstructed \n\\[\\overline{Q}_{n, \\epsilon}(,W) = \\overline{Q}_n(,W) + \\epsilon (H_1(g),\nH_2(g),\\] \\(\\delta\\), \\(H_1(g)\\) auxiliary covariate \n\\(\\beta_0\\) \\(H_2(g)\\) auxiliary covariate \\(\\beta_1\\).construct targeted maximum likelihood estimator directly targets \nparameters working marginal structural model, may use \ntmle_vimshift_msm Spec (instead tmle_vimshift_delta Spec \nappears ):","code":"\n# initialize a tmle specification\ntmle_msm_spec <- tmle_vimshift_msm(\n  shift_grid = delta_grid,\n  max_shifted_ratio = 2\n)\n\n# fit the TML estimator and examine the results\ntmle_msm_fit <- tmle3(tmle_msm_spec, data, node_list, learner_list)\n#> \n#> Iter: 1 fn: 1383.8221     Pars:  0.25259038 0.00001286 0.74739676\n#> Iter: 2 fn: 1383.8221     Pars:  0.25259037 0.00000784 0.74740179\n#> solnp--> Completed in 2 iterations\ntmle_msm_fit\n#> A tmle3_Fit that took 100 step(s)\n#>          type          param init_est tmle_est        se   lower   upper\n#> 1: MSM_linear MSM(intercept)  0.73688  0.73682 0.0120245 0.71326 0.76039\n#> 2: MSM_linear     MSM(slope)  0.11604  0.11615 0.0053919 0.10558 0.12672\n#>    psi_transformed lower_transformed upper_transformed\n#> 1:         0.73682           0.71326           0.76039\n#> 2:         0.11615           0.10558           0.12672"},{"path":"stochastic-treatment-regimes-optional.html","id":"example-with-the-wash-benefits-data","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.5.5 Example with the WASH Benefits Data","text":"complete walk , let’s turn using stochastic interventions \ninvestigate data WASH Benefits trial. start, let’s load \ndata, convert columns class numeric, take quick look itNext, specify NPSEM via node_list object. example analysis,\n’ll consider outcome weight--height Z-score (previous\nsections), intervention interest mother’s age time \nchild’s birth, take covariates potential confounders.consider counterfactual weight--height Z-score shifts \nage mother child’s birth, interpret estimates \nparameter?simplify interpretation, consider shift () two years \nmother’s age (.e., \\(\\delta = \\{-2, 0, 2\\}\\)); setting, stochastic\nintervention correspond policy advocating potential mothers\ndefer accelerate plans child two calendar years, possibly\nimplemented deployment encouragement design.First, let’s try simple upward shift just two years:examine effect modification approach looked previous chapters,\n’ll estimate effect shift \\(\\delta = 2\\) stratifying \nmother’s education level (momedu, categorical variable three levels).\n, augment initialized tmle3_Spec object like soPrior running analysis, ’ll modify learner_list object \ncreated include just one semiparametric location-scale conditional\ndensity estimators, fitting estimators much faster \ncomputationally intensive approach implemented \nhaldensify package\n(Hejazi, Benkeser, van der Laan 2020).Now ’re ready construct TML estimate shift parameter \n\\(\\delta = 2\\), stratified across levels variable interest:next example, ’ll use variable importance strategy considering\ngrid stochastic interventions evaluate weight--height Z-score\nshift mother’s age two years (\\(\\delta = -2\\)) \ntwo years (\\(\\delta = 2\\)), incrementing single year two. \n, simply initialize Spec tmle_vimshift_delta similar \nprevious example:made preparations, ’re now ready estimate \ncounterfactual mean weight--height Z-score small grid \nshifts mother’s age child’s birth. Just , \nsimple call tmle3 wrapper function:","code":"\nwashb_data <- fread(\"https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data_subset.csv\", stringsAsFactors = TRUE)\nwashb_data <- washb_data[!is.na(momage) & !is.na(momheight), ]\nhead(washb_data, 3)\n#>      whz          tr fracode month aged    sex momage         momedu momheight\n#> 1: -0.94 Handwashing  N06505     7  237   male     21 Primary (1-5y)    146.00\n#> 2: -1.13     Control  N06505     8  310 female     26   No education    148.90\n#> 3: -1.61     Control  N06524     3  162   male     25 Primary (1-5y)    153.75\n#>        hfiacat Nlt18 Ncomp watmin elec floor walls roof asset_wardrobe\n#> 1: Food Secure     1    25      2    1     0     1    1              0\n#> 2: Food Secure     1     7      4    1     0     0    1              0\n#> 3: Food Secure     0    15      2    0     0     1    1              0\n#>    asset_table asset_chair asset_khat asset_chouki asset_tv asset_refrig\n#> 1:           1           0          0            1        0            0\n#> 2:           1           1          0            1        0            0\n#> 3:           1           0          1            1        0            0\n#>    asset_bike asset_moto asset_sewmach asset_mobile\n#> 1:          0          0             0            0\n#> 2:          0          0             0            1\n#> 3:          0          0             0            0\nnode_list <- list(\n  W = names(washb_data)[!(names(washb_data) %in%\n    c(\"whz\", \"momage\"))],\n  A = \"momage\", Y = \"whz\"\n)\n# initialize a tmle specification for just a single delta shift\nwashb_shift_spec <- tmle_shift(\n  shift_val = 2,\n  shift_fxn = shift_additive,\n  shift_fxn_inv = shift_additive_inv\n)\n# initialize effect modification specification around previous specification\nwashb_shift_strat_spec <-  tmle_stratified(washb_shift_spec)\n# learners used for conditional density regression (i.e., propensity score),\n# but we need to turn on cross-validation for this conditional density learner\nhose_learner_xgb_cv <- Lrnr_cv$new(\n  learner = hose_learner_xgb,\n  full_fit = TRUE\n)\n\n# modify learner list, using existing SL for Q fit\nlearner_list <- list(Y = Q_learner, A = hose_learner_xgb_cv)\n# fit stratified TMLE\nstrat_node_list <- copy(node_list)\nstrat_node_list$W <- setdiff(strat_node_list$W,\"momedu\")\nstrat_node_list$V <- \"momedu\"\nwashb_shift_strat_fit <- tmle3(washb_shift_strat_spec, washb_data, strat_node_list,\n                               learner_list)\nwashb_shift_strat_fit\n#> A tmle3_Fit that took 1 step(s)\n#>              type                             param init_est tmle_est       se\n#> 1:            TSM                     E[Y_{A=NULL}] -0.57206 -0.56936 0.048211\n#> 2: stratified TSM  E[Y_{A=NULL}] | V=Primary (1-5y) -0.62295 -0.69200 0.076813\n#> 3: stratified TSM    E[Y_{A=NULL}] | V=No education -0.68673 -0.86672 0.128939\n#> 4: stratified TSM E[Y_{A=NULL}] | V=Secondary (>5y) -0.50717 -0.40686 0.067633\n#>       lower    upper psi_transformed lower_transformed upper_transformed\n#> 1: -0.66385 -0.47487        -0.56936          -0.66385          -0.47487\n#> 2: -0.84255 -0.54145        -0.69200          -0.84255          -0.54145\n#> 3: -1.11944 -0.61401        -0.86672          -1.11944          -0.61401\n#> 4: -0.53942 -0.27431        -0.40686          -0.53942          -0.27431\n# initialize a tmle specification for the variable importance parameter\nwashb_vim_spec <- tmle_vimshift_delta(\n  shift_grid = seq(from = -2, to = 2, by = 1),\n  max_shifted_ratio = 2\n)\nwashb_tmle_fit <- tmle3(washb_vim_spec, washb_data, node_list, learner_list)\nwashb_tmle_fit\n#> A tmle3_Fit that took 1 step(s)\n#>          type          param   init_est   tmle_est        se      lower\n#> 1:        TSM  E[Y_{A=NULL}] -0.5608084 -0.5552156 0.0469064 -0.6471504\n#> 2:        TSM  E[Y_{A=NULL}] -0.5638623 -0.5644702 0.0466125 -0.6558290\n#> 3:        TSM  E[Y_{A=NULL}] -0.5663920 -0.5652941 0.0466314 -0.6566901\n#> 4:        TSM  E[Y_{A=NULL}] -0.5687408 -0.5681002 0.0463126 -0.6588712\n#> 5:        TSM  E[Y_{A=NULL}] -0.5708337 -0.5716047 0.0468992 -0.6635256\n#> 6: MSM_linear MSM(intercept) -0.5661274 -0.5649370 0.0465180 -0.6561105\n#> 7: MSM_linear     MSM(slope) -0.0024929 -0.0036408 0.0012834 -0.0061563\n#>         upper psi_transformed lower_transformed upper_transformed\n#> 1: -0.4632807      -0.5552156        -0.6471504        -0.4632807\n#> 2: -0.4731114      -0.5644702        -0.6558290        -0.4731114\n#> 3: -0.4738982      -0.5652941        -0.6566901        -0.4738982\n#> 4: -0.4773292      -0.5681002        -0.6588712        -0.4773292\n#> 5: -0.4796839      -0.5716047        -0.6635256        -0.4796839\n#> 6: -0.4737634      -0.5649370        -0.6561105        -0.4737634\n#> 7: -0.0011254      -0.0036408        -0.0061563        -0.0011254"},{"path":"stochastic-treatment-regimes-optional.html","id":"exercises-1","chapter":"6 Stochastic Treatment Regimes (optional)","heading":"6.6 Exercises","text":"Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate counterfactual\nmean mother’s age child’s birth (momage) shift \\(\\delta = 0\\).\ncounterfactual mean equate terms observed data?Set sl3 library algorithms Super Learner simple,\ninterpretable library use new library estimate counterfactual\nmean mother’s age child’s birth (momage) shift \\(\\delta = 0\\).\ncounterfactual mean equate terms observed data?Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.Describe two (equivalent) ways causal effects stochastic\ninterventions may interpreted.Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)), repeat analysis variable interest (momage),\nsummarizing trend sequence shifts using marginal structural\nmodel.Using grid values shift parameter \\(\\delta\\) (e.g., \\(\\{-1, 0, +1\\}\\)), repeat analysis variable interest (momage),\nsummarizing trend sequence shifts using marginal structural\nmodel.either grid shifts example preceding exercises \nestimated (3) , plot resultant estimates respective\ncounterfactual shifts. Graphically add scatterplot line slope\nintercept equivalent MSM fit individual TML estimates.either grid shifts example preceding exercises \nestimated (3) , plot resultant estimates respective\ncounterfactual shifts. Graphically add scatterplot line slope\nintercept equivalent MSM fit individual TML estimates.marginal structural model used summarize trend along\nsequence shifts previously help contextualize estimated effect\nsingle shift? , access estimates across several\nshifts marginal structural model parameters allow us richly\ninterpret findings?marginal structural model used summarize trend along\nsequence shifts previously help contextualize estimated effect\nsingle shift? , access estimates across several\nshifts marginal structural model parameters allow us richly\ninterpret findings?","code":""},{"path":"r6.html","id":"r6","chapter":"7 A Primer on the R6 Class System","heading":"7 A Primer on the R6 Class System","text":"central goal Targeted Learning statistical paradigm estimate\nscientifically relevant parameters realistic (usually nonparametric) models.tlverse designed using basic OOP principles R6 OOP framework.\n’ve tried make easy use tlverse packages without worrying\nmuch OOP, helpful intuition tlverse \nstructured. , briefly outline key concepts OOP. Readers\nfamiliar OOP basics invited skip section.","code":""},{"path":"r6.html","id":"classes-fields-and-methods","chapter":"7 A Primer on the R6 Class System","heading":"7.1 Classes, Fields, and Methods","text":"key concept OOP object, collection data functions\ncorresponds conceptual unit. Objects two main types \nelements:fields, can thought nouns, information object,\nandmethods, can thought verbs, actions object can\nperform.Objects members classes, define specific fields \nmethods . Classes can inherit elements classes (sometimes called\nbase classes) – accordingly, classes similar, exactly \n, can share parts definitions.Many different implementations OOP exist, variations \nconcepts implemented used. R several different implementations,\nincluding S3, S4, reference classes, R6. tlverse uses R6\nimplementation. R6, methods fields class object accessed using\n$ operator. thorough introduction R6, see https://adv-r.hadley.nz/r6.html, Hadley Wickham’s Advanced\nR (Wickham 2014).","code":""},{"path":"r6.html","id":"object-oriented-programming-python-and-r","chapter":"7 A Primer on the R6 Class System","heading":"7.2 Object Oriented Programming: Python and R","text":"OO concepts (classes inherentence) baked Python first\npublished version (version 0.9 1991). contrast, R gets OO “approach”\npredecessor, S, first released 1976. first 15 years, S\nsupport classes, , suddenly, S got two OO frameworks bolted \nrapid succession: informal classes S3 1991, formal classes \nS4 1998. process continues, new OO frameworks periodically\nreleased, try improve lackluster OO support R, reference\nclasses (R5, 2010) R6 (2014). , R6 behaves like Python\nclasses (also like OOP focused languages like C++ Java), including\nmethod definitions part class definitions, allowing objects \nmodified reference.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"Baker, Monya. 2016. “Reproducibility Crisis? Nature Survey Lifts Lid Researchers View Crisis Rocking Science Think Help.” Nature 533 (7604): 452–55.Breiman, Leo. 1996. “Stacked Regressions.” Machine Learning 24 (1): 49–64.Buckheit, Jonathan B, David L Donoho. 1995. “Wavelab Reproducible Research.” Wavelets Statistics, 55–81. Springer.Dı́az, Iván, Mark J van der Laan. 2011. “Super Learner Based Conditional Density Estimation Application Marginal Structural Models.” International Journal Biostatistics 7 (1): 1–20.———. 2012. “Population Intervention Causal Effects Based Stochastic Interventions.” Biometrics 68 (2): 541–49.———. 2018. “Stochastic Treatment Regimes.” Targeted Learning Data Science: Causal Inference Complex Longitudinal Studies, 167–80. Springer Science & Business Media.Haneuse, Sebastian, Andrea Rotnitzky. 2013. “Estimation Effect Interventions Modify Received Treatment.” Statistics Medicine 32 (30): 5260–77.Hejazi, Nima S, David C Benkeser, Mark J van der Laan. 2020. haldensify: Highly Adaptive Lasso Conditional Density Estimation. https://github.com/nhejazi/haldensify. https://doi.org/10.5281/zenodo.3698329.Luby, Stephen P, Mahbubur Rahman, Benjamin F Arnold, Leanne Unicomb, Sania Ashraf, Peter J Winch, Christine P Stewart, et al. 2018. “Effects Water Quality, Sanitation, Handwashing, Nutritional Interventions Diarrhoea Child Growth Rural Bangladesh: Cluster Randomised Controlled Trial.” Lancet Global Health 6 (3): e302–e315.Munafò, Marcus R, Brian Nosek, Dorothy VM Bishop, Katherine S Button, Christopher D Chambers, Nathalie Percie Du Sert, Uri Simonsohn, Eric-Jan Wagenmakers, Jennifer J Ware, John PA Ioannidis. 2017. “Manifesto Reproducible Science.” Nature Human Behaviour 1 (1): 0021.Nature Editorial. 2015a. “Scientists Fool — Can Stop.” Nature 526 (7572).———. 2015b. “Let’s Think Cognitive Bias.” Nature 526 (7572).Nosek, Brian , Charles R Ebersole, Alexander C DeHaven, David T Mellor. 2018. “Preregistration Revolution.” Proceedings National Academy Sciences 115 (11): 2600–2606.Peng, Roger. 2015. “Reproducibility Crisis Science: Statistical Counterattack.” Significance 12 (3): 30–32.Polley, Eric C, Mark J van der Laan. 2010. “Super Learner Prediction.” Bepress.Pullenayegum, Eleanor M, Robert W Platt, Melanie Barwick, Brian M Feldman, Martin Offringa, Lehana Thabane. 2016. “Knowledge Translation Biostatistics: Survey Current Practices, Preferences, Barriers Dissemination Uptake New Statistical Methods.” Statistics Medicine 35 (6): 805–18.Rubin, Donald B. 1978. “Bayesian Inference Causal Effects: Role Randomization.” Annals Statistics, 34–58.———. 1980. “Randomization Analysis Experimental Data: Fisher Randomization Test Comment.” Journal American Statistical Association 75 (371): 591–93.Stark, Philip B, Andrea Saltelli. 2018. “Cargo-Cult Statistics Scientific Crisis.” Significance 15 (4): 40–43.Stromberg, Arnold, others. 2004. “Write Statistical Software? Case Robust Statistical Methods.” Journal Statistical Software 10 (5): 1–8.Szucs, Denes, John Ioannidis. 2017. “Null Hypothesis Significance Testing Unsuitable Research: Reassessment.” Frontiers Human Neuroscience 11: 390.van der Laan, Mark J, Sandrine Dudoit. 2003. “Unified Cross-Validation Methodology Selection Among Estimators General Cross-Validated Adaptive Epsilon-Net Estimator: Finite Sample Oracle Inequalities Examples.” Bepress.van der Laan, Mark J, Eric C Polley, Alan E Hubbard. 2007. “Super Learner.” Statistical Applications Genetics Molecular Biology 6 (1).van der Laan, Mark J, Richard JCM Starmans. 2014. “Entering Era Data Science: Targeted Learning Integration Statistics Computational Data Analysis.” Advances Statistics 2014.Van der Vaart, Aad W, Sandrine Dudoit, Mark J van der Laan. 2006. “Oracle Inequalities Multi-Fold Cross Validation.” Statistics & Decisions 24 (3): 351–71.Wickham, Hadley. 2014. Advanced R. Chapman; Hall/CRC.Wolpert, David H. 1992. “Stacked Generalization.” Neural Networks 5 (2): 241–59.Young, Jessica G, Miguel Hernán, James M Robins. 2014. “Identification, Estimation Approximation Risk Interventions Depend Natural Value Treatment Using Observational Data.” Epidemiologic Methods 3 (1): 1–19.","code":""}]
