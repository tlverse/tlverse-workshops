% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  12pt, krantz2,
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={{[}Workshop{]} Targeted Learning in the tlverse},
  pdfauthor={Mark van der Laan, Alan Hubbard, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage[inline]{enumitem}
\usepackage{float}
\usepackage{graphicx}
%\usepackage[round]{natbib}
\usepackage{geometry}
\usepackage{tikz}
\usepackage[english]{babel}
\usepackage{longtable}
\usepackage{color}
\usepackage{mathtools,bm,amssymb,amsmath,amsthm}
\usepackage{multirow}
\usepackage[titletoc,title]{appendix}
\usepackage{authblk}
\usepackage{setspace}
\usepackage{dsfont}
\PassOptionsToPackage{utf8x}{inputenc}
\usepackage[OT1]{fontenc}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage{refcount}
\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}
\urlstyle{tt}
\usepackage[none]{hyphenat} 

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\usepackage{makeidx}
\makeindex

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\newtheorem*{remark}{Remark}
\newtheorem{theorem}{Theorem}
\AtEndDocument{\refstepcounter{theorem}\label{finalthm}}
{
  \theoremstyle{definition}
  \newtheorem{assumption}{}
}
{
  \theoremstyle{definition}
  \newtheorem{assumptioniden}{}
}
{
  \theoremstyle{definition}
  \newtheorem{example}{Example}[section]
}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\dr}{IF}
\newcommand{\hopt}{\hat h_{\opt}}
\newcommand{\supp}{\mathop{\mathrm{supp}}}
\renewcommand\theassumptioniden{{A}\arabic{assumptioniden}}
\renewcommand\theassumption{{C}\arabic{assumption}}
\renewcommand\theexample{\arabic{example}}

\newtheorem{lemma}{Lemma}
\newtheorem{coro}{Corollary}
\newtheorem{definition}{Definition}
\DeclareMathOperator{\bern}{Bern}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\Rem}{Rem}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\1}{\mathbbm{1}}
\DeclareMathOperator{\expit}{expit}
\DeclareMathOperator{\logit}{logit}
\newcommand{\indep}{\mbox{$\perp\!\!\!\perp$}}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\usepackage{color}
\lstset{
  breaklines=true,
  language=R,
  showspaces=false, 
  showstringspaces=false,
  basicstyle=\footnotesize\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\color{blue},
  stringstyle=\color{orange}
}
\DeclareUnicodeCharacter{2212}{-}
% setting bookdown frontmatter option
\frontmatter

\usepackage{framed}
\setlength{\fboxsep}{.8em}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{{[}Workshop{]} Targeted Learning in the \passthrough{\lstinline!tlverse!}}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Causal Inference Meets Machine Learning}
\author{Mark van der Laan, Alan Hubbard, Jeremy Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips}
\date{updated: October 12, 2021}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

% setting bookdown mainmatter (e.g., arabic numerals for page numbering)
\mainmatter

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{welcome}{%
\chapter*{Welcome!}\label{welcome}}


This open source, reproducible vignette is for a half-day workshop on the
Targeted Learning framework for statistical and causal inference with machine
learning. Beyond introducing Targeted Learning, the workshop focuses on
applying the methodology in practice using the \href{https://github.com/tlverse}{\passthrough{\lstinline!tlverse!} software
ecosystem}. These materials are based on a working
draft of the book \href{https://tlverse.org/tlverse-handbook/}{\emph{Targeted Learning in \passthrough{\lstinline!R!}: Causal Data Science with the
\passthrough{\lstinline!tlverse!} Software Ecosystem}}, which
includes in-depth discussion of these topics and much more, and may serve as a
useful reference to accompany these workshop materials.

\hypertarget{important-links}{%
\section*{Important links}\label{important-links}}


\begin{itemize}
\item
  \textbf{Software installation}: Please install the relevant software before the
  workshop using the \href{https://github.com/tlverse/tlverse-workshops/blob/master/install.R}{installation
  script}.
\item
  You will probably exceed the GitHub API rate limit during this installation,
  which will throw an error. This issue and the solution are addressed
  \protect\hyperlink{installtlverse}{here}.
\item
  \textbf{Code}: \passthrough{\lstinline!R!} script files for each section of the workshop are available via
  the GitHub repository for the workshop at
  \url{https://github.com/tlverse/tlverse-workshops/tree/master/R_code}
\end{itemize}

\hypertarget{about-this-workshop}{%
\section*{About this workshop}\label{about-this-workshop}}


This workshop will provide a comprehensive introduction to the field of
\emph{Targeted Learning} for causal inference, and the corresponding \href{https://github.com/tlverse}{\passthrough{\lstinline!tlverse!} software
ecosystem}. Emphasis will be placed on targeted
minimum loss-based estimators of the causal effects of single timepoint
interventions, including extensions for missing covariate and outcome data.
These multiply robust, efficient plug-in estimators use state-of-the-art,
ensemble machine learning tools to flexibly adjust for confounding while
yielding valid statistical inference. In particular, we will discuss targeted
estimators of the causal effects of static and dynamic interventions; time
permitting, additional topics to be discussed will include estimation of the
causal effects of optimal dynamic and stochastic interventions.

In addition to discussion, this workshop will incorporate both interactive
activities and hands-on, guided \passthrough{\lstinline!R!} programming exercises, to allow participants
the opportunity to familiarize themselves with methodology and tools that will
translate to real-world data analysis. It is highly recommended for participants
to have an understanding of basic statistical concepts such as confounding,
probability distributions, confidence intervals, hypothesis testing, and
regression. Advanced knowledge of mathematical statistics is useful but not
necessary. Familiarity with the \passthrough{\lstinline!R!} programming language will be essential.

\hypertarget{outline}{%
\section*{Outline}\label{outline}}


\begin{itemize}
\tightlist
\item
  \emph{Warm-up}: The Roadmap of Targeted Learning and \href{https://senseaboutscienceusa.org/super-learning-and-the-revolution-in-knowledge/}{Why We Need A Statistical
  Revolution}
  with an \emph{\href{https://www.dropbox.com/s/7b6ru2ahycqq80v/ENAR2021-lecture.mp4?dl=0}{introductory video lecture by Mark van der Laan and Alan
  Hubbard}}
  (\textbf{Please watch this hour-long lecture before the workshop.})
\item
  09:00-09:30A: \href{https://tlverse.org}{Introduction to the \passthrough{\lstinline!tlverse!} Software
  Ecosystem} and the \href{http://www.washbenefits.net/}{WASH Benefits
  data}
\item
  09:30-10:00A: Super learning with the \href{https://github.com/tlverse/sl3}{\passthrough{\lstinline!sl3!} \passthrough{\lstinline!R!}
  package}
\item
  10:00-11:00A: Programming exercises with \passthrough{\lstinline!sl3!}
\item
  11:00-11:15A: Morning Coffee Break and Q\&A
\item
  11:15-12:00P: Targeted Learning for causal inference with the \href{https://github.com/tlverse/tmle3}{\passthrough{\lstinline!tmle3!} \passthrough{\lstinline!R!}
  package}
\item
  12:00-12:45P: Programming exercises with \passthrough{\lstinline!tmle3!}
\item
  12:45-01:30P: Lunch Break
\item
  01:30-02:15P: Optimal treatment regimes with the \href{https://github.com/tlverse/tmle3mopttx}{\passthrough{\lstinline!tmle3mopttx!} \passthrough{\lstinline!R!}
  package}
\item
  02:15-03:00P: Programming exercises with \passthrough{\lstinline!tmle3mopttx!}
\item
  03:00-03:15P: Afternoon Coffee Break
\item
  03:15-04:00P: Stochastic treatment regimes with the \href{https://github.com/tlverse/tmle3shift}{\passthrough{\lstinline!tmle3shift!} \passthrough{\lstinline!R!}
  package}
\item
  04:00-04:30P: Programming exercises with \passthrough{\lstinline!tmle3shift!}
\item
  04:30-05:00P: Concluding remarks and discussion
\end{itemize}

\hypertarget{about-the-instructors}{%
\section*{About the instructors}\label{about-the-instructors}}


\hypertarget{mark-van-der-laan}{%
\subsection*{Mark van der Laan}\label{mark-van-der-laan}}


Mark van der Laan, PhD, is Professor of Biostatistics and Statistics at UC
Berkeley. His research interests include statistical methods in computational
biology, survival analysis, censored data, adaptive designs, targeted maximum
likelihood estimation, causal inference, data-adaptive loss-based learning, and
multiple testing. His research group developed loss-based super learning in
semiparametric models, based on cross-validation, as a generic optimal tool for
the estimation of infinite-dimensional parameters, such as nonparametric density
estimation and prediction with both censored and uncensored data. Building on
this work, his research group developed targeted maximum likelihood estimation
for a target parameter of the data-generating distribution in arbitrary
semiparametric and nonparametric models, as a generic optimal methodology for
statistical and causal inference. Most recently, Mark's group has focused in
part on the development of a centralized, principled set of software tools for
targeted learning, the \passthrough{\lstinline!tlverse!}.

\hypertarget{alan-hubbard}{%
\subsection*{Alan Hubbard}\label{alan-hubbard}}


Alan Hubbard is Professor of Biostatistics, former head of the Division of
Biostatistics at UC Berkeley, and head of data analytics core at UC Berkeley's
SuperFund research program. His current research interests include causal
inference, variable importance analysis, statistical machine learning,
estimation of and inference for data-adaptive statistical target parameters, and
targeted minimum loss-based estimation. Research in his group is generally
motivated by applications to problems in computational biology, epidemiology,
and precision medicine.

\hypertarget{jeremy-coyle}{%
\subsection*{Jeremy Coyle}\label{jeremy-coyle}}


Jeremy Coyle, PhD, is a consulting data scientist and statistical programmer,
currently leading the software development effort that has produced the
\passthrough{\lstinline!tlverse!} ecosystem of R packages and related software tools. Jeremy earned his
PhD in Biostatistics from UC Berkeley in 2016, primarily under the supervision
of Alan Hubbard.

\hypertarget{nima-hejazi}{%
\subsection*{Nima Hejazi}\label{nima-hejazi}}


Nima Hejazi is a PhD candidate in biostatistics, working under the collaborative
direction of Mark van der Laan and Alan Hubbard. Nima is affiliated with UC
Berkeley's Center for Computational Biology and NIH Biomedical Big Data training
program, as well as with the Fred Hutchinson Cancer Research Center. Previously,
he earned an MA in Biostatistics and a BA (with majors in Molecular and Cell
Biology, Psychology, and Public Health), both at UC Berkeley. His research
interests fall at the intersection of causal inference and machine learning,
drawing on ideas from non/semi-parametric estimation in large, flexible
statistical models to develop efficient and robust statistical procedures for
evaluating complex target estimands in observational and randomized studies.
Particular areas of current emphasis include mediation/path analysis,
outcome-dependent sampling designs, targeted loss-based estimation, and vaccine
efficacy trials. Nima is also passionate about statistical computing and open
source software development for applied statistics.

\hypertarget{ivana-malenica}{%
\subsection*{Ivana Malenica}\label{ivana-malenica}}


Ivana Malenica is a PhD student in biostatistics advised by Mark van der Laan.
Ivana is currently a fellow at the Berkeley Institute for Data Science, after
serving as a NIH Biomedical Big Data and Freeport-McMoRan Genomic Engine fellow.
She earned her Master's in Biostatistics and Bachelor's in Mathematics, and
spent some time at the Translational Genomics Research Institute. Very broadly,
her research interests span non/semi-parametric theory, probability theory,
machine learning, causal inference and high-dimensional statistics. Most of her
current work involves complex dependent settings (dependence through time and
network) and adaptive sequential designs.

\hypertarget{rachael-phillips}{%
\subsection*{Rachael Phillips}\label{rachael-phillips}}


Rachael Phillips is a PhD student in biostatistics, advised by Alan Hubbard and
Mark van der Laan. She has an MA in Biostatistics, BS in Biology, and BA in
Mathematics. A student of targeted learning and causal inference, Rachael's
research integrates semiparametric statistical estimation and inference. She is
motivated by applied projects and some of her current work involves personalized
online learning from data streams of vital signs, human-computer interaction,
automated machine learning, and developing statistical analysis plans using
targeted learning.

\hypertarget{repro}{%
\section{\texorpdfstring{Reproduciblity with the \texttt{tlverse}}{Reproduciblity with the tlverse}}\label{repro}}

The \passthrough{\lstinline!tlverse!} software ecosystem is a growing collection of packages, several of
which are quite early on in the software lifecycle. The team does its best to
maintain backwards compatibility. Once this work reaches completion, the
specific versions of the \passthrough{\lstinline!tlverse!} packages used will be archived and tagged to
produce it.

This book was written using \href{http://bookdown.org/}{bookdown}, and the complete
source is available on \href{https://github.com/tlverse/tlverse-handbook}{GitHub}.
This version of the book was built with R version 4.1.1 (2021-08-10),
\href{https://pandoc.org/}{pandoc} version 2.7.3, and the
following packages:

\begin{longtable}[]{@{}lll@{}}
\toprule
package & version & source\tabularnewline
\midrule
\endhead
bookdown & 0.24.1 & Github (rstudio/bookdown@84bde8e)\tabularnewline
bslib & 0.3.1 & Github (rstudio/bslib@efc475c)\tabularnewline
data.table & 1.14.2 & CRAN (R 4.1.1)\tabularnewline
delayed & 0.4.0 & Github (tlverse/delayed@bf7ca82)\tabularnewline
devtools & 2.4.2 & CRAN (R 4.1.1)\tabularnewline
downlit & 0.2.1 & CRAN (R 4.1.1)\tabularnewline
dplyr & 1.0.7 & CRAN (R 4.1.1)\tabularnewline
ggplot2 & 3.3.5 & CRAN (R 4.1.1)\tabularnewline
here & 1.0.1 & CRAN (R 4.1.1)\tabularnewline
kableExtra & 1.3.4 & CRAN (R 4.1.1)\tabularnewline
knitr & 1.36 & CRAN (R 4.1.1)\tabularnewline
mvtnorm & 1.1-2 & CRAN (R 4.1.1)\tabularnewline
origami & 1.0.5 & Github (tlverse/origami@e1b8fe6)\tabularnewline
readr & 2.0.2 & CRAN (R 4.1.1)\tabularnewline
rmarkdown & 2.11 & CRAN (R 4.1.1)\tabularnewline
skimr & 2.1.3 & CRAN (R 4.1.1)\tabularnewline
sl3 & 1.4.3 & Github (tlverse/sl3@20834ae)\tabularnewline
stringr & 1.4.0 & CRAN (R 4.1.1)\tabularnewline
tibble & 3.1.5 & CRAN (R 4.1.1)\tabularnewline
tidyr & 1.1.4 & CRAN (R 4.1.1)\tabularnewline
tidyverse & 1.3.1 & CRAN (R 4.1.1)\tabularnewline
tmle3 & 0.2.0 & Github (tlverse/tmle3@425e21c)\tabularnewline
tmle3mopttx & 0.1.0 & Github (tlverse/tmle3mopttx@9fb1a3b)\tabularnewline
tmle3shift & 0.2.0 & Github (tlverse/tmle3shift@43f6fc0)\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{setup}{%
\section{Setup instructions}\label{setup}}

\hypertarget{r-and-rstudio}{%
\subsection{R and RStudio}\label{r-and-rstudio}}

\textbf{R} and \textbf{RStudio} are separate downloads and installations. R is the
underlying statistical computing environment. RStudio is a graphical integrated
development environment (IDE) that makes using R much easier and more
interactive. You need to install R before you install RStudio.

\hypertarget{windows}{%
\subsubsection{Windows}\label{windows}}

\hypertarget{if-you-already-have-r-and-rstudio-installed}{%
\paragraph{If you already have R and RStudio installed}\label{if-you-already-have-r-and-rstudio-installed}}

\begin{itemize}
\tightlist
\item
  Open RStudio, and click on ``Help'' \textgreater{} ``Check for updates''. If a new version is
  available, quit RStudio, and download the latest version for RStudio.
\item
  To check which version of R you are using, start RStudio and the first thing
  that appears in the console indicates the version of R you are
  running. Alternatively, you can type \passthrough{\lstinline!sessionInfo()!}, which will also display
  which version of R you are running. Go on the \href{https://cran.r-project.org/bin/windows/base/}{CRAN
  website} and check whether a
  more recent version is available. If so, please download and install it. You
  can \href{https://cran.r-project.org/bin/windows/base/rw-FAQ.html\#How-do-I-UNinstall-R_003f}{check here}
  for more information on how to remove old versions from your system if you
  wish to do so.
\end{itemize}

\hypertarget{if-you-dont-have-r-and-rstudio-installed}{%
\paragraph{If you don't have R and RStudio installed}\label{if-you-dont-have-r-and-rstudio-installed}}

\begin{itemize}
\tightlist
\item
  Download R from
  the \href{http://cran.r-project.org/bin/windows/base/release.htm}{CRAN website}.
\item
  Run the \passthrough{\lstinline!.exe!} file that was just downloaded
\item
  Go to the \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio download page}
\item
  Under \emph{Installers} select \textbf{RStudio x.yy.zzz - Windows
  XP/Vista/7/8} (where x, y, and z represent version numbers)
\item
  Double click the file to install it
\item
  Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
\end{itemize}

\hypertarget{macos-mac-os-x}{%
\subsubsection{macOS / Mac OS X}\label{macos-mac-os-x}}

\hypertarget{if-you-already-have-r-and-rstudio-installed-1}{%
\paragraph{If you already have R and RStudio installed}\label{if-you-already-have-r-and-rstudio-installed-1}}

\begin{itemize}
\tightlist
\item
  Open RStudio, and click on ``Help'' \textgreater{} ``Check for updates''. If a new version is
  available, quit RStudio, and download the latest version for RStudio.
\item
  To check the version of R you are using, start RStudio and the first thing
  that appears on the terminal indicates the version of R you are running.
  Alternatively, you can type \passthrough{\lstinline!sessionInfo()!}, which will also display which
  version of R you are running. Go on the \href{https://cran.r-project.org/bin/macosx/}{CRAN
  website} and check whether a more
  recent version is available. If so, please download and install it.
\end{itemize}

\hypertarget{if-you-dont-have-r-and-rstudio-installed-1}{%
\paragraph{If you don't have R and RStudio installed}\label{if-you-dont-have-r-and-rstudio-installed-1}}

\begin{itemize}
\tightlist
\item
  Download R from
  the \href{http://cran.r-project.org/bin/macosx}{CRAN website}.
\item
  Select the \passthrough{\lstinline!.pkg!} file for the latest R version
\item
  Double click on the downloaded file to install R
\item
  It is also a good idea to install \href{https://www.xquartz.org/}{XQuartz} (needed
  by some packages)
\item
  Go to the \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio download
  page}
\item
  Under \emph{Installers} select \textbf{RStudio x.yy.zzz - Mac OS X 10.6+ (64-bit)}
  (where x, y, and z represent version numbers)
\item
  Double click the file to install RStudio
\item
  Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
\end{itemize}

\hypertarget{linux}{%
\subsubsection{Linux}\label{linux}}

\begin{itemize}
\tightlist
\item
  Follow the instructions for your distribution
  from \href{https://cloud.r-project.org/bin/linux}{CRAN}, they provide information
  to get the most recent version of R for common distributions. For most
  distributions, you could use your package manager (e.g., for Debian/Ubuntu run
  \passthrough{\lstinline!sudo apt-get install r-base!}, and for Fedora \passthrough{\lstinline!sudo yum install R!}), but we
  don't recommend this approach as the versions provided by this are
  usually out of date. In any case, make sure you have at least R 3.3.1.
\item
  Go to the \href{https://www.rstudio.com/products/rstudio/download/\#download}{RStudio download
  page}
\item
  Under \emph{Installers} select the version that matches your distribution, and
  install it with your preferred method (e.g., with Debian/Ubuntu \passthrough{\lstinline!sudo dpkg -i rstudio-x.yy.zzz-amd64.deb!} at the terminal).
\item
  Once it's installed, open RStudio to make sure it works and you don't get any
  error messages.
\end{itemize}

These setup instructions are adapted from those written for \href{http://www.datacarpentry.org/R-ecology-lesson/}{Data Carpentry: R
for Data Analysis and Visualization of Ecological
Data}.

\hypertarget{motivation}{%
\chapter*{Motivation}\label{motivation}}


\begin{quote}
``One enemy of robust science is our humanity --- our appetite for
being right, and our tendency to find patterns in noise, to see supporting
evidence for what we already believe is true, and to ignore the facts that do
not fit.''

--- \citet{naturenews_2015}
\end{quote}

Scientific research is at a unique point in history. The need to improve rigor
and reproducibility in our field is greater than ever; corroboration moves
science forward, yet there is a growing alarm about results that cannot be
reproduced and that report false discoveries \citep{baker2016there}. Consequences of
not meeting this need will result in further decline in the rate of scientific
progression, the reputation of the sciences, and the public's trust in its
findings \citep{munafo2017manifesto, naturenews2_2015}.

\begin{quote}
``The key question we want to answer when seeing the results of any scientific
study is whether we can trust the data analysis.''

--- \citet{peng2015reproducibility}
\end{quote}

Unfortunately, at its current state the culture of data analysis and statistics
actually enables human bias through improper model selection. All hypothesis
tests and estimators are derived from statistical models, so to obtain valid
estimates and inference it is critical that the statistical model contains the
process that generated the data. Perhaps treatment was randomized or only
depended on a small number of baseline covariates; this knowledge should and
can be incorporated in the model. Alternatively, maybe the data is
observational, and there is no knowledge about the data-generating process (DGP).
If this is the case, then the statistical model should contain \emph{all} data
distributions. In practice; however, models are not selected based on knowledge
of the DGP, instead models are often selected based on (1) the p-values they
yield, (2) their convenience of implementation, and/or (3) an analysts loyalty
to a particular model. This practice of ``cargo-cult statistics --- the
ritualistic miming of statistics rather than conscientious practice,''
\citep{stark2018cargo} is characterized by arbitrary modeling choices, even though
these choices often result in different answers to the same research question.
That is, ``increasingly often, {[}statistics{]} is used instead to aid and
abet weak science, a role it can perform well when used mechanically or
ritually,'' as opposed to its original purpose of safeguarding against weak
science \citep{stark2018cargo}. This presents a fundamental drive behind the epidemic
of false findings that scientific research is suffering from \citep{vdl2014entering}.

\begin{quote}
``We suggest that the weak statistical understanding is probably due to
inadequate''statistics lite" education. This approach does not build up
appropriate mathematical fundamentals and does not provide scientifically
rigorous introduction into statistics. Hence, students' knowledge may remain
imprecise, patchy, and prone to serious misunderstandings. What this approach
achieves, however, is providing students with false confidence of being able
to use inferential tools whereas they usually only interpret the p-value
provided by black box statistical software. While this educational problem
remains unaddressed, poor statistical practices will prevail regardless of
what procedures and measures may be favored and/or banned by editorials."

--- \citet{szucs2017null}
\end{quote}

Our team at The University of California, Berkeley, is uniquely positioned to
provide such an education. Spearheaded by Professor Mark van der Laan, and
spreading rapidly by many of his students and colleagues who have greatly
enriched the field, the aptly named ``Targeted Learning'' methodology targets the
scientific question at hand and is counter to the current culture of
``convenience statistics'' which opens the door to biased estimation, misleading
results, and false discoveries. Targeted Learning restores the fundamentals that
formalized the field of statistics, such as the that facts that a statistical
model represents real knowledge about the experiment that generated the data,
and a target parameter represents what we are seeking to learn from the data as
a feature of the distribution that generated it \citep{vdl2014entering}. In this way,
Targeted Learning defines a truth and establishes a principled standard for
estimation, thereby inhibiting these all-too-human biases (e.g., hindsight bias,
confirmation bias, and outcome bias) from infiltrating analysis.

\begin{quote}
``The key for effective classical {[}statistical{]} inference is to have
well-defined questions and an analysis plan that tests those questions.''

--- \citet{nosek2018preregistration}
\end{quote}

Our objective is to provide training to students, researchers, industry professionals, faculty in science, public health, statistics, and other
fields to empower them with the necessary knowledge and skills to utilize the
sound methodology of Targeted Learning --- a technique that provides tailored
pre-specified machines for answering queries, so that each data analysis is
completely reproducible, and estimators are efficient, minimally biased, and
provide formal statistical inference.

Just as the conscientious use of modern statistical methodology is necessary to
ensure that scientific practice thrives, it remains critical to acknowledge the
role that robust software plays in allowing practitioners direct access to
published results. We recall that ``an article\ldots in a scientific publication is
not the scholarship itself, it is merely advertising of the scholarship. The
actual scholarship is the complete software development environment and the
complete set of instructions which generated the figures,'' thus making the
availability and adoption of robust statistical software key to enhancing the
transparency that is an inherent aspect of science \citep{buckheit1995wavelab}.

For a statistical methodology to be readily accessible in practice, it is
crucial that it is accompanied by robust user-friendly software
\citep{pullenayegum2016knowledge, stromberg2004write}. The \passthrough{\lstinline!tlverse!} software
ecosystem was developed to fulfill this need for the Targeted Learning
methodology. Not only does this software facilitate computationally reproducible
and efficient analyses, it is also a tool for Targeted Learning education since
its workflow mirrors that of the methodology. In particular, the \passthrough{\lstinline!tlverse!}
paradigm does not focus on implementing a specific estimator or a small set of
related estimators. Instead, the focus is on exposing the statistical framework
of Targeted Learning itself --- all \passthrough{\lstinline!R!} packages in the \passthrough{\lstinline!tlverse!} ecosystem
directly model the key objects defined in the mathematical and theoretical
framework of Targeted Learning. What's more, the \passthrough{\lstinline!tlverse!} \passthrough{\lstinline!R!} packages share a
core set of design principles centered on extensibility, allowing for them to be
used in conjunction with each other and built upon one other in a cohesive
fashion.

In this workshop, the reader will embark on a journey through the \passthrough{\lstinline!tlverse!}
ecosystem. Guided by \passthrough{\lstinline!R!} programming exercises, case studies, and
intuitive explanation readers will build a toolbox for applying the Targeted
Learning statistical methodology, which will translate to real-world causal
inference analyses. Participants need not be a fully trained statistician to
begin understanding and applying these methods. However, it is highly
recommended for participants to have an understanding of basic statistical
concepts such as confounding, probability distributions, confidence intervals,
hypothesis tests, and regression. Advanced knowledge of mathematical statistics
may be useful but is not necessary. Familiarity with the \passthrough{\lstinline!R!} programming
language will be essential. We also recommend an understanding of introductory
causal inference.

For introductory materials for learning the \passthrough{\lstinline!R!} programming language we recommend the following free resources:

\begin{itemize}
\tightlist
\item
  \href{http://swcarpentry.github.io/r-novice-inflammation/}{Software Carpentry's \emph{Programming with
  \passthrough{\lstinline!R!}}}
\item
  \href{http://swcarpentry.github.io/r-novice-gapminder/}{Software Carpentry's \emph{\passthrough{\lstinline!R!} for Reproducible Scientific
  Analysis}}
\item
  \href{https://r4ds.had.co.nz}{Grolemund and Wickham's \emph{\passthrough{\lstinline!R!} for Data
  Science}}
\end{itemize}

For causal inference learning materials we recommend the following resources:

\begin{itemize}
\tightlist
\item
  \href{https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/}{Hernán MA, Robins JM (2019). \emph{Causal
  Inference}.}
\item
  \href{https://www.coursera.org/learn/crash-course-in-causality}{Jason A. Roy's coursera Course \emph{A Crash Course in Causality: Inferring
  Causal Effects from Observational Data}}
\end{itemize}

\hypertarget{tlverse}{%
\chapter{\texorpdfstring{Welcome to the \texttt{tlverse}}{Welcome to the tlverse}}\label{tlverse}}

\hypertarget{learning-objectives}{%
\section{Learning Objectives}\label{learning-objectives}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand the \passthrough{\lstinline!tlverse!} ecosystem conceptually
\item
  Identify the core components of the \passthrough{\lstinline!tlverse!}
\item
  Install \passthrough{\lstinline!tlverse!} \passthrough{\lstinline!R!} packages
\item
  Understand the Targeted Learning roadmap
\item
  Learn about the WASH Benefits example data
\end{enumerate}

\hypertarget{what-is-the-tlverse}{%
\section{\texorpdfstring{What is the \texttt{tlverse}?}{What is the tlverse?}}\label{what-is-the-tlverse}}

The \passthrough{\lstinline!tlverse!} is a new framework for doing Targeted Learning in R, inspired by
the \href{https://tidyverse.org}{\passthrough{\lstinline!tidyverse!} ecosystem} of R packages.

By analogy to the \href{https://tidyverse.org/}{\passthrough{\lstinline!tidyverse!}}:

\begin{quote}
The \passthrough{\lstinline!tidyverse!} is an opinionated collection of R packages designed for data
science. All packages share an underlying design philosophy, grammar, and data
structures.
\end{quote}

So, the \href{https://tlverse.org}{\passthrough{\lstinline!tlverse!}} is

\begin{itemize}
\tightlist
\item
  an opinionated collection of R packages for Targeted Learning
\item
  sharing an underlying philosophy, grammar, and set of data structures
\end{itemize}

\hypertarget{tlverse-components}{%
\section{\texorpdfstring{\texttt{tlverse} components}{tlverse components}}\label{tlverse-components}}

These are the main packages that represent the \textbf{core} of the \passthrough{\lstinline!tlverse!}:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/tlverse/sl3}{\passthrough{\lstinline!sl3!}}: Modern Super Learning with Pipelines

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A modern object-oriented re-implementation of the Super Learner
    algorithm, employing recently developed paradigms for \passthrough{\lstinline!R!} programming.
  \item
    \emph{Why?} A design that leverages modern tools for fast computation, is
    forward-looking, and can form one of the cornerstones of the \passthrough{\lstinline!tlverse!}.
  \end{itemize}
\item
  \href{https://github.com/tlverse/tmle3}{\passthrough{\lstinline!tmle3!}}: An Engine for Targeted Learning

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A generalized framework that simplifies Targeted Learning by
    identifying and implementing a series of common statistical estimation
    procedures.
  \item
    \emph{Why?} A common interface and engine that accommodates current algorithmic
    approaches to Targeted Learning and is still flexible enough to remain the
    engine even as new techniques are developed.
  \end{itemize}
\end{itemize}

In addition to the engines that drive development in the \passthrough{\lstinline!tlverse!}, there are
some supporting packages -- in particular, we have two\ldots{}

\begin{itemize}
\tightlist
\item
  \href{https://github.com/tlverse/origami}{\passthrough{\lstinline!origami!}}: A Generalized Framework for
  Cross-Validation

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A generalized framework for flexible cross-validation
  \item
    \emph{Why?} Cross-validation is a key part of ensuring error estimates are honest
    and preventing overfitting. It is an essential part of the both the Super
    Learner algorithm and Targeted Learning.
  \end{itemize}
\item
  \href{https://github.com/tlverse/delayed}{\passthrough{\lstinline!delayed!}}: Parallelization Framework for
  Dependent Tasks

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} A framework for delayed computations (futures) based on task
    dependencies.
  \item
    \emph{Why?} Efficient allocation of compute resources is essential when deploying
    large-scale, computationally intensive algorithms.
  \end{itemize}
\end{itemize}

A key principle of the \passthrough{\lstinline!tlverse!} is extensibility. That is, we want to support
new Targeted Learning estimators as they are developed. The model for this is
new estimators are implemented in additional packages using the core packages
above. There are currently two featured examples of this:

\begin{itemize}
\tightlist
\item
  \href{https://github.com/tlverse/tmle3mopttx}{\passthrough{\lstinline!tmle3mopttx!}}: Optimal Treatments
  in \passthrough{\lstinline!tlverse!}

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} Learn an optimal rule and estimate the mean outcome under the rule
  \item
    \emph{Why?} Optimal Treatment is a powerful tool in precision healthcare and
    other settings where a one-size-fits-all treatment approach is not
    appropriate.
  \end{itemize}
\item
  \href{https://github.com/tlverse/tmle3shift}{\passthrough{\lstinline!tmle3shift!}}: Shift Interventions in
  \passthrough{\lstinline!tlverse!}

  \begin{itemize}
  \tightlist
  \item
    \emph{What?} Shift interventions for continuous treatments
  \item
    \emph{Why?} Not all treatment variables are discrete. Being able to estimate the
    effects of continuous treatment represents a powerful extension of the
    Targeted Learning approach.
  \end{itemize}
\end{itemize}

\hypertarget{installtlverse}{%
\section{Installation}\label{installtlverse}}

The \passthrough{\lstinline!tlverse!} ecosystem of packages are currently hosted at
\url{https://github.com/tlverse}, not yet on \href{http://cran.r-project.org/}{CRAN}. You
can use the \passthrough{\lstinline!devtools!} package to install them:

\begin{lstlisting}[language=R]
install.packages("devtools")
devtools::install_github("tlverse/tlverse")
\end{lstlisting}

The \passthrough{\lstinline!tlverse!} depends on a large number of other packages that are also hosted
on GitHub. Because of this, you may see the following error:

\begin{lstlisting}
Error: HTTP error 403.
  API rate limit exceeded for 71.204.135.82. (But here's the good news:
  Authenticated requests get a higher rate limit. Check out the documentation
  for more details.)

  Rate limit remaining: 0/60
  Rate limit reset at: 2019-03-04 19:39:05 UTC

  To increase your GitHub API rate limit
  - Use `usethis::browse_github_pat()` to create a Personal Access Token.
  - Use `usethis::edit_r_environ()` and add the token as `GITHUB_PAT`.
\end{lstlisting}

This just means that R tried to install too many packages from GitHub in too
short of a window. To fix this, you need to tell R how to use GitHub as your
user (you'll need a GitHub user account). Follow these two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Type \passthrough{\lstinline!usethis::browse\_github\_pat()!} in your R console, which will direct
  you to GitHub's page to create a New Personal Access Token.
\item
  Create a Personal Access Token simply by clicking ``Generate token'' at the
  bottom of the page.
\item
  Copy your Personal Access Token, a long string of lowercase letters and
  numbers.
\item
  Type \passthrough{\lstinline!usethis::edit\_r\_environ()!} in your R console, which will open your
  \passthrough{\lstinline!.Renviron!} file in the source window of RStudio. If you are not able to
  access your \passthrough{\lstinline!.Renviron!} file with this command, then try inputting
  \passthrough{\lstinline!Sys.setenv(GITHUB\_PAT = )!} with your Personal Access Token inserted as a
  string after the equals symbol; and if this does not error, then skip to
  step 8.
\item
  In your \passthrough{\lstinline!.Renviron!} file, type \passthrough{\lstinline!GITHUB\_PAT=!} and then paste your Personal
  Access Token after the equals symbol with no space.
\item
  In your \passthrough{\lstinline!.Renviron!} file, press the enter key to ensure that your \passthrough{\lstinline!.Renviron!}
  ends with a newline.
\item
  Save your \passthrough{\lstinline!.Renviron!} file.
\item
  Restart R for changes to take effect. You can restart R via the drop-down
  menu on the ``Session'' tab. The ``Session'' tab is at the top of the RStudio
  interface.
\end{enumerate}

After following these steps, you should be able to successfully install the
package which threw the error above.

\hypertarget{intro}{%
\chapter{The Roadmap for Targeted Learning}\label{intro}}

\hypertarget{learning-objectives-1}{%
\section{Learning Objectives}\label{learning-objectives-1}}

By the end of this chapter you will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Translate scientific questions to statistical questions.
\item
  Define a statistical model based on the knowledge of the experiment that
  generated the data.
\item
  Identify a causal parameter as a function of the observed data distribution.
\item
  Explain the following causal and statistical assumptions and their
  implications: i.i.d., consistency, interference, positivity, SUTVA.
\end{enumerate}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The roadmap of statistical learning is concerned with the translation from
real-world data applications to a mathematical and statistical formulation of
the relevant estimation problem. This involves data as a random variable having
a probability distribution, scientific knowledge represented by a statistical
model, a statistical target parameter representing an answer to the question of
interest, and the notion of an estimator and sampling distribution of the
estimator.

\hypertarget{the-roadmap}{%
\section{The Roadmap}\label{the-roadmap}}

Following the roadmap is a process of five stages.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data as a random variable with a probability distribution, \(O \sim P_0\).
\item
  The statistical model \(\mathcal{M}\) such that \(P_0 \in \mathcal{M}\).
\item
  The statistical target parameter \(\Psi\) and estimand \(\Psi(P_0)\).
\item
  The estimator \(\hat{\Psi}\) and estimate \(\hat{\Psi}(P_n)\).
\item
  A measure of uncertainty for the estimate \(\hat{\Psi}(P_n)\).
\end{enumerate}

\hypertarget{data-as-a-random-variable-with-a-probability-distribution-o-sim-p_0}{%
\subsection*{\texorpdfstring{(1) Data as a random variable with a probability distribution, \(O \sim P_0\)}{(1) Data as a random variable with a probability distribution, O \textbackslash sim P\_0}}\label{data-as-a-random-variable-with-a-probability-distribution-o-sim-p_0}}


The data set we're confronted with is the result of an experiment and we can
view the data as a random variable, \(O\), because if we repeat the experiment
we would have a different realization of this experiment. In particular, if we
repeat the experiment many times we could learn the probability distribution,
\(P_0\), of our data. So, the observed data \(O\) with probability distribution
\(P_0\) are \(n\) independent identically distributed (i.i.d.) observations of the
random variable \(O; O_1, \ldots, O_n\). Note that while not all data are i.i.d.,
there are ways to handle non-i.i.d. data, such as establishing conditional
independence, stratifying data to create sets of identically distributed data,
etc. It is crucial that researchers be absolutely clear about what they actually
know about the data-generating distribution for a given problem of interest.
Unfortunately, communication between statisticians and researchers is often
fraught with misinterpretation. The roadmap provides a mechanism by which to
ensure clear communication between research and statistician -- it truly helps
with this communication!

\hypertarget{the-empirical-probability-measure-p_n}{%
\subsubsection*{\texorpdfstring{The empirical probability measure, \(P_n\)}{The empirical probability measure, P\_n}}\label{the-empirical-probability-measure-p_n}}


Once we have \(n\) of such i.i.d. observations we have an empirical probability
measure, \(P_n\). The empirical probability measure is an approximation of the
true probability measure \(P_0\), allowing us to learn from our data. For
example, we can define the empirical probability measure of a set, \(A\), to be
the proportion of observations which end up in \(A\). That is,
\begin{equation*}
  P_n(A) = \frac{1}{n}\sum_{i=1}^{n} \mathbb{I}(O_i \in A)
\end{equation*}

In order to start learning something, we need to ask \emph{``What do we know about the
probability distribution of the data?''} This brings us to Step 2.

\hypertarget{the-statistical-model-mathcalm-such-that-p_0-in-mathcalm}{%
\subsection*{\texorpdfstring{(2) The statistical model \(\mathcal{M}\) such that \(P_0 \in \mathcal{M}\)}{(2) The statistical model \textbackslash mathcal\{M\} such that P\_0 \textbackslash in \textbackslash mathcal\{M\}}}\label{the-statistical-model-mathcalm-such-that-p_0-in-mathcalm}}


The statistical model \(\mathcal{M}\) is defined by the question we asked at the
end of \ref{step1}. It is defined as the set of possible probability
distributions for our observed data. Often \(\mathcal{M}\) is very large (possibly
infinite-dimensional), to reflect the fact that statistical knowledge is
limited. In the case that \(\mathcal{M}\) is infinite-dimensional, we deem this a
nonparametric statistical model.

Alternatively, if the probability distribution of the data at hand is described
by a finite number of parameters, then the statistical model is parametric. In
this case, we prescribe to the belief that the random variable \(O\) being
observed has, e.g., a normal distribution with mean \(\mu\) and variance
\(\sigma^2\). More formally, a parametric model may be defined
\begin{equation*}
  \mathcal{M} = \{P_{\theta} : \theta \in \mathcal{R}^d \}
\end{equation*}

Sadly, the assumption that the data-generating distribution has a specific,
parametric forms is all-too-common, even when such is a leap of faith. This
practice of oversimplification in the current culture of data analysis typically
derails any attempt at trying to answer the scientific question at hand; alas,
such statements as the ever-popular quip of Box that ``All models are wrong but
some are useful,'' encourage the data analyst to make arbitrary choices even when
that often force significant differences in answers to the same estimation
problem. The Targeted Learning paradigm does not suffer from this bias since it
defines the statistical model through a representation of the true
data-generating distribution corresponding to the observed data.

Now, on to Step 3: \emph{``What are we trying to learn from the data?"}

\hypertarget{the-statistical-target-parameter-psi-and-estimand-psip_0}{%
\subsection*{\texorpdfstring{(3) The statistical target parameter \(\Psi\) and estimand \(\Psi(P_0)\)}{(3) The statistical target parameter \textbackslash Psi and estimand \textbackslash Psi(P\_0)}}\label{the-statistical-target-parameter-psi-and-estimand-psip_0}}


The statistical target parameter, \(\Psi\), is defined as a mapping from the
statistical model, \(\mathcal{M}\), to the parameter space (i.e., a real number)
\(\mathcal{R}\). That is, \(\Psi: \mathcal{M}\rightarrow\mathbb{R}\). The target
parameter may be seen as a representation of the
quantity that we wish to learn from the data, the answer to a well-specified
(often causal) question of interest. In contrast to purely statistical target
parameters, causal target parameters require \emph{identification from the observed
data}, based on causal models that include several untestable assumptions,
described in more detail in the section on \protect\hyperlink{causal}{causal target parameters}.

For a simple example, consider a data set which contains observations of a
survival time on every subject, for which our question of interest is ``What's
the probability that someone lives longer than five years?'' We have,
\begin{equation*}
  \Psi(P_0) = \mathbb{P}(O > 5)
\end{equation*}

This answer to this question is the \textbf{estimand, \(\Psi(P_0)\)}, which is the
quantity we're trying to learn from the data. Once we have defined \(O\),
\(\mathcal{M}\) and \(\Psi(P_0)\) we have formally defined the statistical
estimation problem.

\hypertarget{the-estimator-hatpsi-and-estimate-hatpsip_n}{%
\subsection*{\texorpdfstring{(4) The estimator \(\hat{\Psi}\) and estimate \(\hat{\Psi}(P_n)\)}{(4) The estimator \textbackslash hat\{\textbackslash Psi\} and estimate \textbackslash hat\{\textbackslash Psi\}(P\_n)}}\label{the-estimator-hatpsi-and-estimate-hatpsip_n}}


To obtain a good approximation of the estimand, we need an estimator, an \emph{a
priori}-specified algorithm defined as a mapping from the set of possible
empirical distributions, \(P_n\), which live in a non-parametric statistical
model, \(\mathcal{M}_{NP}\) (\(P_n \in \mathcal{M}_{NP}\)), to the parameter space
of the parameter of interest. That is, \(\hat{\Psi} : \mathcal{M}_{NP} \rightarrow \mathbb{R}^d\). The estimator is a function that takes as input
the observed data, a realization of \(P_n\), and gives as output a value in the
parameter space, which is the \textbf{estimate, \(\hat{\Psi}(P_n)\)}.

Where the estimator may be seen as an operator that maps the observed data and
corresponding empirical distribution to a value in the parameter space, the
numerical output that produced such a function is the estimate. Thus, it is an
element of the parameter space based on the empirical probability distribution
of the observed data. If we plug in a realization of \(P_n\) (based on a sample
size \(n\) of the random variable \(O\)), we get back an estimate \(\hat{\Psi}(P_n)\)
of the true parameter value \(\Psi(P_0)\).

In order to quantify the uncertainty in our estimate of the target parameter
(i.e., to construct statistical inference), an understanding of the sampling
distribution of our estimator will be necessary. This brings us to Step 5.

\hypertarget{a-measure-of-uncertainty-for-the-estimate-hatpsip_n}{%
\subsection*{\texorpdfstring{(5) A measure of uncertainty for the estimate \(\hat{\Psi}(P_n)\)}{(5) A measure of uncertainty for the estimate \textbackslash hat\{\textbackslash Psi\}(P\_n)}}\label{a-measure-of-uncertainty-for-the-estimate-hatpsip_n}}


Since the estimator \(\hat{\Psi}\) is a function of the empirical
distribution \(P_n\), the estimator itself is a random variable with a sampling
distribution. So, if we repeat the experiment of drawing \(n\) observations we
would every time end up with a different realization of our estimate and our
estimator has a sampling distribution. The sampling distribution of an estimator
can be theoretically validated to be approximately normally distributed by a
Central Limit Theorem (CLT).

A class of \textbf{Central Limit Theorems} (CLTs) are statements regarding the
convergence of the \textbf{sampling distribution of an estimator} to a normal
distribution. In general, we will construct estimators whose limit sampling
distributions may be shown to be approximately normal distributed as sample size
increases. For large enough \(n\) we have,
\begin{equation*}
  \hat{\Psi}(P_n) \sim N \left(\Psi(P_0), \frac{\sigma^2}{n}\right),
\end{equation*}
permitting statistical inference. Now, we can proceed to quantify the
uncertainty of our chosen estimator by construction of hypothesis tests and
confidence intervals. For example, we may construct a confidence interval at
level \((1 - \alpha)\) for our estimand, \(\Psi(P_0)\):
\begin{equation*}
  \hat{\Psi}(P_n) \pm z_{1 - \frac{\alpha}{2}}
    \left(\frac{\sigma}{\sqrt{n}}\right),
\end{equation*}
where \(z_{1 - \frac{\alpha}{2}}\) is the \((1 - \frac{\alpha}{2})^\text{th}\)
quantile of the standard normal distribution. Often, we will be interested in
constructing 95\% confidence intervals, corresponding to mass \(\alpha = 0.05\) in
either tail of the limit distribution; thus, we will typically take
\(z_{1 - \frac{\alpha}{2}} \approx 1.96\).

\emph{Note:} we will typically have to estimate the standard error,
\(\frac{\sigma}{\sqrt{n}}\).

A 95\% confidence interval means that if we were to take 100 different samples
of size \(n\) and compute a 95\% confidence interval for each sample then
approximately 95 of the 100 confidence intervals would contain the estimand,
\(\Psi(P_0)\). More practically, this means that there is a 95\% probability
(or 95\% confidence) that the confidence interval procedure will contain the
true estimand. However, any single estimated confidence interval either will
contain the true estimand or will not.

\hypertarget{summary-of-the-roadmap}{%
\section{Summary of the Roadmap}\label{summary-of-the-roadmap}}

Data, \(O\), is viewed as a random variable that has a probability distribution.
We often have \(n\) units of independent identically distributed units with
probability distribution \(P_0\) (\(O_1, \ldots, O_n \sim P_0\)). We have
statistical knowledge about the experiment that generated this data. In other
words, we make a statement that the true data distribution \(P_0\) falls in a
certain set called a statistical model, \(\mathcal{M}\). Often these sets are very
large because statistical knowledge is very limited so these statistical models
are often infinite dimensional models. Our statistical query is, ``What are we
trying to learn from the data?'' denoted by the statistical target parameter,
\(\Psi\), which maps the \(P_0\) into the estimand, \(\Psi(P_0)\). At this point the
statistical estimation problem is formally defined and now we will need
statistical theory to guide us in the construction of estimators. There's a lot
of statistical theory we will review in this course that, in particular, relies
on the Central Limit Theorem, allowing us to come up with estimators that are
approximately normally distributed and also allowing us to come with statistical
inference (i.e., confidence intervals and hypothesis tests).

\hypertarget{causal}{%
\section{Causal Target Parameters}\label{causal}}

\hypertarget{the-causal-model}{%
\subsection{The Causal Model}\label{the-causal-model}}

After formalizing the data and the statistical model, we can define a causal
model to express causal parameters of interest. Directed acyclic graphs (DAGs)
are one useful tool to express what we know about the causal relations among
variables. Ignoring exogenous \(U\) terms (explained below), we assume the
following ordering of the variables in the observed data \(O\).

\textbackslash begin\{center\}\includegraphics[width=0.8\linewidth]{03-intro-roadmap_files/figure-latex/unnamed-chunk-1-1}

While directed acyclic graphs (DAGs) like above provide a convenient means by
which to visualize causal relations between variables, the same causal
relations among variables can be represented via a set of structural equations,
which define the non-parametric structural equation model (NPSEM):
\begin{align*}
  W &= f_W(U_W) \\
  A &= f_A(W, U_A) \\
  Y &= f_Y(W, A, U_Y),
\end{align*}
where \(U_W\), \(U_A\), and \(U_Y\) represent the unmeasured exogenous background
characteristics that influence the value of each variable. In the NPSEM, \(f_W\),
\(f_A\) and \(f_Y\) denote that each variable (for \(W\), \(A\) and \(Y\), respectively)
is a function of its parents and unmeasured background characteristics, but note
that there is no imposition of any particular functional constraints(e.g.,
linear, logit-linear, only one interaction, etc.). For this reason, they are
called non-parametric structural equation models (NPSEMs). The
DAG and set of nonparametric structural equations represent exactly the same
information and so may be used interchangeably.

The first hypothetical experiment we will consider is assigning exposure to the
whole population and observing the outcome, and then assigning no exposure to
the whole population and observing the outcome. On the nonparametric structural
equations, this corresponds to a comparison of the outcome distribution in the
population under two interventions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(A\) is set to \(1\) for all individuals, and
\item
  \(A\) is set to \(0\) for all individuals.
\end{enumerate}

These interventions imply two new nonparametric structural equation models. For
the case \(A = 1\), we have
\begin{align*}
  W &= f_W(U_W) \\
  A &= 1 \\
  Y(1) &= f_Y(W, 1, U_Y),
\end{align*}
and for the case \(A=0\),
\begin{align*}
  W &= f_W(U_W) \\
  A &= 0 \\
  Y(0) &= f_Y(W, 0, U_Y).
\end{align*}

In these equations, \(A\) is no longer a function of \(W\) because we have
intervened on the system, setting \(A\) deterministically to either of the values
\(1\) or \(0\). The new symbols \(Y(1)\) and \(Y(0)\) indicate the outcome variable in
our population if it were generated by the respective NPSEMs above; these are
often called \emph{counterfactuals} (since they run contrary-to-fact). The difference
between the means of the outcome under these two interventions defines a
parameter that is often called the ``average treatment effect'' (ATE), denoted
\begin{equation}\label{eqn:ate}
  ATE = \mathbb{E}_X(Y(1)-Y(0)),
\end{equation}
where \(\mathbb{E}_X\) is the mean under the theoretical (unobserved) full data
\(X = (W, Y(1), Y(0))\).

Note, we can define much more complicated interventions on NPSEM's, such as
interventions based upon rules (themselves based upon covariates), stochastic
rules, etc. and each results in a different targeted parameter and entails
different identifiability assumptions discussed below.

\hypertarget{identifiability}{%
\subsection{Identifiability}\label{identifiability}}

Because we can never observe both \(Y(0)\) (the counterfactual outcome when \(A=0\))
and \(Y(1)\) (similarly, the counterfactual outcome when \(A=1\)), we cannot
estimate \ref{eqn:ate} directly. Instead, we have to make assumptions under
which this quantity may be estimated from the observed data \(O \sim P_0\) under
the data-generating distribution \(P_0\). Fortunately, given the causal model
specified in the NPSEM above, we can, with a handful of untestable assumptions,
estimate the ATE, even from observational data. These assumptions may be
summarized as follows

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The causal graph implies \(Y(a) \perp A\) for all \(a \in \mathcal{A}\), which
  is the \emph{randomization} assumption. In the case of observational data, the
  analogous assumption is \emph{strong ignorability} or \emph{no unmeasured confounding}
  \(Y(a) \perp A \mid W\) for all \(a \in \mathcal{A}\);
\item
  Although not represented in the causal graph, also required is the assumption
  of no interference between units, that is, the outcome for unit \(i\) \(Y_i\) is
  not affected by exposure for unit \(j\) \(A_j\) unless \(i=j\);
\item
  \emph{Consistency} of the treatment mechanism is also required, i.e., the outcome
  for unit \(i\) is \(Y_i(a)\) whenever \(A_i = a\), an assumption also known as ``no
  other versions of treatment'';
\item
  It is also necessary that all observed units, across strata defined by \(W\),
  have a bounded (non-deterministic) probability of receiving treatment --
  that is, \(0 < \mathbb{P}(A = a \mid W) < 1\) for all \(a\) and \(W\)). This assumption
  is referred to as \emph{positivity} or \emph{overlap}.
\end{enumerate}

\emph{Remark}: Together, (2) and (3), the assumptions of no interference and
consistency, respectively, are jointly referred to as the \emph{stable unit
treatment value assumption} (SUTVA).

Given these assumptions, the ATE may be re-written as a function of \(P_0\),
specifically
\begin{equation}\label{eqn:estimand}
  ATE = \mathbb{E}_0(Y(1) - Y(0)) = \mathbb{E}_0
    \left(\mathbb{E}_0[Y \mid A = 1, W] - \mathbb{E}_0[Y \mid A = 0, W]\right),
\end{equation}
or the difference in the predicted outcome values for each subject, under the
contrast of treatment conditions (\(A = 0\) vs.~\(A = 1\)), in the population,
averaged over all observations. Thus, a parameter of a theoretical ``full'' data
distribution can be represented as an estimand of the observed data
distribution. Significantly, there is nothing about the representation in
\ref{eqn:estimand} that requires parameteric assumptions; thus, the regressions
on the right hand side may be estimated freely with machine learning. With
different parameters, there will be potentially different identifiability
assumptions and the resulting estimands can be functions of different components
of \(P_0\).

\hypertarget{the-wash-benefits-example-dataset}{%
\section{The WASH Benefits Example Dataset}\label{the-wash-benefits-example-dataset}}

The data come from a study of the effect of water quality, sanitation, hand
washing, and nutritional interventions on child development in rural Bangladesh
(WASH Benefits Bangladesh): a cluster-randomised controlled trial
\citep{luby2018effects}. The study enrolled pregnant women in their first or second
trimester from the rural villages of Gazipur, Kishoreganj, Mymensingh, and
Tangail districts of central Bangladesh, with an average of eight women per
cluster. Groups of eight geographically adjacent clusters were block-randomised,
using a random number generator, into six intervention groups (all of which
received weekly visits from a community health promoter for the first 6 months
and every 2 weeks for the next 18 months) and a double-sized control group (no
intervention or health promoter visit). The six intervention groups were:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  chlorinated drinking water;
\item
  improved sanitation;
\item
  hand-washing with soap;
\item
  combined water, sanitation, and hand washing;
\item
  improved nutrition through counseling and provision of lipid-based nutrient
  supplements; and
\item
  combined water, sanitation, handwashing, and nutrition.
\end{enumerate}

In the workshop, we concentrate on child growth (size for age) as the outcome of
interest. For reference, this trial was registered with ClinicalTrials.gov as
NCT01590095.

\begin{lstlisting}[language=R]
library(tidyverse)

# read in data
dat <- read_csv("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv")
dat
# A tibble: 4,695 x 28
    whz tr      fracode month  aged sex    momage momedu momheight hfiacat Nlt18
  <dbl> <chr>   <chr>   <dbl> <dbl> <chr>   <dbl> <chr>      <dbl> <chr>   <dbl>
1  0    Control N05265      9   268 male       30 Prima~      146. Food S~     3
2 -1.16 Control N05265      9   286 male       25 Prima~      149. Modera~     2
3 -1.05 Control N08002      9   264 male       25 Prima~      152. Food S~     1
4 -1.26 Control N08002      9   252 female     28 Prima~      140. Food S~     3
5 -0.59 Control N06531      9   336 female     19 Secon~      151. Food S~     2
# ... with 4,690 more rows, and 17 more variables: Ncomp <dbl>, watmin <dbl>,
#   elec <dbl>, floor <dbl>, walls <dbl>, roof <dbl>, asset_wardrobe <dbl>,
#   asset_table <dbl>, asset_chair <dbl>, asset_khat <dbl>, asset_chouki <dbl>,
#   asset_tv <dbl>, asset_refrig <dbl>, asset_bike <dbl>, asset_moto <dbl>,
#   asset_sewmach <dbl>, asset_mobile <dbl>
\end{lstlisting}

For the purposes of this workshop, we we start by treating the data as independent
and identically distributed (i.i.d.) random draws from a very large target
population. We could, with available options, account for the clustering of the
data (within sampled geographic units), but, for simplification, we avoid these
details in these workshop presentations, although modifications of our
methodology for biased samples, repeated measures, etc., are available.

We have 28 variables measured, of which 1 variable is set to be the outcome of
interest. This outcome, \(Y\), is the weight-for-height Z-score (\passthrough{\lstinline!whz!} in \passthrough{\lstinline!dat!});
the treatment of interest, \(A\), is the randomized treatment group (\passthrough{\lstinline!tr!} in
\passthrough{\lstinline!dat!}); and the adjustment set, \(W\), consists simply of \emph{everything else}. This
results in our observed data structure being \(n\) i.i.d. copies of \(O_i = (W_i, A_i, Y_i)\), for \(i = 1, \ldots, n\).

Using the \href{https://CRAN.R-project.org/package=skimr}{\passthrough{\lstinline!skimr!} package}, we can
quickly summarize the variables measured in the WASH Benefits data set:

\begin{tabular}{l|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
skim\_type & skim\_variable & n\_missing & complete\_rate & character.min & character.max & character.empty & character.n\_unique & character.whitespace & numeric.mean & numeric.sd & numeric.p0 & numeric.p25 & numeric.p50 & numeric.p75 & numeric.p100\\
\hline
character & tr & 0 & 1.00000 & 3 & 15 & 0 & 7 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & fracode & 0 & 1.00000 & 2 & 6 & 0 & 20 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & sex & 0 & 1.00000 & 4 & 6 & 0 & 2 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & momedu & 0 & 1.00000 & 12 & 15 & 0 & 3 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
character & hfiacat & 0 & 1.00000 & 11 & 24 & 0 & 4 & 0 & NA & NA & NA & NA & NA & NA & NA\\
\hline
numeric & whz & 0 & 1.00000 & NA & NA & NA & NA & NA & -0.58608 & 1.03212 & -4.67 & -1.28 & -0.6 & 0.08 & 4.97\\
\hline
numeric & month & 0 & 1.00000 & NA & NA & NA & NA & NA & 6.45474 & 3.33214 & 1.00 & 4.00 & 6.0 & 9.00 & 12.00\\
\hline
numeric & aged & 0 & 1.00000 & NA & NA & NA & NA & NA & 266.31502 & 52.17465 & 42.00 & 230.00 & 266.0 & 303.00 & 460.00\\
\hline
numeric & momage & 18 & 0.99617 & NA & NA & NA & NA & NA & 23.90592 & 5.24055 & 14.00 & 20.00 & 23.0 & 27.00 & 60.00\\
\hline
numeric & momheight & 31 & 0.99340 & NA & NA & NA & NA & NA & 150.50407 & 5.22667 & 120.65 & 147.05 & 150.6 & 154.06 & 168.00\\
\hline
numeric & Nlt18 & 0 & 1.00000 & NA & NA & NA & NA & NA & 1.60469 & 1.24726 & 0.00 & 1.00 & 1.0 & 2.00 & 10.00\\
\hline
numeric & Ncomp & 0 & 1.00000 & NA & NA & NA & NA & NA & 11.04324 & 6.35044 & 2.00 & 6.00 & 10.0 & 14.00 & 52.00\\
\hline
numeric & watmin & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.94867 & 9.48125 & 0.00 & 0.00 & 0.0 & 1.00 & 600.00\\
\hline
numeric & elec & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.59510 & 0.49092 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & floor & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.10671 & 0.30878 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & walls & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.71502 & 0.45145 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & roof & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.98530 & 0.12035 & 0.00 & 1.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_wardrobe & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.16720 & 0.37319 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_table & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.73440 & 0.44170 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_chair & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.73440 & 0.44170 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_khat & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.61321 & 0.48707 & 0.00 & 0.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_chouki & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.78126 & 0.41344 & 0.00 & 1.00 & 1.0 & 1.00 & 1.00\\
\hline
numeric & asset\_tv & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.30394 & 0.46001 & 0.00 & 0.00 & 0.0 & 1.00 & 1.00\\
\hline
numeric & asset\_refrig & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.07945 & 0.27046 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_bike & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.31906 & 0.46616 & 0.00 & 0.00 & 0.0 & 1.00 & 1.00\\
\hline
numeric & asset\_moto & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.06603 & 0.24836 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_sewmach & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.06475 & 0.24611 & 0.00 & 0.00 & 0.0 & 0.00 & 1.00\\
\hline
numeric & asset\_mobile & 0 & 1.00000 & NA & NA & NA & NA & NA & 0.85857 & 0.34850 & 0.00 & 1.00 & 1.0 & 1.00 & 1.00\\
\hline
\end{tabular}

A convenient summary of the relevant variables is given just above, complete
with a small visualization describing the marginal characteristics of each
covariate. Note that the \emph{asset} variables reflect socioeconomic status of the
study participants. Notice also the uniform distribution of the treatment groups
(with twice as many controls); this is, of course, by design.

\hypertarget{sl3}{%
\chapter{Super (Machine) Learning}\label{sl3}}

\emph{Ivana Malenica} and \emph{Rachael Phillips}

Based on the \href{https://github.com/tlverse/sl3}{\passthrough{\lstinline!sl3!} \passthrough{\lstinline!R!} package} by \emph{Jeremy
Coyle, Nima Hejazi, Ivana Malenica, Rachael Phillips, and Oleg Sofrygin}.

Updated: 2021-10-12

\hypertarget{learning-objectives-2}{%
\section*{Learning Objectives}\label{learning-objectives-2}}


By the end of this chapter you will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Select an objective function that (i) aligns with the intention of the
  analysis and (ii) is optimized by the target parameter.
\item
  Assemble a diverse library of learners to be considered in the Super Learner
  ensemble. In particular, you should be able to:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Customize a learner by modifying it's tuning parameters.
  \item
    Create several different versions of the same learner at once by
    specifying a grid of tuning parameters.
  \item
    Curate covariate screening pipelines in order to pass a screener's
    output, a subset of covariates, as input for another learner that will
    use the subset of covariates selected by the screener to model the data.
  \end{enumerate}
\item
  Specify the learner for ensembling (the metalearner) such that it corresponds
  to your objective function.
\item
  Fit the Super Learner ensemble with nested cross-validation to obtain an
  estimate of the performance of the ensemble itself on out-of-sample data.
\item
  Obtain \passthrough{\lstinline!sl3!} variable importance metrics.
\item
  Interpret the fit for discrete and continuous Super Learners' from the
  cross-validated risk table and the coefficients.
\item
  Justify the base library of machine learning algorithms and the ensembling
  learner in terms of the prediction problem, statistical model \(\M\), data
  sparsity, and the dimensionality of the covariates.
\end{enumerate}

\hypertarget{motivation-1}{%
\section*{Motivation}\label{motivation-1}}


\begin{itemize}
\tightlist
\item
  A common task in data analysis is prediction -- using the observed data (input
  variables and outcomes) to learn a function that can map new input variables
  into a predicted outcome.
\item
  For some data, algorithms that learn complex relationships between variables
  are necessary to adequately model the data. For other data, main terms
  regression models might fit the data quite well.
\item
  It is generally impossible to know a priori which algorithm will be the best
  for a given data set and prediction problem.
\item
  The Super Learner solves this issue of algorithm selection by creating an
  ensemble of many algorithms, from the simplest (intercept-only) to most
  complex (neural nets, tree-based methods, support vector machines, etc.).
\item
  Super Learner works by using cross-validation in a manner that theoretically
  (in large samples) guarantees the resulting fit will be as good as possible,
  given the algorithms provided.
\end{itemize}

\hypertarget{introduction-1}{%
\section*{Introduction}\label{introduction-1}}


In \protect\hyperlink{intro}{Chapter 1}, we introduced the \protect\hyperlink{roadmap}{\emph{Roadmap for Targeted
Learning}} as a general template to translate real-world data
applications into formal statistical estimation problems. The first steps of
this roadmap define the \emph{statistical estimation problem}, which establish

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The data \$O\$ as a random variable, or equivalently, a realization of a}
  \textbf{particular experiment/study, which has probability distribution \(P_0\).}
  This is written \(O \sim P_0\), and \(P_0\) is also commonly referred to as the
  data-generating process (DGP) and also the data-generating distribution
  (DGD). The data structure \(O\) is comprised of variables, such as a
  vector of covariates \(W\), a treatment or exposure \(A\), and an outcome \(Y\),
  \(O=(W,A,Y) \sim P_0\). We often observe the random variable \(O\) \(n\) times, by
  repeating the common experiment \(n\) times. For example, \(O_1,\ldots, O_n\)
  random variables could be the result of a random sample of \(n\) subjects from
  a population, collecting baseline characteristics \(W\), randomly assigning
  treatment \(A\), and then later measuring an outcome \(Y\).
\item
  \textbf{A statistical model \(\M\) as a set of possible probability distributions}
  \textbf{that could have given rise to the data.} It's essential for \(\M\) to only
  constrained by factual subject-matter knowledge in order to guarantee \(P_0\)
  resides in the statistical model, written \(P_0 \in \M\). Continuing
  the example from step 1, the following restrictions could be placed on the
  statistical model: the \(O_1, \ldots, O_n\) observations in the data are
  independent and identically distributed (i.i.d.), the assignment of
  treatment \(A\) was random and not based on covariates \(W\).
\item
  \textbf{A translation of the scientific question of interest into a function of}
  \textbf{\(P_0\), the target statistical estimand \(\Psi(P_0)\).} For example, we might
  be interested in the average difference in mean outcomes under treatment
  \(A=1\) versus placebo \(A=0\):
  \(\Psi(P_0)=E_{P_0}\Big[E_{P_0}(Y|A=1,W)−E_{P_0}(Y|A=0,W)\Big]\). Note
  that, if the scientific question is causal, then it's translation will
  produce a target \emph{causal} estimand; another layer of translation,
  identifiability, is required to express the target causal estimand as a
  function of the observed data distribution \(P_0\). See \protect\hyperlink{causal}{causal target
  parameters} for more information on causal quantities, causal models
  and identifiability.
  Note that if the target estimand is causal, step 3 also requires establishing
  so-called ``identifiability'' of this estimand from the observed data. See \protect\hyperlink{causal}{causal
  target parameters} for more detail on causal models and identifiability.
\end{enumerate}

After finalizing steps 1--3 above, the estimation procedure can be specified.
We advocate for the use of the Super Learner (SL) algorithm in the estimation
procedure it is flexible and grounded in optimality theory \citep{vdl2007super}.

\hypertarget{why-use-the-super-learner}{%
\subsection*{Why use the Super Learner?}\label{why-use-the-super-learner}}


\begin{itemize}
\tightlist
\item
  It offers a system for combining/ensembling many machine learning (ML)
  algorithms into an improved ML algorithm.
\item
  In large samples, SL is proven to perform at least as well as the unknown
  best candidate ML algorithm \citep{vdl2003unified, vaart2006oracle}.
\item
  When we have tested different ML algorithms on actual data and looked at the
  performance, never does one algorithm always win (see below).
\end{itemize}

\begin{center}\includegraphics[width=0.8\linewidth]{img/png/ericSL} \end{center}

The figure above shows the performance of several different ML algorithms,
including the SL, across many datasets. Here, the performance was assessed with
the relative mean squared error and the target estimand that was being estimated
for all datasets was the conditional mean outcome, given covariates
\citep{polley2010super}.

\hypertarget{general-overview-of-the-algorithm}{%
\subsection{General Overview of the Algorithm}\label{general-overview-of-the-algorithm}}

\begin{itemize}
\tightlist
\item
  SL uses cross-validation and an objective function (e.g., loss function) to
  optimize the fit of the target parameter, based on a weighted combination of a
  so-called ``library'' of candidate ML algorithms.
\item
  The library of ML algorithms consists of functions (``learners'' in the SL
  nomenclature). These functions should align with the statistical model, both
  in terms of it's vastness and any potential constraints, where the
  statistical model's constraints are restrictions based on subject-matter
  knowledge regarding the process that generated the data.
\item
  The so-called ``metalearning'', or ensembling of the library of ML algorithms
  has been shown to be adaptive and robust, even in small
  samples \citep{polley2010super}.
\end{itemize}

\hypertarget{cross-validation}{%
\subsubsection{Cross-validation}\label{cross-validation}}

\begin{itemize}
\tightlist
\item
  There are many different cross-validation schemes, which are designed to
  accommodate different study designs, data structures, and prediction
  problems. See \protect\hyperlink{origami}{cross-validation} for more detail.
\end{itemize}

\begin{center}\includegraphics[width=0.8\linewidth]{img/png/vs} \end{center}

The figure above shows an example of \(V\)-fold cross-validation with \(V=10\)
folds, and this is the default cross-validation structure in the \passthrough{\lstinline!sl3!} \passthrough{\lstinline!R!}
package. The darker boxes represent the so-called ``validation data'' and the
lighter boxes represent the so-called ``training data''. The following details
are important to notice:

\begin{itemize}
\tightlist
\item
  Across all folds, there are \(V\) (10) copies of the dataset. The only
  difference between each copy is the coloring, which distinguishes the subset
  of the data that's considered as the training data from the subset that's
  considered as the validation data.
\item
  Within each fold 1/\(V\) (1/10) of the data is the validation data.
\item
  Across all folds, all of the data will be considered as validation data and
  no observation will be included twice as validation data. Therefore, the
  total number of validation data observations across all of the folds is
  equal to the total number of observations in the data.
\end{itemize}

\hypertarget{step-by-step-procedure-with-v-fold-cross-validation}{%
\subsubsection{Step-by-step procedure with V-fold cross-validation}\label{step-by-step-procedure-with-v-fold-cross-validation}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Fit each learner (say there are \(K\) learners) on the whole dataset. We refer
  to these learners that are trained on the whole dataset as ``full-fit''
  learners.
\item
  Break up the data evenly into \(V\) disjoint subsets. Separately, create
  \(V\) copies of the data. For each copy \(v\), where \(v=1,\ldots,V\), create the
  \(V\) folds by labelling the portion of the data that was included in subset
  \(v\) as the validation sample, and the labelling what's remaining of the data
  as the training sample.
\item
  For each fold \(v\), \(v=1,\ldots,V\), fit each learner (say there are \(K\)
  learners) on the training sample and predict the validation sample outcomes
  by providing each fitted learner with the validation sample covariates as
  input. Notice that each learner will be fit \(V\) times. We refer to these
  learners that are trained across the \(V\) cross-validation folds as
  ``cross-validated fit'' learners.
\item
  Combine the validation sample predictions from all folds and all learners to
  create the so-called \(K\) column matrix of ``cross-validated predictions''.
  This matrix is also commonly referred to as the \(Z\) matrix. Notice that it
  contains, for each learner, out-of-sample predictions for all of the
  observations in the data.
\item
  Train the metalearner (e.g., a non-negative least squares regression) on
  data with predictors and outcomes being the \(Z\) matrix and the observed data
  outcomes, respectively. The metalearner --- just like any ordinary ML
  algorithm --- estimates the parameters of it's model using the training data
  and afterwards, the fitted model can be used to obtain predicted outcomes
  from new input data. What's special about the metalearner is that it's
  estimated model parameters (e.g., regression coefficients) correspond to
  it's predictors, which are the variables in the \(Z\) matrix, the \(K\) learners'
  predictions. Once the metalearner is fit, it can be used to obtain predicted
  outcomes from new input data; that is, new \(K\) learners predictions' can be
  supplied to the fitted metalearner in order to obtain predicted outcomes.
\item
  The fitted metalearner and the full-fit learners define the weighted
  combination of the \(K\) learners, finalizing the Super Learner (SL) fit. To
  obtain SL predictions the full-fit learners' predictions are first obtained
  and then fed as input to the fitted metalearner; the metalearner's output
  is the SL predictions.
\end{enumerate}

\begin{center}\includegraphics[width=0.8\linewidth]{img/png/SLKaiserNew} \end{center}

\hypertarget{how-to-pick-the-library-of-candidate-ml-algorithms}{%
\subsubsection{How to pick the library of candidate ML algorithms?}\label{how-to-pick-the-library-of-candidate-ml-algorithms}}

\begin{itemize}
\tightlist
\item
  The library of candidate ML algorithms should be chosen based on contextual
  knowledge regarding the study/experiment that generated the data, and on the
  information available in the data.\\
\item
  Having a ``go-to'' library to use as a ``default'' when the sample size is
  relatively large can be useful in practice.
\item
  The algorithms may range from a simple linear regression model to multi-step
  algorithms involving screening covariates, penalizations, optimizing tuning
  parameters, etc.
\end{itemize}

\hypertarget{theoretical-foundations}{%
\subsubsection{Theoretical Foundations}\label{theoretical-foundations}}

For more detail on Super Learner algorithm we refer the reader to
\citet{polley2010super} and \citet{vdl2007super}. The optimality results for the
cross-validation selector among a family of algorithms were established in
\citet{vdl2003unified} and extended in \citet{vaart2006oracle}.

\hypertarget{sl3-microwave-dinner-implementation}{%
\section*{\texorpdfstring{\texttt{sl3} ``Microwave Dinner'' Implementation}{sl3 ``Microwave Dinner'' Implementation}}\label{sl3-microwave-dinner-implementation}}


We begin by illustrating the core functionality of the SL algorithm as
implemented in \passthrough{\lstinline!sl3!}.

The \passthrough{\lstinline!sl3!} implementation consists of the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{-1}
\tightlist
\item
  Load the necessary libraries and data
\item
  Define the machine learning task
\item
  Make an SL by creating library of base learners and a metalearner
\item
  Train the SL on the machine learning task
\item
  Obtain predicted values
\end{enumerate}

\hypertarget{wash-benefits-study-example}{%
\subsection*{WASH Benefits Study Example}\label{wash-benefits-study-example}}


Using the WASH Benefits Bangladesh data, we are interested in predicting
weight-for-height z-score \passthrough{\lstinline!whz!} using the available covariate data. More
information on this dataset, and all other data that we will work with, are
described in \href{ihttps://tlverse.org/tlverse-handbook/data.html}{this chapter of the \passthrough{\lstinline!tlverse!}
handbook}. Let's begin!

\hypertarget{load-the-necessary-libraries-and-data}{%
\subsection*{0. Load the necessary libraries and data}\label{load-the-necessary-libraries-and-data}}


First, we will load the relevant \passthrough{\lstinline!R!} packages, set a seed, and load the data.

\begin{lstlisting}[language=R]
library(data.table)
library(dplyr)
library(readr)
library(ggplot2)
library(SuperLearner)
library(origami)
library(sl3)
library(knitr)
library(kableExtra)

# load data set and take a peek
washb_data <- fread(
  paste0(
    "https://raw.githubusercontent.com/tlverse/tlverse-data/master/",
    "wash-benefits/washb_data.csv"
  ),
  stringsAsFactors = TRUE
)
if (is_latex_output()) {
  head(washb_data) %>%
    kable(format = "latex")
} else if (is_html_output()) {
   head(washb_data) %>%
     kable() %>%
     kable_styling(fixed_thead = TRUE) %>%
     scroll_box(width = "100%", height = "300px")
}
\end{lstlisting}

\begin{tabular}{r|l|l|r|r|l|r|l|r|l|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
whz & tr & fracode & month & aged & sex & momage & momedu & momheight & hfiacat & Nlt18 & Ncomp & watmin & elec & floor & walls & roof & asset\_wardrobe & asset\_table & asset\_chair & asset\_khat & asset\_chouki & asset\_tv & asset\_refrig & asset\_bike & asset\_moto & asset\_sewmach & asset\_mobile\\
\hline
0.00 & Control & N05265 & 9 & 268 & male & 30 & Primary (1-5y) & 146.40 & Food Secure & 3 & 11 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.16 & Control & N05265 & 9 & 286 & male & 25 & Primary (1-5y) & 148.75 & Moderately Food Insecure & 2 & 4 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.05 & Control & N08002 & 9 & 264 & male & 25 & Primary (1-5y) & 152.15 & Food Secure & 1 & 10 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-1.26 & Control & N08002 & 9 & 252 & female & 28 & Primary (1-5y) & 140.25 & Food Secure & 3 & 5 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1\\
\hline
-0.59 & Control & N06531 & 9 & 336 & female & 19 & Secondary (>5y) & 150.95 & Food Secure & 2 & 7 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
-0.51 & Control & N06531 & 9 & 304 & male & 20 & Secondary (>5y) & 154.20 & Severely Food Insecure & 0 & 3 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1\\
\hline
\end{tabular}

\hypertarget{define-the-machine-learning-task}{%
\subsection*{1. Define the machine learning task}\label{define-the-machine-learning-task}}


To define the machine learning \passthrough{\lstinline!task!} (predict weight-for-height Z-score
\passthrough{\lstinline!whz!} using the available covariate data), we need to create an \passthrough{\lstinline!sl3\_Task!}
object.

The \passthrough{\lstinline!sl3\_Task!} keeps track of the roles the variables play in the machine
learning problem, the data, and any metadata (e.g., observational-level
weights, IDs, offset).

Also, if we had missing outcomes, we would need to set \passthrough{\lstinline!drop\_missing\_outcome = TRUE!} when we create the task. In the next analysis, with the \protect\hyperlink{ist}{IST stroke trial
data}, we do have a missing outcome. In the following chapter, we need to
estimate this missingness mechanism; which is the conditional probably that
the outcome is observed, given the history (i.e., variables that were measured
before the missingness). Estimating the missingness mechanism requires learning
a prediction function that outputs the predicted probability that a unit
is missing, given their history.

\begin{lstlisting}[language=R]
# specify the outcome and covariates
outcome <- "whz"
covars <- colnames(washb_data)[-which(names(washb_data) == outcome)]

# create the sl3 task
washb_task <- make_sl3_Task(
  data = washb_data,
  covariates = covars,
  outcome = outcome
)
Warning in process_data(data, nodes, column_names = column_names, flag = flag, :
Missing covariate data detected: imputing covariates.
\end{lstlisting}

\emph{This warning is important.} The task just imputed missing covariates for us.
Specifically, for each covariate column with missing values, \passthrough{\lstinline!sl3!} uses the
median to impute missing continuous covariates, and the mode to impute binary
and categorical covariates.

Also, for each covariate column with missing values, \passthrough{\lstinline!sl3!} adds an additional
column indicating whether or not the value was imputed, which is particularly
handy when the missingness in the data might be informative.

Also, notice that we did not specify the number of folds, or the loss function
in the task. The default cross-validation scheme is V-fold, with \(V=10\) number
of folds.

Let's visualize our \passthrough{\lstinline!washb\_task!}:

\begin{lstlisting}[language=R]
washb_task
A sl3 Task with 4695 obs and these nodes:
$covariates
 [1] "tr"              "fracode"         "month"           "aged"           
 [5] "sex"             "momage"          "momedu"          "momheight"      
 [9] "hfiacat"         "Nlt18"           "Ncomp"           "watmin"         
[13] "elec"            "floor"           "walls"           "roof"           
[17] "asset_wardrobe"  "asset_table"     "asset_chair"     "asset_khat"     
[21] "asset_chouki"    "asset_tv"        "asset_refrig"    "asset_bike"     
[25] "asset_moto"      "asset_sewmach"   "asset_mobile"    "delta_momage"   
[29] "delta_momheight"

$outcome
[1] "whz"

$id
NULL

$weights
NULL

$offset
NULL

$time
NULL
\end{lstlisting}

We can't see when we print the task, but the default cross-validation fold
structure (\(V\)-fold cross-validation with \(V\)=10 folds) was created when we
defined the task.

\begin{lstlisting}[language=R]
length(washb_task$folds) # how many folds?
[1] 10

head(washb_task$folds[[1]]$training_set) # row indexes for fold 1 training
[1] 1 2 3 4 5 6
head(washb_task$folds[[1]]$validation_set) # row indexes for fold 1 validation
[1] 12 21 29 41 43 53

any(
  washb_task$folds[[1]]$training_set %in% washb_task$folds[[1]]$validation_set
)
[1] FALSE
\end{lstlisting}

Tip: If you type \passthrough{\lstinline!washb\_task$!} and then press the tab button (you will
need to press tab twice if you're not in RStudio), you can view all of the
active and public fields and methods that can be accessed from the \passthrough{\lstinline!washb\_task!}
object.

\hypertarget{make-a-super-learner}{%
\subsection*{2. Make a Super Learner}\label{make-a-super-learner}}


Now that we have defined our machine learning problem with the \passthrough{\lstinline!sl3\_Task!}, we
are ready to make the Super Learner (SL). This requires specification of

\begin{itemize}
\tightlist
\item
  A set of candidate machine learning algorithms, also commonly referred to as
  a library of learners. The set should include a diversity of algorithms
  that are believed to be consistent with the true data-generating distribution.
\item
  A metalearner, to ensemble the base learners.
\end{itemize}

We might also incorporate

\begin{itemize}
\tightlist
\item
  Feature selection, to pass only a subset of the predictors to the algorithm.
\item
  Hyperparameter specification, to tune base learners.
\end{itemize}

Learners have properties that indicate what features they support. We may use
\passthrough{\lstinline!sl3\_list\_properties()!} to get a list of all properties supported by at least
one learner.

\begin{lstlisting}[language=R]
sl3_list_properties()
 [1] "binomial"      "categorical"   "continuous"    "cv"           
 [5] "density"       "h2o"           "ids"           "importance"   
 [9] "offset"        "preprocessing" "sampling"      "screener"     
[13] "timeseries"    "weights"       "wrapper"      
\end{lstlisting}

Since we have a continuous outcome, we may identify the learners that support
this outcome type with \passthrough{\lstinline!sl3\_list\_learners()!}.

\begin{lstlisting}[language=R]
sl3_list_learners("continuous")
 [1] "Lrnr_arima"                     "Lrnr_bartMachine"              
 [3] "Lrnr_bayesglm"                  "Lrnr_bilstm"                   
 [5] "Lrnr_bound"                     "Lrnr_caret"                    
 [7] "Lrnr_cv_selector"               "Lrnr_dbarts"                   
 [9] "Lrnr_earth"                     "Lrnr_expSmooth"                
[11] "Lrnr_gam"                       "Lrnr_gbm"                      
[13] "Lrnr_glm"                       "Lrnr_glm_fast"                 
[15] "Lrnr_glmnet"                    "Lrnr_grf"                      
[17] "Lrnr_gru_keras"                 "Lrnr_gts"                      
[19] "Lrnr_h2o_glm"                   "Lrnr_h2o_grid"                 
[21] "Lrnr_hal9001"                   "Lrnr_HarmonicReg"              
[23] "Lrnr_hts"                       "Lrnr_lightgbm"                 
[25] "Lrnr_lstm_keras"                "Lrnr_mean"                     
[27] "Lrnr_multiple_ts"               "Lrnr_nnet"                     
[29] "Lrnr_nnls"                      "Lrnr_optim"                    
[31] "Lrnr_pkg_SuperLearner"          "Lrnr_pkg_SuperLearner_method"  
[33] "Lrnr_pkg_SuperLearner_screener" "Lrnr_polspline"                
[35] "Lrnr_randomForest"              "Lrnr_ranger"                   
[37] "Lrnr_rpart"                     "Lrnr_rugarch"                  
[39] "Lrnr_screener_correlation"      "Lrnr_solnp"                    
[41] "Lrnr_stratified"                "Lrnr_svm"                      
[43] "Lrnr_tsDyn"                     "Lrnr_xgboost"                  
\end{lstlisting}

Now that we have an idea of some learners, we can construct them using the
\passthrough{\lstinline!make\_learner!} function or the \passthrough{\lstinline!new!} method.

\begin{lstlisting}[language=R]
# choose base learners
lrn_glm <- make_learner(Lrnr_glm)
lrn_mean <- Lrnr_mean$new()
\end{lstlisting}

We can customize learner hyperparameters to incorporate a diversity of different
settings. Documentation for the learners and their hyperparameters can be found
in the \href{https://tlverse.org/sl3/reference/index.html\#section-sl-learners}{\passthrough{\lstinline!sl3!} Learners
Reference}.

\begin{lstlisting}[language=R]
lrn_lasso <- make_learner(Lrnr_glmnet) # alpha default is 1
lrn_ridge <- Lrnr_glmnet$new(alpha = 0)
lrn_enet.5 <- make_learner(Lrnr_glmnet, alpha = 0.5)

lrn_polspline <- Lrnr_polspline$new()

lrn_ranger100 <- make_learner(Lrnr_ranger, num.trees = 100)

lrn_hal_faster <- Lrnr_hal9001$new(max_degree = 2, reduce_basis = 0.05)

xgb_fast <- Lrnr_xgboost$new() # default with nrounds = 20 is pretty fast
xgb_50 <- Lrnr_xgboost$new(nrounds = 50)
\end{lstlisting}

We can use \passthrough{\lstinline!Lrnr\_define\_interactions!} to define interaction terms among
covariates. The interactions should be supplied as list of character vectors,
where each vector specifies an interaction. For example, we specify
interactions below between (1) \passthrough{\lstinline!tr!} (whether or not the subject received the
WASH intervention) and \passthrough{\lstinline!elec!} (whether or not the subject had electricity); and
between (2) \passthrough{\lstinline!tr!} and \passthrough{\lstinline!hfiacat!} (the subject's level of food security).

\begin{lstlisting}[language=R]
interactions <- list(c("elec", "tr"), c("tr", "hfiacat"))
# main terms as well as the interactions above will be included
lrn_interaction <- make_learner(Lrnr_define_interactions, interactions)
\end{lstlisting}

What we just defined above is incomplete. In order to fit learners with these
interactions, we need to create a \passthrough{\lstinline!Pipeline!}. A \passthrough{\lstinline!Pipeline!} is a set of learners
to be fit sequentially, where the fit from one learner is used to define the
task for the next learner. We need to create a \passthrough{\lstinline!Pipeline!} with the interaction
defining learner and another learner that incorporate these terms when fitting
a model. Let's create a learner pipeline that will fit a linear model with the
combination of main terms and interactions terms, as specified in
\passthrough{\lstinline!lrn\_interaction!}.

\begin{lstlisting}[language=R]
# we already instantiated a linear model learner, no need to do that again
lrn_glm_interaction <- make_learner(Pipeline, lrn_interaction, lrn_glm)
lrn_glm_interaction
[1] "Lrnr_define_interactions_TRUE"
[1] "Lrnr_glm_TRUE"
\end{lstlisting}

We can also include learners from the \passthrough{\lstinline!SuperLearner!} \passthrough{\lstinline!R!} package.

\begin{lstlisting}[language=R]
lrn_bayesglm <- Lrnr_pkg_SuperLearner$new("SL.bayesglm")
\end{lstlisting}

Here is a fun trick to create customized learners over a grid of parameters.

\begin{lstlisting}[language=R]
# I like to crock pot my SLs
grid_params <- list(
  cost = c(0.01, 0.1, 1, 10, 100, 1000),
  gamma = c(0.001, 0.01, 0.1, 1),
  kernel = c("polynomial", "radial", "sigmoid"),
  degree = c(1, 2, 3)
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
svm_learners <- apply(grid, MARGIN = 1, function(tuning_params) {
  do.call(Lrnr_svm$new, as.list(tuning_params))
})
\end{lstlisting}

\begin{lstlisting}[language=R]
grid_params <- list(
  max_depth = c(2, 4, 6),
  eta = c(0.001, 0.1, 0.3),
  nrounds = 100
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
grid
  max_depth   eta nrounds
1         2 0.001     100
2         4 0.001     100
3         6 0.001     100
4         2 0.100     100
5         4 0.100     100
6         6 0.100     100
7         2 0.300     100
8         4 0.300     100
9         6 0.300     100

xgb_learners <- apply(grid, MARGIN = 1, function(tuning_params) {
  do.call(Lrnr_xgboost$new, as.list(tuning_params))
})
xgb_learners
[[1]]
[1] "Lrnr_xgboost_100_1_2_0.001"

[[2]]
[1] "Lrnr_xgboost_100_1_4_0.001"

[[3]]
[1] "Lrnr_xgboost_100_1_6_0.001"

[[4]]
[1] "Lrnr_xgboost_100_1_2_0.1"

[[5]]
[1] "Lrnr_xgboost_100_1_4_0.1"

[[6]]
[1] "Lrnr_xgboost_100_1_6_0.1"

[[7]]
[1] "Lrnr_xgboost_100_1_2_0.3"

[[8]]
[1] "Lrnr_xgboost_100_1_4_0.3"

[[9]]
[1] "Lrnr_xgboost_100_1_6_0.3"
\end{lstlisting}

Did you see \passthrough{\lstinline!Lrnr\_caret!} when we called \passthrough{\lstinline!sl3\_list\_learners(c("binomial"))!}? All
we need to specify to use this popular algorithm as a candidate in our SL is
the \passthrough{\lstinline!algorithm!} we want to tune, which is passed as \passthrough{\lstinline!method!} to \passthrough{\lstinline!caret::train()!}.

\begin{lstlisting}[language=R]
# Unlike xgboost, I have no idea how to tune a neural net or BART machine, so
# I let caret take the reins
lrnr_caret_nnet <- make_learner(Lrnr_caret, algorithm = "nnet")
lrnr_caret_bartMachine <- make_learner(Lrnr_caret,
  algorithm = "bartMachine",
  method = "boot", metric = "Accuracy",
  tuneLength = 10
)
\end{lstlisting}

In order to assemble the library of learners, we need to \passthrough{\lstinline!Stack!} them
together.

A \passthrough{\lstinline!Stack!} is a special learner and it has the same interface as all other
learners. What makes a stack special is that it combines multiple learners by
training them simultaneously, so that their predictions can be either combined
or compared.

\begin{lstlisting}[language=R]
stack <- make_learner(
  Stack, lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50
)
stack
[1] "Lrnr_glm_TRUE"                                  
[2] "Lrnr_polspline_5"                               
[3] "Lrnr_glmnet_NULL_deviance_10_0.5_100_TRUE_FALSE"
[4] "Lrnr_glmnet_NULL_deviance_10_0_100_TRUE_FALSE"  
[5] "Lrnr_glmnet_NULL_deviance_10_1_100_TRUE_FALSE"  
[6] "Lrnr_xgboost_50_1"                              
\end{lstlisting}

We can also stack the learners by first creating a vector, and then
instantiating the stack. I prefer this method, since it easily allows us to
modify the names of the learners.

\begin{lstlisting}[language=R]
# named vector of learners first
learners <- c(
  lrn_glm, lrn_polspline, lrn_enet.5, lrn_ridge, lrn_lasso, xgb_50
)
names(learners) <- c(
  "glm", "polspline", "enet.5", "ridge", "lasso", "xgboost50"
)
# next make the stack
stack <- make_learner(Stack, learners)
# now the names are pretty
stack
[1] "glm"       "polspline" "enet.5"    "ridge"     "lasso"     "xgboost50"
\end{lstlisting}

We're jumping ahead a bit, but let's check something out quickly. It's
straightforward, and just one more step, to set up this stack such that
all of the learners will train in a cross-validated manner.

\begin{lstlisting}[language=R]
cv_stack <- Lrnr_cv$new(stack)
cv_stack
[1] "Lrnr_cv"
[1] "glm"       "polspline" "enet.5"    "ridge"     "lasso"     "xgboost50"
\end{lstlisting}

\hypertarget{screening-algorithms-for-feature-selection}{%
\subsubsection*{Screening Algorithms for Feature Selection}\label{screening-algorithms-for-feature-selection}}


We can optionally select a subset of available covariates and pass only
those variables to the modeling algorithm. The current set of learners that
can be used for prescreening covariates is included below.

\begin{itemize}
\tightlist
\item
  \passthrough{\lstinline!Lrnr\_screener\_importance!} selects \passthrough{\lstinline!num\_screen!} (default = 5) covariates
  based on the variable importance ranking provided by the \passthrough{\lstinline!learner!}. Any
  learner with an importance method can be used in \passthrough{\lstinline!Lrnr\_screener\_importance!};
  and this currently includes \passthrough{\lstinline!Lrnr\_ranger!}, \passthrough{\lstinline!Lrnr\_randomForest!}, and
  \passthrough{\lstinline!Lrnr\_xgboost!}.
\item
  \passthrough{\lstinline!Lrnr\_screener\_coefs!}, which provides screening of covariates based on the
  magnitude of their estimated coefficients in a (possibly regularized) GLM.
  The \passthrough{\lstinline!threshold!} (default = 1e-3) defines the minimum absolute size of the
  coefficients, and thus covariates, to be kept. Also, a \passthrough{\lstinline!max\_retain!} argument
  can be optionally provided to restrict the number of selected covariates to be
  no more than \passthrough{\lstinline!max\_retain!}.
\item
  \passthrough{\lstinline!Lrnr\_screener\_correlation!} provides covariate screening procedures by
  running a test of correlation (Pearson default), and then selecting the (1)
  top ranked variables (default), or (2) the variables with a pvalue lower than
  some pre-specified threshold.
\item
  \passthrough{\lstinline!Lrnr\_screener\_augment!} augments a set of screened covariates with additional
  covariates that should be included by default, even if the screener did not
  select them. An example of how to use this screener is included below.
\end{itemize}

Let's consider screening covariates based on their \passthrough{\lstinline!randomForest!} variable
importance ranking (ordered by mean decrease in accuracy). To select the top
5 most important covariates according to this ranking, we can combine
\passthrough{\lstinline!Lrnr\_screener\_importance!} with \passthrough{\lstinline!Lrnr\_ranger!} (limiting the number of trees by
setting \passthrough{\lstinline!ntree = 20!}).

Hang on! Before you think it -- we will confess: Bob Ross and us both know that
20 trees makes for a lonely forest, and we shouldn't consider it, but these are
the sacrifices we make for this chapter to be built in time!

\begin{lstlisting}[language=R]
miniforest <- Lrnr_ranger$new(
  num.trees = 20, write.forest = FALSE,
  importance = "impurity_corrected"
)

# learner must already be instantiated, we did this when we created miniforest
screen_rf <- Lrnr_screener_importance$new(learner = miniforest, num_screen = 5)
screen_rf
[1] "Lrnr_screener_importance_5"

# which covariates are selected on the full data?
screen_rf$train(washb_task)
[1] "Lrnr_screener_importance_5"
$selected
[1] "aged"         "month"        "momedu"       "Nlt18"        "asset_refrig"
\end{lstlisting}

An example of how to format \passthrough{\lstinline!Lrnr\_screener\_augment!} is included below for
clarity.

\begin{lstlisting}[language=R]
keepme <- c("aged", "momage")
# screener must already be instantiated, we did this when we created screen_rf
screen_augment_rf <- Lrnr_screener_augment$new(
  screener = screen_rf, default_covariates = keepme
)
screen_augment_rf
[1] "Lrnr_screener_augment_c(\"aged\", \"momage\")"
\end{lstlisting}

Selecting covariates with non-zero lasso coefficients is quite common. Let's
construct \passthrough{\lstinline!Lrnr\_screener\_coefs!} screener that does just that, and test it
out.

\begin{lstlisting}[language=R]
# we already instantiated a lasso learner above, no need to do it again
screen_lasso <- Lrnr_screener_coefs$new(learner = lrn_lasso, threshold = 0)
screen_lasso
[1] "Lrnr_screener_coefs_0_NULL_2"
\end{lstlisting}

To pipe only the selected covariates to the modeling algorithm, we need to
make a \passthrough{\lstinline!Pipeline!}, similar to the one we built for the regression model with
interaction terms.

\begin{lstlisting}[language=R]
screen_rf_pipe <- make_learner(Pipeline, screen_rf, stack)
screen_lasso_pipe <- make_learner(Pipeline, screen_lasso, stack)
\end{lstlisting}

Now, these learners will be preceded by a screening step.

We also consider the original \passthrough{\lstinline!stack!}, to compare how the feature selection
methods perform in comparison to the methods without feature selection.

Analogous to what we have seen before, we have to stack the pipeline and
original \passthrough{\lstinline!stack!} together, so we may use them as base learners in our super
learner.

\begin{lstlisting}[language=R]
# pretty names again
learners2 <- c(learners, screen_rf_pipe, screen_lasso_pipe)
names(learners2) <- c(names(learners), "randomforest_screen", "lasso_screen")

fancy_stack <- make_learner(Stack, learners2)
fancy_stack
[1] "glm"                 "polspline"           "enet.5"             
[4] "ridge"               "lasso"               "xgboost50"          
[7] "randomforest_screen" "lasso_screen"       
\end{lstlisting}

We will use the \href{https://tlverse.org/sl3/reference/default_metalearner.html}{default
metalearner},
which uses
\href{https://tlverse.org/sl3/reference/Lrnr_solnp.html}{\passthrough{\lstinline!Lrnr\_solnp!}} to
provide fitting procedures for a pairing of \href{https://tlverse.org/sl3/reference/loss_functions.html}{loss
function} and
\href{https://tlverse.org/sl3/reference/metalearners.html}{metalearner
function}. This
default metalearner selects a loss and metalearner pairing based on the outcome
type. Note that any learner can be used as a metalearner.

Now that we have made a diverse stack of base learners, we are ready to make the
SL. The SL algorithm fits a metalearner on the validation set
predictions/losses across all folds.

\begin{lstlisting}[language=R]
sl <- make_learner(Lrnr_sl, learners = fancy_stack)
\end{lstlisting}

We can also use \passthrough{\lstinline!Lrnr\_cv!} to build a SL, cross-validate a stack of
learners to compare performance of the learners in the stack, or cross-validate
any single learner (see ``Cross-validation'' section of this \href{https://tlverse.org/sl3/articles/intro_sl3.html}{\passthrough{\lstinline!sl3!}
introductory tutorial}).

Furthermore, we can \href{https://tlverse.org/sl3/articles/custom_lrnrs.html}{Define New \passthrough{\lstinline!sl3!}
Learners} which can be used
in all the places you could otherwise use any other \passthrough{\lstinline!sl3!} learners, including
\passthrough{\lstinline!Pipelines!}, \passthrough{\lstinline!Stacks!}, and the SL.

Recall that the discrete SL, or cross-validated selector, is a metalearner that
assigns a weight of 1 to the learner with the lowest cross-validated empirical
risk, and weight of 0 to all other learners. This metalearner specification can
be invoked with \passthrough{\lstinline!Lrnr\_cv\_selector!}.

\begin{lstlisting}[language=R]
discrete_sl_metalrn <- Lrnr_cv_selector$new()
discrete_sl <- Lrnr_sl$new(
  learners = fancy_stack,
  metalearner = discrete_sl_metalrn
)
\end{lstlisting}

\hypertarget{train-the-super-learner-on-the-machine-learning-task}{%
\subsection*{3. Train the Super Learner on the machine learning task}\label{train-the-super-learner-on-the-machine-learning-task}}


The SL algorithm fits a metalearner on the validation-set predictions in a
cross-validated manner, thereby avoiding overfitting.

Now we are ready to \passthrough{\lstinline!train!} our SL on our \passthrough{\lstinline!sl3\_task!} object, \passthrough{\lstinline!washb\_task!}.

\begin{lstlisting}[language=R]
set.seed(4197)
sl_fit <- sl$train(washb_task)
\end{lstlisting}

\hypertarget{obtain-predicted-values}{%
\subsection*{4. Obtain predicted values}\label{obtain-predicted-values}}


Now that we have fit the SL, we are ready to calculate the predicted outcome
for each subject.

\begin{lstlisting}[language=R]
# we did it! now we have SL predictions
sl_preds <- sl_fit$predict()
head(sl_preds)
[1] -0.61014 -0.76503 -0.69304 -0.65588 -0.65689 -0.64811
\end{lstlisting}

We can also obtain a summary of the results.

\begin{lstlisting}[language=R]
sl_fit$cv_risk(loss_fun = loss_squared_error)
                          learner coefficients   risk       se  fold_sd
 1:                           glm     0.055568 1.0202 0.023955 0.067500
 2:                     polspline     0.055554 1.0208 0.023577 0.067921
 3:                        enet.5     0.055561 1.0133 0.023618 0.065940
 4:                         ridge     0.055566 1.0154 0.023736 0.065991
 5:                         lasso     0.055561 1.0130 0.023605 0.065828
 6:                     xgboost50     0.055589 1.1136 0.025262 0.077580
 7:       randomforest_screen_glm     0.055549 1.0229 0.023733 0.067706
 8: randomforest_screen_polspline     0.055567 1.0178 0.023753 0.067091
 9:    randomforest_screen_enet.5     0.055549 1.0229 0.023729 0.067516
10:     randomforest_screen_ridge     0.055548 1.0232 0.023746 0.067528
11:     randomforest_screen_lasso     0.055549 1.0226 0.023722 0.067520
12: randomforest_screen_xgboost50     0.055548 1.1207 0.025542 0.088250
13:              lasso_screen_glm     0.055556 1.0165 0.023561 0.064937
14:        lasso_screen_polspline     0.055556 1.0184 0.023537 0.065581
15:           lasso_screen_enet.5     0.055556 1.0164 0.023561 0.064912
16:            lasso_screen_ridge     0.055555 1.0166 0.023567 0.064759
17:            lasso_screen_lasso     0.055556 1.0164 0.023560 0.064898
18:        lasso_screen_xgboost50     0.055511 1.1296 0.025767 0.084180
19:                  SuperLearner           NA 1.0122 0.023500 0.068013
    fold_min_risk fold_max_risk
 1:       0.89442        1.1200
 2:       0.89892        1.1255
 3:       0.88927        1.1082
 4:       0.88570        1.1104
 5:       0.89037        1.1090
 6:       0.96019        1.2337
 7:       0.89923        1.1081
 8:       0.90342        1.1098
 9:       0.89912        1.1080
10:       0.89882        1.1073
11:       0.89912        1.1076
12:       0.95622        1.2330
13:       0.90204        1.1156
14:       0.89742        1.1162
15:       0.90184        1.1154
16:       0.90132        1.1148
17:       0.90183        1.1154
18:       0.96251        1.2327
19:       0.88510        1.1087
\end{lstlisting}

\hypertarget{cross-validated-super-learner}{%
\section*{Cross-validated Super Learner}\label{cross-validated-super-learner}}


We can cross-validate the SL to see how well the SL performs on unseen data, and
obtain an estimate of the cross-validated risk of the SL.

This estimation procedure requires an outer/external layer of
cross-validation, also called nested cross-validation, which involves setting
aside a separate holdout sample that we don't use to fit the SL. This external
cross-validation procedure may also incorporate 10 folds, which is the default
in \passthrough{\lstinline!sl3!}. However, we will incorporate 2 outer/external folds of
cross-validation for computational efficiency.

We also need to specify a loss function to evaluate SL. Documentation for the
available loss functions can be found in the \href{https://tlverse.org/sl3/reference/loss_functions.html}{\passthrough{\lstinline!sl3!} Loss Function
Reference}.

\begin{lstlisting}[language=R]
washb_task_new <- make_sl3_Task(
  data = washb_data,
  covariates = covars,
  outcome = outcome,
  folds = origami::make_folds(washb_data, fold_fun = folds_vfold, V = 2)
)

CVsl <- CV_lrnr_sl(
  lrnr_sl = sl_fit, task = washb_task_new, loss_fun = loss_squared_error
)

if (is_latex_output()) {
   CVsl %>%
     kable(format = "latex")
} else if (is_html_output()) {
   CVsl %>%
     kable() %>%
     kable_styling(fixed_thead = TRUE) %>%
     scroll_box(width = "100%", height = "300px")
}
\end{lstlisting}

\begin{tabular}{l|r|r|r|r|r|r}
\hline
learner & coefficients & risk & se & fold\_sd & fold\_min\_risk & fold\_max\_risk\\
\hline
glm & 0.05556 & 1.0448 & 0.02548 & 0.06295 & 1.00030 & 1.0893\\
\hline
polspline & 0.05556 & 1.0206 & 0.02354 & 0.05104 & 0.98449 & 1.0567\\
\hline
enet.5 & 0.05557 & 1.0191 & 0.02366 & 0.05005 & 0.98372 & 1.0545\\
\hline
ridge & 0.05557 & 1.0264 & 0.02404 & 0.05195 & 0.98963 & 1.0631\\
\hline
lasso & 0.05556 & 1.0201 & 0.02370 & 0.05134 & 0.98375 & 1.0563\\
\hline
xgboost50 & 0.05555 & 1.1871 & 0.02737 & 0.03028 & 1.16566 & 1.2085\\
\hline
randomforest\_screen\_glm & 0.05557 & 1.0171 & 0.02370 & 0.05001 & 0.98176 & 1.0525\\
\hline
randomforest\_screen\_polspline & 0.05556 & 1.0231 & 0.02376 & 0.05456 & 0.98449 & 1.0616\\
\hline
randomforest\_screen\_enet.5 & 0.05557 & 1.0171 & 0.02370 & 0.04994 & 0.98183 & 1.0525\\
\hline
randomforest\_screen\_ridge & 0.05557 & 1.0171 & 0.02372 & 0.05005 & 0.98173 & 1.0525\\
\hline
randomforest\_screen\_lasso & 0.05557 & 1.0172 & 0.02370 & 0.04992 & 0.98185 & 1.0525\\
\hline
randomforest\_screen\_xgboost50 & 0.05549 & 1.1967 & 0.02737 & 0.04170 & 1.16725 & 1.2262\\
\hline
lasso\_screen\_glm & 0.05556 & 1.0197 & 0.02372 & 0.04487 & 0.98793 & 1.0514\\
\hline
lasso\_screen\_polspline & 0.05556 & 1.0229 & 0.02374 & 0.04655 & 0.98997 & 1.0558\\
\hline
lasso\_screen\_enet.5 & 0.05556 & 1.0195 & 0.02372 & 0.04475 & 0.98790 & 1.0512\\
\hline
lasso\_screen\_ridge & 0.05556 & 1.0198 & 0.02374 & 0.04513 & 0.98785 & 1.0517\\
\hline
lasso\_screen\_lasso & 0.05556 & 1.0195 & 0.02372 & 0.04474 & 0.98791 & 1.0512\\
\hline
lasso\_screen\_xgboost50 & 0.05549 & 1.1884 & 0.02720 & 0.08492 & 1.12834 & 1.2484\\
\hline
SuperLearner & NA & 1.0179 & 0.02379 & 0.04688 & 0.98479 & 1.0511\\
\hline
\end{tabular}

\hypertarget{variable-importance-measures-with-sl3}{%
\section*{\texorpdfstring{Variable Importance Measures with \texttt{sl3}}{Variable Importance Measures with sl3}}\label{variable-importance-measures-with-sl3}}


Variable importance can be interesting and informative. It can also be
contradictory and confusing. Nevertheless, we like it, and so do our
collaborators, so we created a variable importance function in \passthrough{\lstinline!sl3!}! The \passthrough{\lstinline!sl3!}
\passthrough{\lstinline!importance!} function returns a table with variables listed in decreasing order
of importance (i.e., most important on the first row).

The measure of importance in \passthrough{\lstinline!sl3!} is based on a risk ratio, or risk difference,
between the learner fit with a removed, or permuted, covariate and the learner
fit with the true covariate, across all covariates. In this manner, the larger
the risk difference, the more important the variable is in the prediction.

The intuition of this measure is that it calculates the risk (in terms of the
average loss in predictive accuracy) of losing one covariate, while keeping
everything else fixed, and compares it to the risk if the covariate was not
lost. If this risk ratio is one, or risk difference is zero, then losing that
covariate had no impact, and is thus not important by this measure. We do this
across all of the covariates. As stated above, we can remove the covariate and
refit the SL without it, or we just permute the covariate (faster)
and hope for the shuffling to distort any meaningful information that was
present in the covariate. This idea of permuting instead of removing saves a lot
of time, and is also incorporated in the \passthrough{\lstinline!randomForest!} variable importance
measures. However, the permutation approach is risky, so the importance function
default is to remove and refit.

Let's explore the \passthrough{\lstinline!sl3!} variable importance measurements for the \passthrough{\lstinline!washb!} data.

\begin{lstlisting}[language=R]
washb_varimp <- importance(sl_fit, loss = loss_squared_error, type = "permute")

if (is_latex_output()) {
  washb_varimp %>%
    kable(format = "latex")
} else if (is_html_output()) {
  washb_varimp %>%
    kable() %>%
    kable_styling(fixed_thead = TRUE) %>%
    scroll_box(width = "100%", height = "300px")
}
\end{lstlisting}

\begin{tabular}{l|r}
\hline
X & risk\_ratio\\
\hline
aged & 1.03318\\
\hline
momedu & 1.01312\\
\hline
asset\_refrig & 1.00649\\
\hline
asset\_chair & 1.00400\\
\hline
month & 1.00312\\
\hline
tr & 1.00143\\
\hline
Nlt18 & 1.00104\\
\hline
elec & 1.00079\\
\hline
momheight & 1.00054\\
\hline
asset\_chouki & 1.00053\\
\hline
walls & 1.00048\\
\hline
asset\_tv & 1.00047\\
\hline
watmin & 1.00046\\
\hline
asset\_mobile & 1.00038\\
\hline
hfiacat & 1.00037\\
\hline
asset\_sewmach & 1.00033\\
\hline
sex & 1.00015\\
\hline
asset\_moto & 1.00015\\
\hline
asset\_table & 1.00015\\
\hline
floor & 1.00015\\
\hline
momage & 1.00013\\
\hline
delta\_momheight & 1.00003\\
\hline
asset\_bike & 0.99998\\
\hline
roof & 0.99994\\
\hline
delta\_momage & 0.99992\\
\hline
Ncomp & 0.99977\\
\hline
asset\_khat & 0.99976\\
\hline
fracode & 0.99929\\
\hline
asset\_wardrobe & 0.99916\\
\hline
\end{tabular}

\begin{lstlisting}[language=R]
# plot variable importance
importance_plot(
  washb_varimp,
  main = "sl3 Variable Importance for WASH Benefits Example Data"
)
\end{lstlisting}

\begin{center}\includegraphics[width=1\linewidth]{04-sl3_files/figure-latex/varimp-plot-1} \end{center}

\hypertarget{sl3-exercises}{%
\section{Exercises}\label{sl3-exercises}}

\hypertarget{sl3ex1}{%
\subsection{\texorpdfstring{Predicting Myocardial Infarction with \texttt{sl3}}{Predicting Myocardial Infarction with sl3}}\label{sl3ex1}}

Follow the steps below to predict myocardial infarction (\passthrough{\lstinline!mi!}) using the
available covariate data. We thank Prof.~David Benkeser at Emory University for
making the this Cardiovascular Health Study (CHS) data accessible.

\begin{lstlisting}[language=R]
# load the data set
db_data <- url(
  paste0(
    "https://raw.githubusercontent.com/benkeser/sllecture/master/",
    "chspred.csv"
  )
)
chspred <- read_csv(file = db_data, col_names = TRUE)

# take a quick peek
if (is_latex_output()) {
   head(chspred) %>%
     kable(format = "latex")
} else if (is_html_output()) {
   head(chspred) %>%
     kable() %>%
     kable_styling(fixed_thead = TRUE) %>%
     scroll_box(width = "100%", height = "300px")
}
\end{lstlisting}

\begin{tabular}{r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r}
\hline
waist & alcoh & hdl & beta & smoke & ace & ldl & bmi & aspirin & gend & age & estrgn & glu & ins & cysgfr & dm & fetuina & whr & hsed & race & logcystat & logtrig & logcrp & logcre & health & logkcal & sysbp & mi\\
\hline
110.164 & 0.0000 & 66.497 & 0 & 0 & 1 & 114.216 & 27.997 & 0 & 0 & 73.518 & 0 & 159.931 & 70.3343 & 75.008 & 1 & 0.17516 & 1.16898 & 1 & 1 & -0.34202 & 5.4063 & 2.01260 & -0.67385 & 0 & 4.3926 & 177.135 & 0\\
\hline
89.976 & 0.0000 & 50.065 & 0 & 0 & 0 & 103.777 & 20.893 & 0 & 0 & 61.772 & 0 & 153.389 & 33.9695 & 82.743 & 1 & 0.57165 & 0.90114 & 0 & 0 & -0.08465 & 4.8592 & 3.29328 & -0.55509 & 1 & 6.2071 & 136.374 & 0\\
\hline
106.194 & 8.4174 & 40.506 & 0 & 0 & 0 & 165.716 & 28.455 & 1 & 1 & 72.931 & 0 & 121.715 & -17.3017 & 74.699 & 0 & 0.35168 & 1.17971 & 0 & 1 & -0.44511 & 4.5088 & 0.30132 & -0.01152 & 0 & 6.7320 & 135.199 & 0\\
\hline
90.057 & 0.0000 & 36.175 & 0 & 0 & 0 & 45.203 & 23.961 & 0 & 0 & 79.119 & 0 & 53.969 & 11.7315 & 95.782 & 0 & 0.54391 & 1.13599 & 0 & 0 & -0.48072 & 5.1832 & 3.02426 & -0.57507 & 1 & 7.3972 & 139.018 & 0\\
\hline
78.614 & 2.9790 & 71.064 & 0 & 1 & 0 & 131.312 & 10.966 & 0 & 1 & 69.018 & 0 & 94.315 & 9.7112 & 72.711 & 0 & 0.49159 & 1.10276 & 1 & 0 & 0.31206 & 4.2190 & -0.70568 & 0.00534 & 1 & 8.2779 & 88.047 & 0\\
\hline
91.659 & 0.0000 & 59.496 & 0 & 0 & 0 & 171.187 & 29.132 & 0 & 1 & 81.835 & 0 & 212.907 & -28.2269 & 69.218 & 1 & 0.46215 & 0.95291 & 1 & 0 & -0.28716 & 5.1773 & 0.97046 & 0.21268 & 1 & 5.9942 & 69.594 & 0\\
\hline
\end{tabular}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Create an \passthrough{\lstinline!sl3!} task, setting myocardial infarction \passthrough{\lstinline!mi!} as the outcome and
  using all available covariate data.
\item
  Make a library of seven relatively fast base learning algorithms. Customize
  tuning parameters for one of your learners. Feel free to use learners from
  \passthrough{\lstinline!sl3!} or \passthrough{\lstinline!SuperLearner!}. You may use the same base learning library that is
  presented above.
\item
  Incorporate at least one pipeline with feature selection. Any screener and
  learner(s) can be used.
\item
  With the default metalearner and base learners, make the Super Learner (SL)
  and train it on the task.
\item
  Print your SL fit by calling \passthrough{\lstinline!print()!} with \passthrough{\lstinline!$!}.
\item
  Cross-validate your SL fit to see how well it performs on unseen
  data. Specify a valid loss function to evaluate the SL.
\item
  Use the \passthrough{\lstinline!importance()!} function to identify the ``most important'' predictor of
  myocardial infarction, according to \passthrough{\lstinline!sl3!} importance metrics.
\end{enumerate}

\hypertarget{concluding-remarks}{%
\section{Concluding Remarks}\label{concluding-remarks}}

\begin{itemize}
\item
  Super Learner (SL) is a general approach that can be applied to a diversity of
  estimation and prediction problems which can be defined by a loss function.
\item
  It would be straightforward to plug in the estimator returned by SL into the
  target parameter mapping.

  \begin{itemize}
  \tightlist
  \item
    For example, suppose we are after the average treatment effect (ATE) of a
    binary treatment intervention:
    \(\Psi_0 = \mathbb{E}_{0,W}[\mathbb{E}_0(Y \mid A=1,W) -  \mathbb{E}_0(Y \mid A=0,W)]\).
  \item
    We could use the SL that was trained on the original data (let's call
    this \passthrough{\lstinline!sl\_fit!}) to predict the outcome for all subjects under each
    intervention. All we would need to do is take the average difference
    between the counterfactual outcomes under each intervention of interest.
  \item
    Considering \(\Psi_0\) above, we would first need two \(n\)-length vectors of
    predicted outcomes under each intervention. One vector would represent the
    predicted outcomes under an intervention that sets all subjects to
    receive \(A=1\), \(Y_i \mid A_i=1,W_i\) for all \(i=1,\ldots,n\). The other
    vector would represent the predicted outcomes under an intervention that
    sets all subjects to receive \(A=0\), \(Y_i \mid A_i=0,W_i\) for all
    \(i=1,\ldots,n\).
  \item
    After obtaining these vectors of counterfactual predicted outcomes, all
    we would need to do is average and then take the difference in order to
    plug-in the SL estimator into the target parameter mapping.
  \item
    In \passthrough{\lstinline!sl3!} and with our current ATE example, this could be achieved with
    \passthrough{\lstinline!mean(sl\_fit$predict(A1\_task)) - mean(sl\_fit$predict(A0\_task))!};
    where \passthrough{\lstinline!A1\_task$data!} would contain all 1's (or the level that pertains to
    receiving the treatment) for the treatment column in the data (keeping all
    else the same), and \passthrough{\lstinline!A0\_task$data!} would contain all 0's (or the level
    that pertains to not receiving the treatment) for the treatment column in
    the data.
  \end{itemize}
\item
  It's a worthwhile exercise to obtain the predicted counterfactual outcomes
  and create these counterfactual \passthrough{\lstinline!sl3!} tasks. It's too biased; however, to
  plug the SL fit into the target parameter mapping, (e.g., calling the result
  of \passthrough{\lstinline!mean(sl\_fit$predict(A1\_task)) - mean(sl\_fit$predict(A0\_task))!} the
  estimated ATE. We would end up with an estimator for the ATE that was
  optimized for estimation of the prediction function, and not the ATE!
\item
  Ultimately, we want an estimator that is optimized for our target estimand of
  interest. Here, we cared about doing a good job estimating the ATE. The
  SL is an essential step to help us get there. In fact, we will use the
  counterfactual predicted outcomes that were explained at length above.
  However, SL might not be not the end of the estimation procedure.
  Plugging in the Super Learner in the target parameter representation would
  generally not result in an asymptotically linear estimator of the target
  estimand. This begs the question, why is it important for an estimator to
  possess these properties?

  \begin{itemize}
  \item
    An asymptotically linear estimator converges to the estimand at
    \(\frac{1}{\sqrt{n}}\) rate -- and thus behaves as sample mean -- so it's
    sampling distribution can be estimated in order to conduct formal
    statistical inference (i.e., confidence intervals and \(p\)-values).
  \item
    Substitution, or plug-in, estimators of the estimand are desirable because
    they respect both the local and global constraints of the statistical model
    (e.g., bounds), and have they have better finite-sample properties.
  \item
    An efficient estimator is optimal in the sense that it has the lowest
    possible variance, and is thus the most precise. An estimator is efficient
    if and only if is asymptotically linear with influence curve equal to the
    canonical gradient.

    \begin{itemize}
    \tightlist
    \item
      The canonical gradient is a mathematical object that is specific to
      the target estimand, and it provides information on the level of
      difficulty of the estimation problem. Various canonical gradient are
      shown in the chapters that follow.
    \item
      Practitioner's do not need to know how to calculate a canonical
      gradient in order to understand efficiency and use Targeted Maximum
      Likelihood Estimation (TMLE). Metaphorically, you do not need to be
      Yoda in order to be a Jedi.
    \end{itemize}
  \end{itemize}
\item
  TMLE is a general strategy that succeeds in constructing efficient and
  asymptotically linear plug-in estimators.
\item
  SL is fantastic for pure prediction, and for obtaining an initial
  estimate in the first step of TMLE, but we need the second step of TMLE to
  have the desirable statistical properties mentioned above.
\item
  In the chapters that follow, we focus on the Targeted Maximum Likelihood
  Estimator and it's generalization to Targeted Minimum Loss-based Estimator,
  both referred to as TMLE.
\end{itemize}

\hypertarget{tmle3}{%
\chapter{The TMLE Framework}\label{tmle3}}

\emph{Jeremy Coyle} and \emph{Nima Hejazi}

Based on the \href{https://github.com/tlverse/tmle3}{\passthrough{\lstinline!tmle3!} \passthrough{\lstinline!R!} package}.

\hypertarget{learn-tmle}{%
\section{Learning Objectives}\label{learn-tmle}}

By the end of this chapter, you will be able to

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understand why we use TMLE for effect estimation.
\item
  Use \passthrough{\lstinline!tmle3!} to estimate an Average Treatment Effect (ATE).
\item
  Understand how to use \passthrough{\lstinline!tmle3!} ``Specs'' objects.
\item
  Fit \passthrough{\lstinline!tmle3!} for a custom set of target parameters.
\item
  Use the delta method to estimate transformations of target parameters.
\end{enumerate}

\hypertarget{tmle-intro}{%
\section{Introduction}\label{tmle-intro}}

In the previous chapter on \passthrough{\lstinline!sl3!} we learned how to estimate a regression
function like \(\mathbb{E}[Y \mid X]\) from data. That's an important first step
in learning from data, but how can we use this predictive model to estimate
statistical and causal effects?

Going back to \protect\hyperlink{intro}{the roadmap for targeted learning}, suppose we'd like to
estimate the effect of a treatment variable \(A\) on an outcome \(Y\). As discussed,
one potential parameter that characterizes that effect is the Average Treatment
Effect (ATE), defined as \(\psi_0 = \mathbb{E}_W[\mathbb{E}[Y \mid A=1,W] - \mathbb{E}[Y \mid A=0,W]]\) and interpreted as the difference in mean outcome
under when treatment \(A=1\) and \(A=0\), averaging over the distribution of
covariates \(W\). We'll illustrate several potential estimators for this
parameter, and motivate the use of the TMLE (targeted maximum likelihood
estimation; targeted minimum loss-based estimation) framework, using the
following example data:

\begin{center}\includegraphics[width=0.8\linewidth]{img/png/schematic_1_truedgd} \end{center}

The small ticks on the right indicate the mean outcomes (averaging over \(W\))
under \(A=1\) and \(A=0\) respectively, so their difference is the quantity we'd
like to estimate.

While we hope to motivate the application of TMLE in this chapter, we refer the
interested reader to the two Targeted Learning books and associated works for
full technical details.

\hypertarget{substitution-est}{%
\section{Substitution Estimators}\label{substitution-est}}

We can use \passthrough{\lstinline!sl3!} to fit a Super Learner or other regression model to estimate
the outcome regression function \(\mathbb{E}_0[Y \mid A,W]\), which we often refer
to as \(\overline{Q}_0(A,W)\) and whose estimate we denote \(\overline{Q}_n(A,W)\).
To construct an estimate of the ATE \(\psi_n\), we need only ``plug-in'' the
estimates of \(\overline{Q}_n(A,W)\), evaluated at the two intervention contrasts,
to the corresponding ATE ``plug-in'' formula:
\(\psi_n = \frac{1}{n}\sum(\overline{Q}_n(1,W)-\overline{Q}_n(0,W))\). This kind
of estimator is called a \emph{plug-in} or \emph{substitution} estimator, since accurate
estimates \(\psi_n\) of the parameter \(\psi_0\) may be obtained by substituting
estimates \(\overline{Q}_n(A,W)\) for the relevant regression functions
\(\overline{Q}_0(A,W)\) themselves.

Applying \passthrough{\lstinline!sl3!} to estimate the outcome regression in our example, we can see
that the ensemble machine learning predictions fit the data quite well:

\begin{center}\includegraphics[width=0.8\linewidth]{img/png/schematic_2b_sllik} \end{center}

The solid lines indicate the \passthrough{\lstinline!sl3!} estimate of the regression function, with the
dotted lines indicating the \passthrough{\lstinline!tmle3!} updates \protect\hyperlink{tmle-updates}{(described below)}.

While substitution estimators are intuitive, naively using this approach with a
Super Learner estimate of \(\bar{Q}_0(A,W)\) has several limitations. First, Super
Learner is selecting learner weights to minimize risk across the entire
regression function, instead of ``targeting'' the ATE parameter we hope to
estimate, leading to biased estimation. That is, \passthrough{\lstinline!sl3!} is trying to do well on
the full regression curve on the left, instead of focusing on the small ticks on
the right. What's more, the sampling distribution of this approach is not
asymptotically linear, and therefore inference is not possible.

We can see these limitations illustrated in the estimates generated for the
example data:

\begin{center}\includegraphics[width=0.8\linewidth]{img/png/schematic_3_effects} \end{center}

We see that Super Learner, estimates the true parameter value (indicated by the
dashed vertical line) more accurately than GLM. However, it is still less
accurate than TMLE, and valid inference is not possible. In contrast, TMLE
achieves a less biased estimator and valid inference.

\hypertarget{tmle}{%
\section{Targeted Maximum Likelihood Estimation}\label{tmle}}

TMLE takes an initial estimate \(\overline{Q}_n(A,W)\) as well as an estimate of
the propensity score \(g_n(A \mid W) = \mathbb{P}(A = 1 \mid W)\) and produces an
updated estimate \(\overline{Q}^{\star}_n(A,W)\) that is ``targeted'' to the
parameter of interest. TMLE keeps the benefits of substitution estimators (it is
one), but augments the original, potentially erratic estimates to \emph{correct for
bias} while also resulting in an \emph{asymptotically linear} (and thus normally
distributed) estimator that accommodates inference via asymptotically consistent
Wald-style confidence intervals.

\hypertarget{tmle-updates}{%
\subsection{TMLE Updates}\label{tmle-updates}}

There are different types of TMLEs (and, sometimes, multiple for the same set of
target parameters) -- below, we give an example of the algorithm for TML
estimation of the ATE. \(\overline{Q}^{\star}_n(A,W)\) is the TMLE-augmented
estimate \(f(\overline{Q}^{\star}_n(A,W)) = f(\overline{Q}_n(A,W)) + \epsilon \cdot H_n(A,W)\), where \(f(\cdot)\) is the appropriate link function (e.g.,
\(\text{logit}(x) = \log\left(\frac{x}{1 - x}\right)\)), and an estimate
\(\epsilon_n\) of the coefficient \(\epsilon\) of the ``clever covariate'' \(H_n(A,W)\)
is computed. The form of the covariate \(H_n(A,W)\) differs across target
parameters; in this case of the ATE, it is \(H_n(A,W) = \frac{A}{g_n(A \mid W)} - \frac{1-A}{1-g_n(A, W)}\), with \(g_n(A,W) = \mathbb{P}(A=1 \mid W)\) being the
estimated propensity score, so the estimator depends both on the initial fit (by
\passthrough{\lstinline!sl3!}) of the outcome regression (\(\overline{Q}_n\)) and of the propensity score
(\(g_n\)).

There are several robust augmentations that are used across the \passthrough{\lstinline!tlverse!},
including the use of an additional layer of cross-validation to avoid
over-fitting bias (i.e., CV-TMLE) as well as approaches for more consistently
estimating several parameters simultaneously (e.g., the points on a survival
curve).

\hypertarget{tmle-infer}{%
\subsection{Statistical Inference}\label{tmle-infer}}

Since TMLE yields an \textbf{asymptotically linear} estimator, obtaining statistical
inference is very convenient. Each TML estimator has a corresponding
\textbf{(efficient) influence function} (often, ``EIF'', for short) that describes the
asymptotic distribution of the estimator. By using the estimated EIF, Wald-style
inference (asymptotically correct confidence intervals) can be constructed
simply by plugging into the form of the EIF our initial estimates
\(\overline{Q}^{\star}_n\) and \(g_n\), then computing the sample standard error.

The following sections describe both a simple and more detailed way of
specifying and estimating a TMLE in the \passthrough{\lstinline!tlverse!}. In designing \passthrough{\lstinline!tmle3!}, we
sought to replicate as closely as possible the very general estimation framework
of TMLE, and so each theoretical object relevant to TMLE is encoded in a
corresponding software object/method. First, we will present the simple
application of \passthrough{\lstinline!tmle3!} to the WASH Benefits example, and then go on to describe
the underlying objects in greater detail.

\hypertarget{easy-bake-example-tmle3-for-ate}{%
\section{\texorpdfstring{Easy-Bake Example: \texttt{tmle3} for ATE}{Easy-Bake Example: tmle3 for ATE}}\label{easy-bake-example-tmle3-for-ate}}

We'll illustrate the most basic use of TMLE using the WASH Benefits data
introduced earlier and estimating an average treatment effect.

\hypertarget{load-the-data}{%
\subsection{Load the Data}\label{load-the-data}}

We'll use the same WASH Benefits data as the earlier chapters:

\begin{lstlisting}[language=R]
library(data.table)
library(dplyr)
library(tmle3)
library(sl3)
washb_data <- fread(
  paste0(
    "https://raw.githubusercontent.com/tlverse/tlverse-data/master/",
    "wash-benefits/washb_data.csv"
  ),
  stringsAsFactors = TRUE
)
\end{lstlisting}

\hypertarget{define-the-variable-roles}{%
\subsection{Define the variable roles}\label{define-the-variable-roles}}

We'll use the common \(W\) (covariates), \(A\) (treatment/intervention), \(Y\)
(outcome) data structure. \passthrough{\lstinline!tmle3!} needs to know what variables in the dataset
correspond to each of these roles. We use a list of character vectors to tell
it. We call this a ``Node List'' as it corresponds to the nodes in a Directed
Acyclic Graph (DAG), a way of displaying causal relationships between variables.

\begin{lstlisting}[language=R]
node_list <- list(
  W = c(
    "month", "aged", "sex", "momage", "momedu",
    "momheight", "hfiacat", "Nlt18", "Ncomp", "watmin",
    "elec", "floor", "walls", "roof", "asset_wardrobe",
    "asset_table", "asset_chair", "asset_khat",
    "asset_chouki", "asset_tv", "asset_refrig",
    "asset_bike", "asset_moto", "asset_sewmach",
    "asset_mobile"
  ),
  A = "tr",
  Y = "whz"
)
\end{lstlisting}

\hypertarget{handle-missingness}{%
\subsection{Handle Missingness}\label{handle-missingness}}

Currently, missingness in \passthrough{\lstinline!tmle3!} is handled in a fairly simple way:

\begin{itemize}
\tightlist
\item
  Missing covariates are median- (for continuous) or mode- (for discrete)
  imputed, and additional covariates indicating imputation are generated, just
  as described in \protect\hyperlink{sl3}{the \passthrough{\lstinline!sl3!} chapter}.
\item
  Missing treatment variables are excluded -- such observations are dropped.
\item
  Missing outcomes are efficiently handled by the automatic calculation (and
  incorporation into estimators) of \emph{inverse probability of censoring weights}
  (IPCW); this is also known as IPCW-TMLE and may be thought of as a joint
  intervention to remove missingness and is analogous to the procedure used with
  classical inverse probability weighted estimators.
\end{itemize}

These steps are implemented in the \passthrough{\lstinline!process\_missing!} function in \passthrough{\lstinline!tmle3!}:

\begin{lstlisting}[language=R]
processed <- process_missing(washb_data, node_list)
washb_data <- processed$data
node_list <- processed$node_list
\end{lstlisting}

\hypertarget{create-a-spec-object}{%
\subsection{Create a ``Spec'' Object}\label{create-a-spec-object}}

\passthrough{\lstinline!tmle3!} is general, and allows most components of the TMLE procedure to be
specified in a modular way. However, most end-users will not be interested in
manually specifying all of these components. Therefore, \passthrough{\lstinline!tmle3!} implements a
\passthrough{\lstinline!tmle3\_Spec!} object that bundles a set of components into a \emph{specification}
(``Spec'') that, with minimal additional detail, can be run by an end-user.

We'll start with using one of the specs, and then work our way down into the
internals of \passthrough{\lstinline!tmle3!}.

\begin{lstlisting}[language=R]
ate_spec <- tmle_ATE(
  treatment_level = "Nutrition + WSH",
  control_level = "Control"
)
\end{lstlisting}

\hypertarget{define-the-learners}{%
\subsection{Define the learners}\label{define-the-learners}}

Currently, the only other thing a user must define are the \passthrough{\lstinline!sl3!} learners used
to estimate the relevant factors of the likelihood: Q and g.

This takes the form of a list of \passthrough{\lstinline!sl3!} learners, one for each likelihood factor
to be estimated with \passthrough{\lstinline!sl3!}:

\begin{lstlisting}[language=R]
# choose base learners
lrnr_mean <- make_learner(Lrnr_mean)
lrnr_rf <- make_learner(Lrnr_ranger)

# define metalearners appropriate to data types
ls_metalearner <- make_learner(Lrnr_nnls)
mn_metalearner <- make_learner(
  Lrnr_solnp, metalearner_linear_multinomial,
  loss_loglik_multinomial
)
sl_Y <- Lrnr_sl$new(
  learners = list(lrnr_mean, lrnr_rf),
  metalearner = ls_metalearner
)
sl_A <- Lrnr_sl$new(
  learners = list(lrnr_mean, lrnr_rf),
  metalearner = mn_metalearner
)
learner_list <- list(A = sl_A, Y = sl_Y)
\end{lstlisting}

Here, we use a Super Learner as defined in the previous chapter. In the future,
we plan to include reasonable defaults learners.

\hypertarget{fit-the-tmle}{%
\subsection{Fit the TMLE}\label{fit-the-tmle}}

We now have everything we need to fit the tmle using \passthrough{\lstinline!tmle3!}:

\begin{lstlisting}[language=R]
tmle_fit <- tmle3(ate_spec, washb_data, node_list, learner_list)
print(tmle_fit)
A tmle3_Fit that took 1 step(s)
   type                                    param   init_est tmle_est       se
1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0031611 0.010044 0.050853
       lower   upper psi_transformed lower_transformed upper_transformed
1: -0.089626 0.10971        0.010044         -0.089626           0.10971
\end{lstlisting}

\hypertarget{evaluate-the-estimates}{%
\subsection{Evaluate the Estimates}\label{evaluate-the-estimates}}

We can see the summary results by printing the fit object. Alternatively, we
can extra results from the summary by indexing into it:

\begin{lstlisting}[language=R]
estimates <- tmle_fit$summary$psi_transformed
print(estimates)
[1] 0.010044
\end{lstlisting}

\hypertarget{tmle3-components}{%
\section{\texorpdfstring{\texttt{tmle3} Components}{tmle3 Components}}\label{tmle3-components}}

Now that we've successfully used a spec to obtain a TML estimate, let's look
under the hood at the components. The spec has a number of functions that
generate the objects necessary to define and fit a TMLE.

\hypertarget{tmle3_task}{%
\subsection{\texorpdfstring{\texttt{tmle3\_task}}{tmle3\_task}}\label{tmle3_task}}

First is, a \passthrough{\lstinline!tmle3\_Task!}, analogous to an \passthrough{\lstinline!sl3\_Task!}, containing the data we're
fitting the TMLE to, as well as an NPSEM generated from the \passthrough{\lstinline!node\_list!}
defined above, describing the variables and their relationships.

\begin{lstlisting}[language=R]
tmle_task <- ate_spec$make_tmle_task(washb_data, node_list)
\end{lstlisting}

\begin{lstlisting}[language=R]
tmle_task$npsem
$W
tmle3_Node: W
    Variables: month, aged, sex, momedu, hfiacat, Nlt18, Ncomp, watmin, elec, floor, walls, roof, asset_wardrobe, asset_table, asset_chair, asset_khat, asset_chouki, asset_tv, asset_refrig, asset_bike, asset_moto, asset_sewmach, asset_mobile, momage, momheight, delta_momage, delta_momheight
    Parents: 

$A
tmle3_Node: A
    Variables: tr
    Parents: W

$Y
tmle3_Node: Y
    Variables: whz
    Parents: A, W
\end{lstlisting}

\hypertarget{initial-likelihood}{%
\subsection{Initial Likelihood}\label{initial-likelihood}}

Next, is an object representing the likelihood, factorized according to the
NPSEM described above:

\begin{lstlisting}[language=R]
initial_likelihood <- ate_spec$make_initial_likelihood(
  tmle_task,
  learner_list
)
print(initial_likelihood)
W: Lf_emp
A: LF_fit
Y: LF_fit
\end{lstlisting}

These components of the likelihood indicate how the factors were estimated: the
marginal distribution of \(W\) was estimated using NP-MLE, and the conditional
distributions of \(A\) and \(Y\) were estimated using \passthrough{\lstinline!sl3!} fits (as defined with
the \passthrough{\lstinline!learner\_list!}) above.

We can use this in tandem with the \passthrough{\lstinline!tmle\_task!} object to obtain likelihood
estimates for each observation:

\begin{lstlisting}[language=R]
initial_likelihood$get_likelihoods(tmle_task)
               W       A        Y
   1: 0.00021299 0.34702 -0.32696
   2: 0.00021299 0.37305 -0.88218
   3: 0.00021299 0.34685 -0.79300
   4: 0.00021299 0.33625 -0.89157
   5: 0.00021299 0.34098 -0.63477
  ---                            
4691: 0.00021299 0.24334 -0.61095
4692: 0.00021299 0.24620 -0.21534
4693: 0.00021299 0.22401 -0.79223
4694: 0.00021299 0.27641 -0.94319
4695: 0.00021299 0.20158 -1.08201
\end{lstlisting}

\hypertarget{targeted-likelihood-updater}{%
\subsection{Targeted Likelihood (updater)}\label{targeted-likelihood-updater}}

We also need to define a ``Targeted Likelihood'' object. This is a special type
of likelihood that is able to be updated using an \passthrough{\lstinline!tmle3\_Update!} object. This
object defines the update strategy (e.g., submodel, loss function, CV-TMLE or
not).

\begin{lstlisting}[language=R]
targeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)
\end{lstlisting}

When constructing the targeted likelihood, you can specify different update
options. See the documentation for \passthrough{\lstinline!tmle3\_Update!} for details of the different
options. For example, you can disable CV-TMLE (the default in \passthrough{\lstinline!tmle3!}) as
follows:

\begin{lstlisting}[language=R]
targeted_likelihood_no_cv <-
  Targeted_Likelihood$new(initial_likelihood,
    updater = list(cvtmle = FALSE)
  )
\end{lstlisting}

\hypertarget{parameter-mapping}{%
\subsection{Parameter Mapping}\label{parameter-mapping}}

Finally, we need to define the parameters of interest. Here, the spec defines a
single parameter, the ATE. In the next section, we'll see how to add additional
parameters.

\begin{lstlisting}[language=R]
tmle_params <- ate_spec$make_params(tmle_task, targeted_likelihood)
print(tmle_params)
[[1]]
Param_ATE: ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}]
\end{lstlisting}

\hypertarget{putting-it-all-together}{%
\subsection{Putting it all together}\label{putting-it-all-together}}

Having used the spec to manually generate all these components, we can now
manually fit a \passthrough{\lstinline!tmle3!}:

\begin{lstlisting}[language=R]
tmle_fit_manual <- fit_tmle3(
  tmle_task, targeted_likelihood, tmle_params,
  targeted_likelihood$updater
)
print(tmle_fit_manual)
A tmle3_Fit that took 1 step(s)
   type                                    param   init_est tmle_est       se
1:  ATE ATE[Y_{A=Nutrition + WSH}-Y_{A=Control}] -0.0062324 0.017515 0.050591
       lower   upper psi_transformed lower_transformed upper_transformed
1: -0.081641 0.11667        0.017515         -0.081641           0.11667
\end{lstlisting}

The result is equivalent to fitting using the \passthrough{\lstinline!tmle3!} function as above.

\hypertarget{fitting-tmle3-with-multiple-parameters}{%
\section{\texorpdfstring{Fitting \texttt{tmle3} with multiple parameters}{Fitting tmle3 with multiple parameters}}\label{fitting-tmle3-with-multiple-parameters}}

Above, we fit a \passthrough{\lstinline!tmle3!} with just one parameter. \passthrough{\lstinline!tmle3!} also supports fitting
multiple parameters simultaneously. To illustrate this, we'll use the
\passthrough{\lstinline!tmle\_TSM\_all!} spec:

\begin{lstlisting}[language=R]
tsm_spec <- tmle_TSM_all()
targeted_likelihood <- Targeted_Likelihood$new(initial_likelihood)
all_tsm_params <- tsm_spec$make_params(tmle_task, targeted_likelihood)
print(all_tsm_params)
[[1]]
Param_TSM: E[Y_{A=Control}]

[[2]]
Param_TSM: E[Y_{A=Handwashing}]

[[3]]
Param_TSM: E[Y_{A=Nutrition}]

[[4]]
Param_TSM: E[Y_{A=Nutrition + WSH}]

[[5]]
Param_TSM: E[Y_{A=Sanitation}]

[[6]]
Param_TSM: E[Y_{A=WSH}]

[[7]]
Param_TSM: E[Y_{A=Water}]
\end{lstlisting}

This spec generates a Treatment Specific Mean (TSM) for each level of the
exposure variable. Note that we must first generate a new targeted likelihood,
as the old one was targeted to the ATE. However, we can recycle the initial
likelihood we fit above, saving us a super learner step.

\hypertarget{delta-method}{%
\subsection{Delta Method}\label{delta-method}}

We can also define parameters based on Delta Method Transformations of other
parameters. For instance, we can estimate a ATE using the delta method and two
of the above TSM parameters:

\begin{lstlisting}[language=R]
ate_param <- define_param(
  Param_delta, targeted_likelihood,
  delta_param_ATE,
  list(all_tsm_params[[1]], all_tsm_params[[4]])
)
print(ate_param)
Param_delta: E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}]
\end{lstlisting}

This can similarly be used to estimate other derived parameters like Relative
Risks, and Population Attributable Risks

\hypertarget{fit}{%
\subsection{Fit}\label{fit}}

We can now fit a TMLE simultaneously for all TSM parameters, as well as the
above defined ATE parameter

\begin{lstlisting}[language=R]
all_params <- c(all_tsm_params, ate_param)

tmle_fit_multiparam <- fit_tmle3(
  tmle_task, targeted_likelihood, all_params,
  targeted_likelihood$updater
)

print(tmle_fit_multiparam)
A tmle3_Fit that took 1 step(s)
   type                                       param   init_est tmle_est
1:  TSM                            E[Y_{A=Control}] -0.5953314 -0.61981
2:  TSM                        E[Y_{A=Handwashing}] -0.6179897 -0.66114
3:  TSM                          E[Y_{A=Nutrition}] -0.6119870 -0.60338
4:  TSM                    E[Y_{A=Nutrition + WSH}] -0.6015639 -0.60250
5:  TSM                         E[Y_{A=Sanitation}] -0.5866311 -0.58147
6:  TSM                                E[Y_{A=WSH}] -0.5213051 -0.45027
7:  TSM                              E[Y_{A=Water}] -0.5653576 -0.53554
8:  ATE E[Y_{A=Nutrition + WSH}] - E[Y_{A=Control}] -0.0062324  0.01731
         se     lower    upper psi_transformed lower_transformed
1: 0.030069 -0.678746 -0.56088        -0.61981         -0.678746
2: 0.041821 -0.743111 -0.57917        -0.66114         -0.743111
3: 0.041553 -0.684825 -0.52194        -0.60338         -0.684825
4: 0.040925 -0.682712 -0.52229        -0.60250         -0.682712
5: 0.042313 -0.664402 -0.49854        -0.58147         -0.664402
6: 0.045216 -0.538891 -0.36165        -0.45027         -0.538891
7: 0.039290 -0.612551 -0.45854        -0.53554         -0.612551
8: 0.050596 -0.081857  0.11648         0.01731         -0.081857
   upper_transformed
1:          -0.56088
2:          -0.57917
3:          -0.52194
4:          -0.52229
5:          -0.49854
6:          -0.36165
7:          -0.45854
8:           0.11648
\end{lstlisting}

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\hypertarget{tmle3-ex1}{%
\subsection{\texorpdfstring{Estimation of the ATE with \texttt{tmle3}}{Estimation of the ATE with tmle3}}\label{tmle3-ex1}}

Follow the steps below to estimate an average treatment effect using data from
the Collaborative Perinatal Project (CPP), available in the \passthrough{\lstinline!sl3!} package. To
simplify this example, we define a binary intervention variable, \passthrough{\lstinline!parity01!} --
an indicator of having one or more children before the current child and a
binary outcome, \passthrough{\lstinline!haz01!} -- an indicator of having an above average height for
age.

\begin{lstlisting}[language=R]
# load the data set
data(cpp)
cpp <- cpp %>%
  as_tibble() %>%
  dplyr::filter(!is.na(haz)) %>%
  mutate(
    parity01 = as.numeric(parity > 0),
    haz01 = as.numeric(haz > 0)
  )
\end{lstlisting}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Define the variable roles \((W,A,Y)\) by creating a list of these nodes.
  Include the following baseline covariates in \(W\): \passthrough{\lstinline!apgar1!}, \passthrough{\lstinline!apgar5!},
  \passthrough{\lstinline!gagebrth!}, \passthrough{\lstinline!mage!}, \passthrough{\lstinline!meducyrs!}, \passthrough{\lstinline!sexn!}. Both \(A\) and \(Y\) are specified
  above.
\item
  Define a \passthrough{\lstinline!tmle3\_Spec!} object for the ATE, \passthrough{\lstinline!tmle\_ATE()!}.
\item
  Using the same base learning libraries defined above, specify \passthrough{\lstinline!sl3!} base
  learners for estimation of \(\overline{Q}_0 = \mathbb{E}_0(Y \mid A,Y)\) and
  \(g_0 = \mathbb{P}(A = 1 \mid W)\).
\item
  Define the metalearner like below.
\end{enumerate}

\begin{lstlisting}[language=R]
metalearner <- make_learner(
  Lrnr_solnp,
  loss_function = loss_loglik_binomial,
  learner_function = metalearner_logistic_binomial
)
\end{lstlisting}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Define one super learner for estimating \(\overline{Q}_0\) and another for
  estimating \(g_0\). Use the metalearner above for both super learners.
\item
  Create a list of the two super learners defined in the step above and call
  this object \passthrough{\lstinline!learner\_list!}. The list names should be \passthrough{\lstinline!A!} (defining the super
  learner for estimation of \(g_0\)) and \passthrough{\lstinline!Y!} (defining the super learner for
  estimation of \(\overline{Q}_0\)).
\item
  Fit the TMLE with the \passthrough{\lstinline!tmle3!} function by specifying (1) the \passthrough{\lstinline!tmle3\_Spec!},
  which we defined in Step 2; (2) the data; (3) the list of nodes, which we
  specified in Step 1; and (4) the list of super learners for estimation of
  \(g_0\) and \(\overline{Q}_0\), which we defined in Step 6. \emph{Note}: Like before,
  you will need to explicitly make a copy of the data (to work around
  \passthrough{\lstinline!data.table!} optimizations), e.g., (\passthrough{\lstinline!cpp2 <- data.table::copy(cpp)!}), then
  use the \passthrough{\lstinline!cpp2!} data going forward.
\end{enumerate}

\hypertarget{tmle3-ex2}{%
\subsection{\texorpdfstring{Estimation of Strata-Specific ATEs with \texttt{tmle3}}{Estimation of Strata-Specific ATEs with tmle3}}\label{tmle3-ex2}}

For this exercise, we will work with a random sample of 5,000 patients who
participated in the International Stroke Trial (IST). This data is described in
the \protect\hyperlink{ist}{Chapter 3.2 of the \passthrough{\lstinline!tlverse!} handbook}. We included the data below
and a summarized description that is relevant for this exercise.

The outcome, \(Y\), indicates recurrent ischemic stroke within 14 days after
randomization (\passthrough{\lstinline!DRSISC!}); the treatment of interest, \(A\), is the randomized
aspirin vs.~no aspirin treatment allocation (\passthrough{\lstinline!RXASP!} in \passthrough{\lstinline!ist!}); and the
adjustment set, \(W\), consists simply of other variables measured at baseline. In
this data, the outcome is occasionally missing, but there is no need to create a
variable indicating this missingness (such as \(\Delta\)) for analyses in the
\passthrough{\lstinline!tlverse!}, since the missingness is automatically detected when \passthrough{\lstinline!NA!} are present
in the outcome. Covariates with missing values (\passthrough{\lstinline!RATRIAL!}, \passthrough{\lstinline!RASP3!} and \passthrough{\lstinline!RHEP24!})
have already been imputed. Additional covariates were created
(\passthrough{\lstinline!MISSING\_RATRIAL\_RASP3!} and \passthrough{\lstinline!MISSING\_RHEP24!}), which indicate whether or not
the covariate was imputed. The missingness was identical for \passthrough{\lstinline!RATRIAL!} and
\passthrough{\lstinline!RASP3!}, which is why only one covariate indicating imputation for these two
covariates was created.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Estimate the average effect of randomized asprin treatment (\passthrough{\lstinline!RXASP!} = 1) on
  recurrent ischemic stroke. Even though the missingness mechanism on \(Y\),
  \(\Delta\), does not need to be specified in the node list, it does still need
  to be accounted for in the TMLE. In other words, for this estimation problem,
  \(\Delta\) is a relevant factor of the likelihood. Thus, when defining the
  list of \passthrough{\lstinline!sl3!} learners for each likelihood factor, be sure to include a list
  of learners for estimation of \(\Delta\), say \passthrough{\lstinline!sl\_Delta!}, and specify this in
  the learner list, like so
  \passthrough{\lstinline!learner\_list <- list(A = sl\_A, delta\_Y = sl\_Delta, Y = sl\_Y)!}.
\item
  Recall that this RCT was conducted internationally. Suposse there is concern
  that the dose of asprin may have varied across geographical regions, and an
  average across all geographical regions may not be warranted. Calculate the
  strata specific ATEs according to geographical region (\passthrough{\lstinline!REGION!}).
\end{enumerate}

\begin{lstlisting}[language=R]
ist_data <- fread(
  paste0(
    "https://raw.githubusercontent.com/tlverse/deming2019-workshop/",
    "master/data/ist_sample.csv"
  )
)
\end{lstlisting}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

\passthrough{\lstinline!tmle3!} is a general purpose framework for generating TML estimates. The easiest
way to use it is to use a predefined spec, allowing you to just fill in the
blanks for the data, variable roles, and \passthrough{\lstinline!sl3!} learners. However, digging under
the hood allows users to specify a wide range of TMLEs. In the next sections,
we'll see how this framework can be used to estimate advanced parameters such as
optimal treatments and stochastic shift interventions.

\hypertarget{optimal-individualized-treatment-regimes-optional}{%
\chapter{Optimal Individualized Treatment Regimes (optional)}\label{optimal-individualized-treatment-regimes-optional}}

\emph{Ivana Malenica}

Based on the \href{https://github.com/tlverse/tmle3mopttx}{\passthrough{\lstinline!tmle3mopttx!} \passthrough{\lstinline!R!} package}
by \emph{Ivana Malenica, Jeremy Coyle, and Mark van der Laan}.

Updated: 2021-10-13

\hypertarget{learning-objectives-3}{%
\section{Learning Objectives}\label{learning-objectives-3}}

By the end of this lesson you will be able to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differentiate dynamic and optimal dynamic treatment interventions from static
  interventions.
\item
  Explain the benefits and challenges associated with using optimal
  individualized treatment regimes in practice.
\item
  Contrast the impact of implementing an optimal individualized treatment
  regime in the population with the impact of implementing static and dynamic
  treatment regimes in the population.
\item
  Estimate causal effects under optimal individualized treatment regimes with
  the \passthrough{\lstinline!tmle3mopttx!} \passthrough{\lstinline!R!} package.
\item
  Implement optimal individualized treatment rules based on sub-optimal
  rules, or ``simple'' rules, and recognize the practical benefit of these rules.
\item
  Construct ``realistic'' optimal individualized treatment regimes that respect
  real data and subject-matter knowledge limitations on interventions by
  only considering interventions that are supported by the data.
\item
  Measure variable importance as defined in terms of the optimal individualized
  treatment interventions.
\end{enumerate}

\hypertarget{introduction-to-optimal-individualized-interventions}{%
\section{Introduction to Optimal Individualized Interventions}\label{introduction-to-optimal-individualized-interventions}}

Identifying which intervention will be effective for which patient
based on lifestyle, genetic and environmental factors is a common goal in
precision medicine. One opts to administer the intervention to individuals
who will benefit from it, instead of assigning treatment on a population level.

\begin{itemize}
\item
  This aim motivates a different type of an intervention, as opposed to the static
  exposures we might be used to.
\item
  In this chapter, we learn about dynamic
  (individualized) interventions that tailor the treatment decision based on the
  collected covariates.
\item
  In the statistics community, such a treatment strategy is termed
  \textbf{individualized treatment regimes} (ITR), and the (counterfactual)
  population mean outcome under an ITR is the \textbf{value of the ITR}.
\item
  Even more, suppose one wishes to maximize the population mean of an outcome,
  where for each individual we have access to some set of measured covariates.
  An ITR with the maximal value is referred to as an \textbf{optimal ITR} or the
  \textbf{optimal individualized treatment}. Consequently, the value of an optimal
  ITR is termed the \textbf{optimal value}, or the \textbf{mean under the optimal
  individualized treatment}.
\item
  One opts to administer the intervention to individuals who will profit from
  it, instead of assigning treatment on a population level. But how do we know
  which intervention works for which patient?
\item
  For example, one might seek to improve retention in HIV care. In a randomized
  clinical trial, several interventions show efficacy- including appointment
  reminders through text messages, small cash incentives for on time clinic
  visits, and peer health workers.
\item
  Ideally, we want to improve effectiveness by assigning each patient the
  intervention they are most likely to benefit from, as well as improve
  efficiency by not allocating resources to individuals that do not need them,
  or would not benefit from an intervention.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{img/png/DynamicA_Illustration} 

}

\caption{Illustration of a Dynamic Treatment Regime in a Clinical Setting}\label{fig:unnamed-chunk-1}
\end{figure}

This aim motivates a different type of intervention, as opposed to the static exposures we
might be used to.

\begin{itemize}
\item
  In this chapter, we examine multiple examples of optimal individualized
  treatment regimes
  and estimate the mean outcome under the ITR
  where the candidate rules are restricted to depend only on user-supplied subset of the
  baseline covariates.
\item
  In order to accomplish this, we present the \href{https://github.com/tlverse/tmle3mopttx}{\passthrough{\lstinline!tmle3mopttx!} R
  package}, which features an
  implementation of a recently developed algorithm for computing targeted minimum
  loss-based estimates of a causal effect based on optimal ITR for
  categorical treatment.
\item
  In particular, we will use \passthrough{\lstinline!tmle3mopttx!} to estimate
  optimal ITR and the corresponding population value,
  construct realistic optimal ITRs, and perform variable importance in terms of the
  mean under the optimal individualized treatment.
\end{itemize}

\hypertarget{data-structure-and-notation}{%
\section{Data Structure and Notation}\label{data-structure-and-notation}}

\begin{itemize}
\item
  Suppose we observe \(n\) independent and identically distributed observations of
  the form \(O=(W,A,Y) \sim P_0\). \(P_0 \in \mathcal{M}\), where \(\mathcal{M}\) is the
  fully nonparametric model.
\item
  Denote \(A \in \mathcal{A}\) as categorical treatment, where
  \(\mathcal{A} \equiv \{a_1, \ldots, a_{n_A} \}\) and \(n_A = |\mathcal{A}|\), with
  \(n_A\) denoting the number of categories.
\item
  Denote \(Y\) as the final outcome, and \(W\) a vector-valued collection of baseline
  covariates.
\item
  The likelihood of the data admits a factorization, implied by the time ordering of \(O\).
  \begin{equation*}\label{eqn:likelihood_factorization}
  p_0(O) = p_{Y,0}(Y|A,W) p_{A,0}(A|W) p_{W,0}(W) = q_{Y,0}(Y|A,W) q_{A,0}(A|W) q_{W,0}(W),
  \end{equation*}
\item
  Consequently, we define
  \(P_{Y,0}(Y|A,W)=Q_{Y,0}(Y|A,W)\), \(P_{A,0}(A|W)=g_0(A|W)\) and \(P_{W,0}(W)=Q_{W,0}(W)\) as the
  corresponding conditional distributions of \(Y\), \(A\) and \(W\).
\item
  We also define \(\bar{Q}_{Y,0}(A,W) \equiv E_0[Y|A,W]\).
\item
  Finally, denote \(V\) as a subset of the baseline covariates \(W\) that
  the optimal individualized rule depends on.
\end{itemize}

\hypertarget{defining-the-causal-effect-of-an-optimal-individualized-intervention}{%
\section{Defining the Causal Effect of an Optimal Individualized Intervention}\label{defining-the-causal-effect-of-an-optimal-individualized-intervention}}

\begin{itemize}
\item
  Consider dynamic treatment rules \(V \rightarrow d(V) \in \{a_1, \ldots, a_{n_A} \} \times \{1\}\),
  for assigning treatment \(A\) based on \(V\).
\item
  Dynamic treatment regime may be viewed as an intervention in which
  \(A\) is set equal to a value based on a hypothetical regime \(d(V)\), and \(Y_{d(V)}\)
  is the corresponding counterfactual outcome under \(d(V)\).
\item
  The goal of any causal analysis motivated by an optimal individualized
  intervention is to estimate a parameter defined as the counterfactual mean of the outcome with
  respect to the modified intervention distribution.
\item
  Recall causal assumptions:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Consistency}: \(Y^{d(v_i)}_i = Y_i\) in the event \(A_i = d(v_i)\),
  for \(i = 1, \ldots, n\).
\item
  \textbf{Stable unit value treatment assumption (SUTVA)}: \(Y^{d(v_i)}_i\) does
  not depend on \(d(v_j)\) for \(i = 1, \ldots, n\) and \(j \neq i\), or lack
  of interference.
\item
  \textbf{Strong ignorability}: \(A \perp \!\!\! \perp Y^{d(v)} \mid W\), for all \(a \in \mathcal{A}\).
\item
  \textbf{Positivity (or overlap)}: \(P_0(\min_{a \in \mathcal{A}} g_0(a|W) > 0)=1\)
\end{enumerate}

\begin{itemize}
\item
  Here, we also assume non-exceptional law is in effect.
\item
  We are primarily interested in the value of an individualized rule,
  \[E_0[Y_{d(V)}] = E_{0,W}[\bar{Q}_{Y,0}(A=d(V),W)].\]
\item
  The optimal rule is the rule with the maximal value:
  \[d_{opt}(V) \equiv \text{argmax}_{d(V) \in \mathcal{D}} E_0[Y_{d(V)}] \]
  where \(\mathcal{D}\) represents the set of possible rules, \(d\), implied by \(V\).
\item
  The target causal estimand of our analysis is:
  \[\psi_0 := E_0[Y_{d_{opt}(V)}] =  E_{0,W}[\bar{Q}_{Y,0}(A=d_{opt}(V),W)].\]
\item
  General, high-level idea:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Learn the optimal ITR using the Super Learner.
\item
  Estimate its value with the cross-validated Targeted Minimum Loss-based
  Estimator (CV-TMLE).
\end{enumerate}

\hypertarget{why-cv-tmle}{%
\subsection{Why CV-TMLE?}\label{why-cv-tmle}}

\begin{itemize}
\item
  CV-TMLE is necessary as the non-cross-validated TMLE is biased upward for the
  mean outcome under the rule, and therefore overly optimistic.
\item
  More generally however, using CV-TMLE allows us more freedom in estimation and
  therefore greater data adaptivity, without sacrificing inference!
\end{itemize}

\hypertarget{binary-treatment}{%
\section{Binary Treatment}\label{binary-treatment}}

\begin{itemize}
\item
  How do we estimate the optimal individualized treatment regime? In the case of
  a binary treatment, a key quantity for optimal ITR is the \textbf{blip} function.
\item
  Optimal ITR ideally assigns treatment to individuals falling in strata in
  which the stratum specific average treatment effect, the \textbf{blip} function, is
  positive and does not assign treatment to individuals for which this quantity
  is negative.
\item
  We define the blip function as: \[\bar{Q}_0(V) \equiv E_0[Y_1-Y_0|V] \equiv
  E_0[\bar{Q}_{Y,0}(1,W) - \bar{Q}_{Y,0}(0,W) | V], \] or the average treatment
  effect within a stratum of \(V\).
\item
  Optimal individualized rule can now be derived as \(d_{opt}(V) = I(\bar{Q}_{0}(V) > 0)\).
\item
  Relying on the Targeted Maximum Likelihood (TML) estimator and the Super
  Learner estimate of the blip function, we follow the below steps in order to
  obtain value of the ITR:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate \(\bar{Q}_{Y,0}(A,W)\) and \(g_0(A|W)\) using \passthrough{\lstinline!sl3!}. We denote such
  estimates as \(\bar{Q}_{Y,n}(A,W)\) and \(g_n(A|W)\).
\item
  Apply the doubly robust Augmented-Inverse Probability Weighted (A-IPW)
  transform to our outcome, where we define:
\end{enumerate}

\[D_{\bar{Q}_Y,g,a}(O) \equiv \frac{I(A=a)}{g(A|W)} (Y-\bar{Q}_Y(A,W)) + \bar{Q}_Y(A=a,W)\]

Note that under the randomization and positivity assumptions we have that
\(E[D_{\bar{Q}_Y,g,a}(O) | V] = E[Y_a |V].\) We emphasize the double robust nature
of the A-IPW transform: consistency of \(E[Y_a |V]\) will depend on correct estimation
of either \(\bar{Q}_{Y,0}(A,W)\) or \(g_0(A|W)\). As such, in a randomized trial, we are
guaranteed a consistent estimate of \(E[Y_a |V]\) even if we get \(\bar{Q}_{Y,0}(A,W)\) wrong!

Using this transform, we can define the following contrast:

\[D_{\bar{Q}_Y,g}(O) = D_{\bar{Q}_Y,g,a=1}(O) - D_{\bar{Q}_Y,g,a=0}(O)\]

We estimate the blip function, \(\bar{Q}_{0,a}(V)\), by regressing \(D_{\bar{Q}_Y,g}(O)\) on \(V\) using
the specified \passthrough{\lstinline!sl3!} library of learners and an appropriate loss function.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Our estimated rule is \(d(V) = \text{argmax}_{a \in \mathcal{A}} \bar{Q}_{0,a}(V)\).
\item
  We obtain inference for the mean outcome under the estimated optimal rule using CV-TMLE.
\end{enumerate}

\hypertarget{evaluating-the-causal-effect-of-an-optimal-itr-with-binary-treatment}{%
\subsection{Evaluating the Causal Effect of an optimal ITR with Binary Treatment}\label{evaluating-the-causal-effect-of-an-optimal-itr-with-binary-treatment}}

To start, let us load the packages we will use and set a seed for simulation:

\begin{lstlisting}[language=R]
library(here)
library(data.table)
library(sl3)
library(tmle3)
library(tmle3mopttx)
library(devtools)
set.seed(116)
\end{lstlisting}

\hypertarget{simulate-data}{%
\subsubsection{Simulate Data}\label{simulate-data}}

Our data generating distribution is of the following form:

\[W \sim \mathcal{N}(\bf{0},I_{3 \times 3})\]
\[P(A=1|W) = \frac{1}{1+\exp^{(-0.8*W_1)}}\]
\[P(Y=1|A,W) = 0.5\text{logit}^{-1}[-5I(A=1)(W_1-0.5)+5I(A=0)(W_1-0.5)] +
0.5\text{logit}^{-1}(W_2W_3)\]

\begin{lstlisting}[language=R]
data("data_bin")
\end{lstlisting}

\begin{itemize}
\item
  The above composes our observed data structure \(O = (W, A, Y)\).
\item
  Note that the mean under the true optimal rule is \(\psi_0=0.578\) for this data
  generating distribution.
\item
  Next, we specify the role that each variable in the data set plays as the
  nodes in a DAG.
\end{itemize}

\begin{lstlisting}[language=R]
# organize data and nodes for tmle3
data <- data_bin
node_list <- list(
  W = c("W1", "W2", "W3"),
  A = "A",
  Y = "Y"
)
\end{lstlisting}

\begin{itemize}
\tightlist
\item
  We now have an observed data structure (\passthrough{\lstinline!data!}), and a specification of the
  role that each variable in the data set plays as the nodes in a DAG.
\end{itemize}

\hypertarget{constructing-optimal-stacked-regressions-with-sl3}{%
\subsubsection{\texorpdfstring{Constructing Optimal Stacked Regressions with \texttt{sl3}}{Constructing Optimal Stacked Regressions with sl3}}\label{constructing-optimal-stacked-regressions-with-sl3}}

\begin{itemize}
\tightlist
\item
  We generate three different ensemble learners that must be fit, corresponding
  to the learners for the outcome regression, propensity score, and the blip
  function.
\end{itemize}

\begin{lstlisting}[language=R]
# Define sl3 library and metalearners:
lrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)
lrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)
lrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)
lrn_mean <- Lrnr_mean$new()
lrn_glm <- Lrnr_glm_fast$new()

## Define the Q learner:
Q_learner <- Lrnr_sl$new(
  learners = list(
    lrn_xgboost_50, lrn_xgboost_100,
    lrn_xgboost_500, lrn_mean, lrn_glm
  ),
  metalearner = Lrnr_nnls$new()
)

## Define the g learner:
g_learner <- Lrnr_sl$new(
  learners = list(lrn_xgboost_100, lrn_glm),
  metalearner = Lrnr_nnls$new()
)

## Define the B learner:
b_learner <- Lrnr_sl$new(
  learners = list(
    lrn_xgboost_50, lrn_xgboost_100,
    lrn_xgboost_500, lrn_mean, lrn_glm
  ),
  metalearner = Lrnr_nnls$new()
)
\end{lstlisting}

We make the above explicit with respect to standard
notation by bundling the ensemble learners into a list object below:

\begin{lstlisting}[language=R]
# specify outcome and treatment regressions and create learner list
learner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)
\end{lstlisting}

\hypertarget{targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects}{%
\subsubsection{Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects}\label{targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects}}

\begin{itemize}
\item
  To start, we will initialize a specification for the TMLE of our parameter of
  interest simply by calling \passthrough{\lstinline!tmle3\_mopttx\_blip\_revere!}.
\item
  We specify the argument \passthrough{\lstinline!V = c("W1", "W2", "W3")!} when initializing the
  \passthrough{\lstinline!tmle3\_Spec!} object in order to communicate that we're interested in learning
  a rule dependent on \passthrough{\lstinline!V!} covariates.
\item
  We also need to specify the type of blip we will use in this estimation
  problem, and the list of learners used to estimate relevant parts of the
  likelihood and the blip function.
\item
  In addition, we need to specify whether we want to maximize or minimize the
  mean outcome under the rule (\passthrough{\lstinline!maximize=TRUE!}).
\item
  If \passthrough{\lstinline!complex=FALSE!}, \passthrough{\lstinline!tmle3mopttx!} will consider all the possible rules under a
  smaller set of covariates including the static rules, and optimize the mean
  outcome over all the suboptimal rules dependent on \(V\).
\item
  If \passthrough{\lstinline!realistic=TRUE!}, only treatments supported by the data will be considered,
  therefore alleviating concerns regarding practical positivity issues.
\end{itemize}

\begin{lstlisting}[language=R]
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W1", "W2", "W3"), type = "blip1",
  learners = learner_list,
  maximize = TRUE, complex = TRUE,
  realistic = FALSE
)
\end{lstlisting}

\begin{lstlisting}[language=R]
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)
fit
A tmle3_Fit that took 1 step(s)
   type         param init_est tmle_est       se   lower   upper
1:  TSM E[Y_{A=NULL}]  0.43164  0.55855 0.027047 0.50554 0.61156
   psi_transformed lower_transformed upper_transformed
1:         0.55855           0.50554           0.61156
\end{lstlisting}

We can see that the confidence interval covers our true mean under the true optimal
individualized treatment!

\hypertarget{categorical-treatment}{%
\section{Categorical Treatment}\label{categorical-treatment}}

\textbf{QUESTION:} Can we still use the blip function if the treatment is
categorical?

\begin{itemize}
\item
  In this section, we consider how to evaluate the mean outcome under the
  optimal individualized treatment when \(A\) has more than two categories!
\item
  We define \textbf{pseudo-blips} as vector valued entities where the output for a
  given \(V\) is a vector of length equal to the number of treatment categories,
  \(n_A\). As such, we define it as: \[\bar{Q}_0^{pblip}(V) =
  \{\bar{Q}_{0,a}^{pblip}(V): a \in \mathcal{A} \}\]
\item
  We implement three different pseudo-blips in \passthrough{\lstinline!tmle3mopttx!}.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Blip1} corresponds to choosing a reference category of treatment, and
  defining the blip for all other categories relative to the specified
  reference: \[\bar{Q}_{0,a}^{pblip-ref}(V) \equiv E_0(Y_a-Y_0|V)\]
\item
  \textbf{Blip2} approach corresponds to defining the blip relative to the average
  of all categories:
  \[\bar{Q}_{0,a}^{pblip-avg}(V) \equiv E_0(Y_a- \frac{1}{n_A} \sum_{a \in \mathcal{A}} Y_a|V)\]
\item
  \textbf{Blip3} reflects an extension of Blip2, where the average is now a weighted
  average:
  \[\bar{Q}_{0,a}^{pblip-wavg}(V) \equiv E_0(Y_a- \frac{1}{n_A}
  \sum_{a \in \mathcal{A}} Y_{a} P(A=a|V)|V)\]
\end{enumerate}

\hypertarget{evaluating-the-causal-effect-of-an-optimal-itr-with-categorical-treatment}{%
\subsection{Evaluating the Causal Effect of an optimal ITR with Categorical Treatment}\label{evaluating-the-causal-effect-of-an-optimal-itr-with-categorical-treatment}}

While the procedure is analogous to the previously described binary treatment,
we now need to pay attention to the type of blip we define in the estimation
stage, as well as how we construct our learners.

\hypertarget{simulated-data}{%
\subsubsection{Simulated Data}\label{simulated-data}}

\begin{itemize}
\tightlist
\item
  First, we load the simulated data. Here, our data generating distribution was
  of the following form:
\end{itemize}

\[W \sim \mathcal{N}(\bf{0},I_{4 \times 4})\]
\[P(A|W) = \frac{1}{1+\exp^{(-(0.05*I(A=1)*W_1+0.8*I(A=2)*W_1+0.8*I(A=3)*W_1))}}\]

\[P(Y|A,W) = 0.5\text{logit}^{-1}[15I(A=1)(W_1-0.5) - 3I(A=2)(2W_1+0.5) \\
+ 3I(A=3)(3W_1-0.5)] +\text{logit}^{-1}(W_2W_1)\]

\begin{itemize}
\tightlist
\item
  We can just load the data available as part of the package as follows:
\end{itemize}

\begin{lstlisting}[language=R]
data("data_cat_realistic")
\end{lstlisting}

\begin{itemize}
\tightlist
\item
  The above composes our observed data structure \(O = (W, A, Y)\). Note that the
  mean under the true optimal rule is \(\psi_0=0.658\), which is the quantity we
  aim to estimate.
\end{itemize}

\begin{lstlisting}[language=R]
# organize data and nodes for tmle3
data <- data_cat_realistic
node_list <- list(
  W = c("W1", "W2", "W3", "W4"),
  A = "A",
  Y = "Y"
)
\end{lstlisting}

\hypertarget{constructing-optimal-stacked-regressions-with-sl3-1}{%
\subsubsection{\texorpdfstring{Constructing Optimal Stacked Regressions with \texttt{sl3}}{Constructing Optimal Stacked Regressions with sl3}}\label{constructing-optimal-stacked-regressions-with-sl3-1}}

\textbf{QUESTION:} With categorical treatment, what is the dimension of the blip now?
How would we go about estimating it?

\begin{lstlisting}[language=R]
# Initialize few of the learners:
lrn_xgboost_50 <- Lrnr_xgboost$new(nrounds = 50)
lrn_xgboost_100 <- Lrnr_xgboost$new(nrounds = 100)
lrn_xgboost_500 <- Lrnr_xgboost$new(nrounds = 500)
lrn_mean <- Lrnr_mean$new()
lrn_glm <- Lrnr_glm_fast$new()

## Define the Q learner, which is just a regular learner:
Q_learner <- Lrnr_sl$new(
  learners = list(lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean, lrn_glm),
  metalearner = Lrnr_nnls$new()
)

# Define the g learner, which is a multinomial learner:
# specify the appropriate loss of the multinomial learner:
mn_metalearner <- make_learner(Lrnr_solnp,
  loss_function = loss_loglik_multinomial,
  learner_function = metalearner_linear_multinomial
)
g_learner <- make_learner(Lrnr_sl, list(lrn_xgboost_100, lrn_xgboost_500, lrn_mean), mn_metalearner)

# Define the Blip learner, which is a multivariate learner:
learners <- list(lrn_xgboost_50, lrn_xgboost_100, lrn_xgboost_500, lrn_mean, lrn_glm)
b_learner <- create_mv_learners(learners = learners)
\end{lstlisting}

\begin{itemize}
\item
  We generate three different ensemble learners that must be fit,
  corresponding to the learners for the outcome regression, propensity score, and the
  blip function.
\item
  Note that we need to estimate \(g_0(A|W)\) for a categorical \(A\)- therefore
  we use the multinomial Super Learner option available within the \passthrough{\lstinline!sl3!} package with learners
  that can address multi-class classification problems.
\item
  In order to see which learners can
  be used to estimate \(g_0(A|W)\) in \passthrough{\lstinline!sl3!}, we run the following:
\end{itemize}

\begin{lstlisting}[language=R]
# See which learners support multi-class classification:
sl3_list_learners(c("categorical"))
 [1] "Lrnr_bound"                "Lrnr_caret"               
 [3] "Lrnr_cv_selector"          "Lrnr_glmnet"              
 [5] "Lrnr_grf"                  "Lrnr_gru_keras"           
 [7] "Lrnr_h2o_glm"              "Lrnr_h2o_grid"            
 [9] "Lrnr_independent_binomial" "Lrnr_lightgbm"            
[11] "Lrnr_lstm_keras"           "Lrnr_mean"                
[13] "Lrnr_multivariate"         "Lrnr_nnet"                
[15] "Lrnr_optim"                "Lrnr_polspline"           
[17] "Lrnr_pooled_hazards"       "Lrnr_randomForest"        
[19] "Lrnr_ranger"               "Lrnr_rpart"               
[21] "Lrnr_screener_correlation" "Lrnr_solnp"               
[23] "Lrnr_svm"                  "Lrnr_xgboost"             
\end{lstlisting}

\begin{lstlisting}[language=R]
# specify outcome and treatment regressions and create learner list
learner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)
\end{lstlisting}

\hypertarget{targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1}{%
\subsubsection{Targeted Estimation of the Mean under the Optimal Individualized Interventions Effects}\label{targeted-estimation-of-the-mean-under-the-optimal-individualized-interventions-effects-1}}

\begin{lstlisting}[language=R]
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W1", "W2", "W3", "W4"), type = "blip2",
  learners = learner_list, maximize = TRUE, complex = TRUE,
  realistic = FALSE
)
\end{lstlisting}

\begin{lstlisting}[language=R]
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)
fit
A tmle3_Fit that took 1 step(s)
   type         param init_est tmle_est      se   lower   upper psi_transformed
1:  TSM E[Y_{A=NULL}]  0.54926  0.62506 0.06366 0.50029 0.74983         0.62506
   lower_transformed upper_transformed
1:           0.50029           0.74983

# How many individuals got assigned each treatment?
table(tmle_spec$return_rule)

  1   2   3 
447 382 171 
\end{lstlisting}

We can see that the confidence interval covers
our true mean under the true optimal individualized treatment.

\textbf{NOTICE the distribution of the assigned treatment! We will need this shortly.}

\hypertarget{extensions-to-causal-effect-of-an-oit}{%
\section{Extensions to Causal Effect of an OIT}\label{extensions-to-causal-effect-of-an-oit}}

\begin{itemize}
\item
  We consider two extensions to the procedure described for
  estimating the value of the ITR.
\item
  The first one considers a setting where the user
  might be interested in a grid of possible suboptimal rules, corresponding to
  potentially limited knowledge of potential effect modifiers (\textbf{Simpler Rules}).
\item
  The second extension concerns implementation of realistic optimal individual
  interventions where certain regimes might be preferred, but due to practical or
  global positivity restraints are not realistic to implement (\textbf{Realistic Interventions}).
\end{itemize}

\hypertarget{simpler-rules}{%
\subsection{Simpler Rules}\label{simpler-rules}}

\begin{itemize}
\item
  In order to not only consider the most ambitious fully \(V\)-optimal rule, we
  define \(S\)-optimal rules as the optimal rule that considers all possible subsets
  of \(V\) covariates, with card(\(S\)) \(\leq\) card(\(V\)) and \(\emptyset \in S\).
\item
  This allows us to consider sub-optimal rules that are easier to estimate and
  potentially provide more realistic rules- as such, we allow for statistical
  inference for the counterfactual mean outcome under the sub-optimal rule.
\end{itemize}

\begin{lstlisting}[language=R]
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W4", "W3", "W2", "W1"), type = "blip2",
  learners = learner_list,
  maximize = TRUE, complex = FALSE, realistic = FALSE
)
\end{lstlisting}

\begin{lstlisting}[language=R]
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)
fit
A tmle3_Fit that took 1 step(s)
   type             param init_est tmle_est       se lower   upper
1:  TSM E[Y_{d(V=W2,W1)}]  0.53979  0.61659 0.068671 0.482 0.75118
   psi_transformed lower_transformed upper_transformed
1:         0.61659             0.482           0.75118
\end{lstlisting}

Even though the user specified all baseline covariates as the basis
for rule estimation, a simpler rule is sufficient to
maximize the mean under the optimal individualized treatment!

\textbf{QUESTION:} How does the set of covariates picked by \passthrough{\lstinline!tmle3mopttx!}
compare to the baseline covariates the true rule depends on?

\hypertarget{realistic-optimal-individual-regimes}{%
\subsection{Realistic Optimal Individual Regimes}\label{realistic-optimal-individual-regimes}}

\begin{itemize}
\item
  \passthrough{\lstinline!tmle3mopttx!} also provides an option to estimate the mean under the
  realistic, or implementable, optimal individualized treatment.
\item
  It is often the case that assigning particular regime might have the ability to
  fully maximize (or minimize) the desired outcome, but due to
  global or practical positivity constrains, such treatment
  can never be implemented in real life (or is highly unlikely).
\item
  Specifying \passthrough{\lstinline!realistic="TRUE"!}, we consider possibly suboptimal
  treatments that optimize the outcome in question while being
  supported by the data.
\end{itemize}

\begin{lstlisting}[language=R]
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("W4", "W3", "W2", "W1"), type = "blip2",
  learners = learner_list,
  maximize = TRUE, complex = TRUE, realistic = TRUE
)
\end{lstlisting}

\begin{lstlisting}[language=R]
# fit the TML estimator
fit <- tmle3(tmle_spec, data, node_list, learner_list)
fit
A tmle3_Fit that took 1 step(s)
   type         param init_est tmle_est       se   lower   upper
1:  TSM E[Y_{A=NULL}]  0.54688  0.66017 0.021458 0.61812 0.70223
   psi_transformed lower_transformed upper_transformed
1:         0.66017           0.61812           0.70223

# How many individuals got assigned each treatment?
table(tmle_spec$return_rule)

  2   3 
516 484 
\end{lstlisting}

\textbf{QUESTION:} Referring back to the data-generating distribution, why do you
think the distribution of allocated treatment changed from the distribution
that we had under the ``non-realistic'' rule?

\hypertarget{variable-importance-analysis}{%
\subsection{Variable Importance Analysis}\label{variable-importance-analysis}}

\begin{itemize}
\item
  In the previous sections we have seen how to obtain a contrast between the
  mean under the optimal individualized rule and the mean under the observed outcome for a
  single covariate. We are now ready to run the variable importance analysis for all of our
  observed covariates!
\item
  In order to run \passthrough{\lstinline!tmle3mopttx!} variable importance measure, we
  need considered covariates to be categorical variables.
\item
  For illustration purpose,
  we bin baseline covariates corresponding to the data-generating distribution
  described in the previous section:
\end{itemize}

\begin{lstlisting}[language=R]
# bin baseline covariates to 3 categories:
data$W1 <- ifelse(data$W1 < quantile(data$W1)[2], 1, ifelse(data$W1 < quantile(data$W1)[3], 2, 3))

node_list <- list(
  W = c("W3", "W4", "W2"),
  A = c("W1", "A"),
  Y = "Y"
)
\end{lstlisting}

\begin{itemize}
\tightlist
\item
  Note that our node list now includes \(W_1\) as treatments as well!
  Don't worry, we will still properly adjust for all baseline covariates when
  considering \(A\) as treatment.
\end{itemize}

\hypertarget{variable-importance-using-targeted-estimation-of-the-value-of-the-itr}{%
\subsubsection{Variable Importance using Targeted Estimation of the value of the ITR}\label{variable-importance-using-targeted-estimation-of-the-value-of-the-itr}}

\begin{itemize}
\tightlist
\item
  We will initialize a specification for the TMLE of our parameter of
  interest (called a \passthrough{\lstinline!tmle3\_Spec!} in the \passthrough{\lstinline!tlverse!} nomenclature) simply by calling
  \passthrough{\lstinline!tmle3\_mopttx\_vim!}.
\end{itemize}

\begin{lstlisting}[language=R]
# initialize a tmle specification
tmle_spec <- tmle3_mopttx_vim(
  V = c("W2"),
  type = "blip2",
  learners = learner_list,
  contrast = "multiplicative",
  maximize = FALSE,
  method = "SL",
  complex = TRUE,
  realistic = FALSE
)
\end{lstlisting}

\begin{lstlisting}[language=R]
# fit the TML estimator
vim_results <- tmle3_vim(tmle_spec, data, node_list, learner_list,
  adjust_for_other_A = TRUE
)

print(vim_results)
   type                  param   init_est tmle_est       se     lower    upper
1:   RR RR(E[Y_{A=NULL}]/E[Y])  0.0011065  0.14769 0.032279  0.084421 0.210952
2:   RR RR(E[Y_{A=NULL}]/E[Y]) -0.0227483 -0.01578 0.047608 -0.109090 0.077529
   psi_transformed lower_transformed upper_transformed  A           W   Z_stat
1:         1.15915           1.08809            1.2349  A W3,W4,W2,W1  4.57534
2:         0.98434           0.89665            1.0806 W1  W3,W4,W2,A -0.33146
         p_nz p_nz_corrected
1: 2.3772e-06     4.7544e-06
2: 3.7015e-01     3.7015e-01
\end{lstlisting}

The final result of \passthrough{\lstinline!tmle3\_vim!} with the \passthrough{\lstinline!tmle3mopttx!} spec is an ordered list
of mean outcomes under the optimal individualized treatment for all categorical
covariates in our dataset.

\hypertarget{exercise}{%
\section{Exercise}\label{exercise}}

\hypertarget{real-world-data-and-tmle3mopttx}{%
\subsection{\texorpdfstring{Real World Data and \texttt{tmle3mopttx}}{Real World Data and tmle3mopttx}}\label{real-world-data-and-tmle3mopttx}}

Finally, we cement everything we learned so far with a real data application.

As in the previous sections, we will be using the WASH Benefits data,
corresponding to the Effect of water quality, sanitation, hand washing, and
nutritional interventions on child development in rural Bangladesh trial.

The main aim of the cluster-randomized controlled trial was to assess the
impact of six intervention groups, including:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Control
\item
  Handwashing with soap
\item
  Improved nutrition through counselling and provision of lipid-based nutrient supplements
\item
  Combined water, sanitation, handwashing, and nutrition.
\item
  Improved sanitation
\item
  Combined water, sanitation, and handwashing
\item
  Chlorinated drinking water
\end{enumerate}

We aim to estimate the optimal ITR and the corresponding value under the optimal ITR
for the main intervention in WASH Benefits data!

Our outcome of interest is the weight-for-height Z-score, whereas our treatment is
the six intervention groups aimed at improving living conditions.

Questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Define \(V\) as mother's education (\passthrough{\lstinline!momedu!}), current living conditions (\passthrough{\lstinline!floor!}),
  and possession of material items including the refrigerator (\passthrough{\lstinline!asset\_refrig!}).
  Why do you think we use these covariates as \(V\)?
  Do we want to minimize or maximize the outcome? Which blip type should we use?
  Construct an appropriate \passthrough{\lstinline!sl3!} library for \(A\), \(Y\) and \(B\).
\item
  Based on the \(V\) defined in the previous question, estimate the mean under the ITR for
  the main randomized intervention used in the WASH Benefits trial
  with weight-for-height Z-score as the outcome. What's the TMLE value of the optimal ITR?
  How does it change from the initial estimate? Which intervention is the most dominant?
  Why do you think that is?
\item
  Using the same formulation as in questions 1 and 2, estimate the realistic optimal ITR
  and the corresponding value of the realistic ITR. Did the results change? Which intervention
  is the most dominant under realistic rules? Why do you think that is?
\end{enumerate}

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

\begin{itemize}
\item
  In summary, the mean outcome under the optimal individualized treatment is a counterfactual
  quantity of interest representing what the mean outcome would have been if everybody, contrary
  to the fact, received treatment that optimized their outcome.
\item
  \passthrough{\lstinline!tmle3mopttx!} estimates the mean outcome under the optimal individualized treatment, where the candidate
  rules are restricted to only respond to a user-supplied subset of the baseline and intermediate
  covariates. In addition it provides options for realistic, data-adaptive interventions.
\item
  In essence, our target parameter answers the key aim of precision medicine: allocating
  the available treatment by tailoring it to the individual characteristics of the patient, with the
  goal of optimizing the final outcome.
\end{itemize}

\hypertarget{solutions}{%
\subsection{Solutions}\label{solutions}}

To start, let's load the data, convert all columns to be of class \passthrough{\lstinline!numeric!},
and take a quick look at it:

\begin{lstlisting}[language=R]
washb_data <- fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data.csv", stringsAsFactors = TRUE)
washb_data <- washb_data[!is.na(momage), lapply(.SD, as.numeric)]
head(washb_data, 3)
\end{lstlisting}

As before, we specify the NPSEM via the \passthrough{\lstinline!node\_list!} object.

\begin{lstlisting}[language=R]
node_list <- list(
  W = names(washb_data)[!(names(washb_data) %in% c("whz", "tr"))],
  A = "tr", Y = "whz"
)
\end{lstlisting}

We pick few potential effect modifiers, including mother's education, current
living conditions (floor), and possession of material items including the refrigerator.
We concentrate of these covariates as they might be indicative of the socio-economic status
of individuals involved in the trial. We can explore the distribution of our \(V\), \(A\) and \(Y\):

\begin{lstlisting}[language=R]
# V1, V2 and V3:
table(washb_data$momedu)
table(washb_data$floor)
table(washb_data$asset_refrig)

# A:
table(washb_data$tr)

# Y:
summary(washb_data$whz)
\end{lstlisting}

We specify a simply library for the outcome regression, propensity score
and the blip function. Since our treatment is categorical, we need to define a
multinomial learner for \(A\) and multivariate learner for \(B\). We
will define the \passthrough{\lstinline!xgboost!} over a grid of parameters, and initialize a mean learner.

\begin{lstlisting}[language=R]
# Initialize few of the learners:
grid_params <- list(
  nrounds = c(100, 500),
  eta = c(0.01, 0.1)
)
grid <- expand.grid(grid_params, KEEP.OUT.ATTRS = FALSE)
xgb_learners <- apply(grid, MARGIN = 1, function(params_tune) {
  do.call(Lrnr_xgboost$new, c(as.list(params_tune)))
})
lrn_mean <- Lrnr_mean$new()

## Define the Q learner, which is just a regular learner:
Q_learner <- Lrnr_sl$new(
  learners = list(
    xgb_learners[[1]], xgb_learners[[2]], xgb_learners[[3]],
    xgb_learners[[4]], lrn_mean
  ),
  metalearner = Lrnr_nnls$new()
)

# Define the g learner, which is a multinomial learner:
# specify the appropriate loss of the multinomial learner:
mn_metalearner <- make_learner(Lrnr_solnp,
  loss_function = loss_loglik_multinomial,
  learner_function = metalearner_linear_multinomial
)
g_learner <- make_learner(Lrnr_sl, list(xgb_learners[[4]], lrn_mean), mn_metalearner)

# Define the Blip learner, which is a multivariate learner:
learners <- list(
  xgb_learners[[1]], xgb_learners[[2]], xgb_learners[[3]],
  xgb_learners[[4]], lrn_mean
)
b_learner <- create_mv_learners(learners = learners)

learner_list <- list(Y = Q_learner, A = g_learner, B = b_learner)
\end{lstlisting}

As seen before, we initialize the \passthrough{\lstinline!tmle3\_mopttx\_blip\_revere!} Spec in order to
answer Question 1. We want to maximize our outcome, with higher the weight-for-height Z-score
the better. We will also use \passthrough{\lstinline!blip2!} as the blip type, but note that we could have used \passthrough{\lstinline!blip1!}
as well since ``Control'' is a good reference category.

\begin{lstlisting}[language=R]
## Question 2:

# Initialize a tmle specification
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("momedu", "floor", "asset_refrig"), type = "blip2",
  learners = learner_list, maximize = TRUE, complex = TRUE,
  realistic = FALSE
)

# Fit the TML estimator.
fit <- tmle3(tmle_spec, data = washb_data, node_list, learner_list)
fit

# Which intervention is the most dominant?
table(tmle_spec$return_rule)
\end{lstlisting}

Using the same formulation as before, we estimate the realistic optimal ITR
and the corresponding value of the realistic ITR:

\begin{lstlisting}[language=R]
## Question 3:

# Initialize a tmle specification with "realistic=TRUE":
tmle_spec <- tmle3_mopttx_blip_revere(
  V = c("momedu", "floor", "asset_refrig"), type = "blip2",
  learners = learner_list, maximize = TRUE, complex = TRUE,
  realistic = TRUE
)

# Fit the TML estimator.
fit <- tmle3(tmle_spec, data = washb_data, node_list, learner_list)
fit

table(tmle_spec$return_rule)
\end{lstlisting}

\hypertarget{stochastic-treatment-regimes-optional}{%
\chapter{Stochastic Treatment Regimes (optional)}\label{stochastic-treatment-regimes-optional}}

\emph{Nima Hejazi}

Based on the \href{https://github.com/tlverse/tmle3shift}{\passthrough{\lstinline!tmle3shift!} \passthrough{\lstinline!R!} package}
by \emph{Nima Hejazi, Jeremy Coyle, and Mark van der Laan}.

Updated: 2021-10-13

\hypertarget{learning-objectives-4}{%
\section{Learning Objectives}\label{learning-objectives-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differentiate stochastic treatment regimes from static, dynamic, and optimal
  treatment regimes.
\item
  Describe how estimating causal effects of stochastic interventions informs a
  real-world data analysis.
\item
  Contrast a population level stochastic intervention policy from a modified
  treatment policy.
\item
  Estimate causal effects under stochastic treatment regimes with the
  \passthrough{\lstinline!tmle3shift!} \passthrough{\lstinline!R!} package.
\item
  Specify a grid of counterfactual shift interventions to be used for defining
  a set of stochastic intervention policies.
\item
  Interpret a set of effect estimates from a grid of counterfactual shift
  interventions.
\item
  Construct marginal structural models to measure variable importance in terms
  of stochastic interventions, using a grid of shift interventions.
\item
  Implement a shift intervention at the individual level, to facilitate
  shifting each individual to a value that's supported by the data.
\item
  Define novel shift intervention functions to extend the \passthrough{\lstinline!tmle3shift!} \passthrough{\lstinline!R!}
  package.
\end{enumerate}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

In this section, we examine a simple example of stochastic treatment regimes in
the context of a continuous treatment variable of interest, defining an
intuitive causal effect through which to examine stochastic interventions more
generally. As a first step to using stochastic
treatment regimes in practice, we present the \href{https://github.com/tlverse/tmle3shift}{\passthrough{\lstinline!tmle3shift!} R
package}, which features an
implementation of a recently developed algorithm for computing targeted minimum
loss-based estimates of a causal effect based on a stochastic treatment regime
that shifts the natural value of the treatment based on a shifting function
\(d(A,W)\). We will also use \passthrough{\lstinline!tmle3shift!} to construct marginal structural models
for variable importance measures, implement shift interventions at the
individual level, and define novel shift intervention functions.

\hypertarget{stochastic-interventions}{%
\section{Stochastic Interventions}\label{stochastic-interventions}}

\begin{itemize}
\item
  Present a relatively simple yet extremely flexible manner by which \emph{realistic}
  causal effects (and contrasts thereof) may be defined.
\item
  May be applied to nearly any manner of treatment variable -- continuous,
  ordinal, categorical, binary -- allowing for a rich set of causal effects to
  be defined through this formalism.
\item
  Arguably the most general of the classes of interventions through which causal
  effects may be defined, and are conceptually simple.
\item
  We may consider stochastic interventions in two ways:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \item
    The equation \(f_A\), which produces \(A\), is replaced by a probabilistic
    mechanism \(g_{\delta}(A \mid W)\) that differs from the original \(g(A \mid W)\). The \emph{stochastically modified} value of the treatment \(A_{\delta}\) is
    drawn from a user-specified distribution \(g_\delta(A \mid W)\), which may
    depend on the original distribution \(g(A \mid W)\) and is indexed by a
    user-specified parameter \(\delta\). In this case, the stochastically
    modified value of the treatment \(A_{\delta} \sim g_{\delta}(\cdot \mid W)\).
  \item
    The observed value \(A\) is replaced by a new value \(A_{d(A,W)}\) based on
    applying a user-defined function \(d(A,W)\) to \(A\). In this case, the
    stochastic treatment regime may be viewed as an intervention in which \(A\)
    is set equal to a value based on a hypothetical regime \(d(A, W)\), where
    regime \(d\) depends on the treatment level \(A\) that would be assigned in the
    absence of the regime as well as the covariates \(W\). Stochastic
    interventions of this variety may be referred to as depending on the
    \emph{natural value of treatment} or as \emph{modified treatment policies}
    \citep{haneuse2013estimation, young2014identification}.
  \end{enumerate}
\end{itemize}

\hypertarget{identifying-the-causal-effect-of-a-stochastic-intervention}{%
\subsection{Identifying the Causal Effect of a Stochastic Intervention}\label{identifying-the-causal-effect-of-a-stochastic-intervention}}

\begin{itemize}
\item
  The stochastic intervention generates a counterfactual random variable
  \(Y_{d(A,W)} := f_Y(d(A,W), W, U_Y) \equiv Y_{g_{\delta}} := f_Y(A_{\delta}, W, U_Y)\), where \(Y_{d(A,W)} \sim \mathcal{P}_0^{\delta}\).
\item
  The target causal estimand of our analysis is \(\psi_{0, \delta} := \mathbb{E}_{P_0^{\delta}}\{Y_{d(A,W)}\}\), the mean of the counterfactual
  outcome variable \(Y_{d(A, W)}\). The statistical target parameter may also be
  denoted \(\Psi(P_0) = \mathbb{E}_{P_0}{\overline{Q}(d(A, W), W)}\), where
  \(\overline{Q}(d(A, W), W)\) is the counterfactual outcome value of a given
  individual under the stochastic intervention distribution
  \citep{diaz2018stochastic}.
\item
  In prior work, \citet{diaz2012population} showed that the causal quantity of interest
  \(\mathbb{E}_0 \{Y_{d(A, W)}\}\) is identified by a functional of the
  distribution of \(O\):
  \begin{align*}\label{eqn:identification2012}
    \psi_{0,d} = \int_{\mathcal{W}} \int_{\mathcal{A}} & \mathbb{E}_{P_0}
     \{Y \mid A = d(a, w), W = w\} \cdot \\ &q_{0, A}^O(a \mid W = w) \cdot
     q_{0, W}^O(w) d\mu(a)d\nu(w).
  \end{align*}
\item
  The four standard assumptions presented in \ref{intro} are necessary in order
  to establish identifiability of the causal parameter from the observed data
  via the statistical functional. These were

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    \emph{Consistency}: \(Y^{d(a_i, w_i)}_i = Y_i\) in the event \(A_i = d(a_i, w_i)\),
    for \(i = 1, \ldots, n\)
  \item
    \emph{Stable unit value treatment assumption (SUTVA)}: \(Y^{d(a_i, w_i)}_i\) does
    not depend on \(d(a_j, w_j)\) for \(i = 1, \ldots, n\) and \(j \neq i\), or lack
    of interference \citep{rubin1978bayesian, rubin1980randomization}.
  \item
    \emph{Strong ignorability}: \(A_i \perp \!\!\! \perp Y^{d(a_i, w_i)}_i \mid W_i\),
    for \(i = 1, \ldots, n\).
  \item
    \emph{Positivity (or overlap)}: \(a_i \in \mathcal{A} \implies d(a_i, w_i) \in \mathcal{A}\) for all \(w \in \mathcal{W}\), where \(\mathcal{A}\) denotes the
    support of \(A \mid W = w_i \quad \forall i = 1, \ldots n\).
  \end{enumerate}
\item
  With the identification assumptions satisfied, \citet{diaz2012population} and
  \citet{diaz2018stochastic} provide an efficient influence function with respect to
  the nonparametric model \(\mathcal{M}\) as
  \begin{equation*}\label{eqn:eif}
    D(P_0)(x) = H(a, w)({y - \overline{Q}(a, w)}) +
    \overline{Q}(d(a, w), w) - \Psi(P_0),
  \end{equation*}
  where the auxiliary covariate \(H(a,w)\) may be expressed
  \begin{equation*}\label{eqn:aux_covar_full}
    H(a,w) = \mathbb{I}(a + \delta < u(w)) \frac{g_0(a - \delta \mid w)} {g_0(a \mid w)}
      + \mathbb{I}(a + \delta \geq u(w)),
  \end{equation*}
  which may be reduced to
  \begin{equation*}\label{eqn:aux_covar_simple}
    H(a,w) = \frac{g_0(a - \delta \mid w)}{g_0(a \mid w)} + 1
  \end{equation*}
  in the case that the treatment is in the limits that arise from conditioning
  on \(W\), i.e., for \(A_i \in (u(w) - \delta, u(w))\).
\end{itemize}

\hypertarget{estimating-the-causal-effect-of-a-stochastic-intervention-with-tmle3shift}{%
\section{\texorpdfstring{Estimating the Causal Effect of a Stochastic Intervention with \texttt{tmle3shift}}{Estimating the Causal Effect of a Stochastic Intervention with tmle3shift}}\label{estimating-the-causal-effect-of-a-stochastic-intervention-with-tmle3shift}}

We use \passthrough{\lstinline!tmle3shift!} to construct a targeted maximum likelihood (TML) estimator of
of a causal effect of a stochastic treatment regime that shifts the natural
value of the treatment based on a shifting function \(d(A,W)\). We will follow
the recipe provided by \citet{diaz2018stochastic}, tailored to the \passthrough{\lstinline!tmle3!} framework:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Construct initial estimators \(g_n\) of \(g_0(A, W)\) and \(Q_n\) of
  \(\overline{Q}_0(A, W)\), perhaps using data-adaptive regression techniques.
\item
  For each observation \(i\), compute an estimate \(H_n(a_i, w_i)\) of the
  auxiliary covariate \(H(a_i,w_i)\).
\item
  Estimate the parameter \(\epsilon\) in the logistic regression model
  \[ \text{logit}\overline{Q}_{\epsilon, n}(a, w) =
  \text{logit}\overline{Q}_n(a, w) + \epsilon H_n(a, w),\]
  or an alternative regression model incorporating weights.
\item
  Compute TML estimator \(\Psi_n\) of the target parameter, defining update
  \(\overline{Q}_n^{\star}\) of the initial estimate
  \(\overline{Q}_{n, \epsilon_n}\):
  \begin{equation*}\label{eqn:tmle}
    \Psi_n = \Psi(P_n^{\star}) = \frac{1}{n} \sum_{i = 1}^n
    \overline{Q}_n^{\star}(d(A_i, W_i), W_i).
  \end{equation*}
\end{enumerate}

To start, let's load the packages we'll use and set a seed for simulation:

\begin{lstlisting}[language=R]
library(tidyverse)
library(data.table)
library(sl3)
library(tmle3)
library(tmle3shift)
set.seed(429153)
\end{lstlisting}

\textbf{1. Construct initial estimators \(g_n\) of \(g_0(A, W)\) and \(Q_n\) of
\(\overline{Q}_0(A, W)\).}

We need to estimate two components of the likelihood in order to construct a
TML estimator.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The outcome regression, \(\hat{Q}_n\), which is a simple regression of the
  form \(\mathbb{E}[Y \mid A,W]\).
\end{enumerate}

\begin{lstlisting}[language=R]
# learners used for conditional expectation regression
mean_learner <- Lrnr_mean$new()
fglm_learner <- Lrnr_glm_fast$new()
xgb_learner <- Lrnr_xgboost$new(nrounds = 200)
sl_regression_learner <- Lrnr_sl$new(
  learners = list(mean_learner, fglm_learner, xgb_learner)
)
\end{lstlisting}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The second of these is an estimate of the treatment mechanism, \(\hat{g}_n\),
  i.e., the \emph{propensity score}. In the case of a continuous intervention node
  \(A\), such a quantity takes the form \(p(A \mid W)\), which is a conditional
  density. Generally speaking, conditional density estimation is a challenging
  problem that has received much attention in the literature. To estimate the
  treatment mechanism, we must make use of learning algorithms specifically
  suited to conditional density estimation; a list of such learners may be
  extracted from \passthrough{\lstinline!sl3!} by using \passthrough{\lstinline!sl3\_list\_learners()!}:
\end{enumerate}

\begin{lstlisting}[language=R]
sl3_list_learners("density")
[1] "Lrnr_density_discretize"     "Lrnr_density_hse"           
[3] "Lrnr_density_semiparametric" "Lrnr_haldensify"            
[5] "Lrnr_solnp_density"         
\end{lstlisting}

To proceed, we'll select two of the above learners, \passthrough{\lstinline!Lrnr\_haldensify!} for using
the highly adaptive lasso for conditional density estimation, based on an
algorithm given by \citet{diaz2011super} and implemented in \citet{hejazi2020haldensify}, and
semiparametric location-scale conditional density estimators implemented in the
\href{https://github.com/tlverse/sl3}{\passthrough{\lstinline!sl3!} package}. A Super Learner may be
constructed by pooling estimates from each of these modified conditional
density estimation techniques.

\begin{lstlisting}[language=R]
# learners used for conditional densities (i.e., generalized propensity score)
haldensify_learner <- Lrnr_haldensify$new(
  n_bins = c(3, 5),
  lambda_seq = exp(seq(-1, -10, length = 200))
)
# semiparametric density estimator based on homoscedastic errors (HOSE)
hose_learner_xgb <- make_learner(Lrnr_density_semiparametric,
  mean_learner = xgb_learner
)
# semiparametric density estimator based on heteroscedastic errors (HESE)
hese_learner_xgb_fglm <- make_learner(Lrnr_density_semiparametric,
  mean_learner = xgb_learner,
  var_learner = fglm_learner
)
# SL for the conditional treatment density
sl_density_learner <- Lrnr_sl$new(
  learners = list(haldensify_learner, hose_learner_xgb,
                  hese_learner_xgb_fglm),
  metalearner = Lrnr_solnp_density$new()
)
\end{lstlisting}

Finally, we construct a \passthrough{\lstinline!learner\_list!} object for use in constructing a TML
estimator of our target parameter of interest:

\begin{lstlisting}[language=R]
Q_learner <- sl_regression_learner
g_learner <- sl_density_learner
learner_list <- list(Y = Q_learner, A = g_learner)
\end{lstlisting}

\hypertarget{simulate-data-1}{%
\subsection{Simulate Data}\label{simulate-data-1}}

\begin{lstlisting}[language=R]
# simulate simple data for tmle-shift sketch
n_obs <- 1000 # number of observations
tx_mult <- 2 # multiplier for the effect of W = 1 on the treatment

## baseline covariates -- simple, binary
W <- replicate(2, rbinom(n_obs, 1, 0.5))

## create treatment based on baseline W
A <- rnorm(n_obs, mean = tx_mult * W, sd = 1)

## create outcome as a linear function of A, W + white noise
Y <- rbinom(n_obs, 1, prob = plogis(A + W))

# organize data and nodes for tmle3
data <- data.table(W, A, Y)
setnames(data, c("W1", "W2", "A", "Y"))
node_list <- list(W = c("W1", "W2"), A = "A", Y = "Y")
head(data)
   W1 W2        A Y
1:  1  1  3.58065 1
2:  1  0  3.20718 1
3:  1  1  1.03584 1
4:  0  0 -0.65785 1
5:  1  1  3.01990 1
6:  1  1  2.78031 1
\end{lstlisting}

We now have an observed data structure (\passthrough{\lstinline!data!}) and a specification of the role
that each variable in the data set plays as the nodes in a \emph{directed acyclic
graph} (DAG) via \emph{nonparametric structural equation models} (NPSEMs).

To start, we will initialize a specification for the TMLE of our parameter of
interest (a \passthrough{\lstinline!tmle3\_Spec!} in the \passthrough{\lstinline!tlverse!} nomenclature) simply by calling
\passthrough{\lstinline!tmle\_shift!}. We specify the argument \passthrough{\lstinline!shift\_val = 0.5!} when initializing the
\passthrough{\lstinline!tmle3\_Spec!} object to communicate that we're interested in a shift of \(0.5\) on
the scale of the treatment \(A\) -- that is, we specify \(\delta = 0.5\).

\begin{lstlisting}[language=R]
# initialize a tmle specification
tmle_spec <- tmle_shift(
  shift_val = 0.5,
  shift_fxn = shift_additive,
  shift_fxn_inv = shift_additive_inv
)
\end{lstlisting}

As seen above, the \passthrough{\lstinline!tmle\_shift!} specification object (like all \passthrough{\lstinline!tmle3\_Spec!}
objects) does \emph{not} store the data for our specific analysis of interest. Later,
we'll see that passing a data object directly to the \passthrough{\lstinline!tmle3!} wrapper function,
alongside the instantiated \passthrough{\lstinline!tmle\_spec!}, will serve to construct a \passthrough{\lstinline!tmle3\_Task!}
object internally (see the \passthrough{\lstinline!tmle3!} documentation for details).

\hypertarget{targeted-estimation-of-stochastic-interventions-effects}{%
\subsection{Targeted Estimation of Stochastic Interventions Effects}\label{targeted-estimation-of-stochastic-interventions-effects}}

\begin{lstlisting}[language=R]
tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)

Iter: 1 fn: 1342.0479    Pars:  0.56750948 0.00000282 0.43248770
Iter: 2 fn: 1342.0479    Pars:  0.5675095127 0.0000005883 0.4324898990
solnp--> Completed in 2 iterations
tmle_fit
A tmle3_Fit that took 1 step(s)
   type         param init_est tmle_est       se   lower   upper
1:  TSM E[Y_{A=NULL}]   0.8008  0.79801 0.012924 0.77268 0.82334
   psi_transformed lower_transformed upper_transformed
1:         0.79801           0.77268           0.82334
\end{lstlisting}

The \passthrough{\lstinline!print!} method of the resultant \passthrough{\lstinline!tmle\_fit!} object conveniently displays the
results from computing our TML estimator.

\hypertarget{stochastic-interventions-over-a-grid-of-counterfactual-shifts}{%
\section{Stochastic Interventions over a Grid of Counterfactual Shifts}\label{stochastic-interventions-over-a-grid-of-counterfactual-shifts}}

\begin{itemize}
\item
  Consider an arbitrary scalar \(\delta\) that defines a counterfactual outcome
  \(\psi_n = Q_n(d(A, W), W)\), where, for simplicity, let \(d(A, W) = A + \delta\).
  A simplified expression of the auxiliary covariate for the TMLE of \(\psi\) is
  \(H_n = \frac{g^{\star}(a \mid w)}{g(a \mid w)}\), where \(g^{\star}(a \mid w)\)
  defines the treatment mechanism with the stochastic intervention implemented.
  In this manner, we can specify a \emph{grid} of shifts \(\delta\) to define a set of
  stochastic intervention policies in an \emph{a priori} manner.
\item
  To ascertain whether a given choice of the shift \(\delta\) is acceptable, let
  there be a bound \(C(\delta) = \frac{g^{\star}(a \mid w)}{g(a \mid w)} \leq M\),
  where \(g^{\star}(a \mid w)\) is a function of \(\delta\) in part, and \(M\) is a
  user-specified upper bound of \(C(\delta)\). Then, \(C(\delta)\) is a measure of
  the influence of a given observation (under a bound of the ratio of the
  conditional densities), which provides a way to limit the maximum influence of
  a given observation through a choice of the shift \(\delta\).
\item
  For the purpose of using such a shift in practice, the present software
  provides the functions \passthrough{\lstinline!shift\_additive\_bounded!} and
  \passthrough{\lstinline!shift\_additive\_bounded\_inv!}, which define a variation of this shift:
  \begin{equation}
    \delta(a, w) =
      \begin{cases}
        \delta, & C(\delta) \leq M \\
        0, \text{otherwise} \\
      \end{cases},
  \end{equation}
  which corresponds to an intervention in which the natural value of treatment
  of a given observational unit is shifted by a value \(\delta\) in the case that
  the ratio of the intervened density \(g^{\star}(a \mid w)\) to the natural
  density \(g(a \mid w)\) (that is, \(C(\delta)\)) does not exceed a bound \(M\). In
  the case that the ratio \(C(\delta)\) exceeds the bound \(M\), the stochastic
  intervention policy does not apply to the given unit and they remain at their
  natural value of treatment \(a\).
\end{itemize}

\hypertarget{initializing-vimshift-through-its-tmle3_spec}{%
\subsection{\texorpdfstring{Initializing \texttt{vimshift} through its \texttt{tmle3\_Spec}}{Initializing vimshift through its tmle3\_Spec}}\label{initializing-vimshift-through-its-tmle3_spec}}

To start, we will initialize a specification for the TMLE of our parameter of
interest (called a \passthrough{\lstinline!tmle3\_Spec!} in the \passthrough{\lstinline!tlverse!} nomenclature) simply by calling
\passthrough{\lstinline!tmle\_shift!}. We specify the argument \passthrough{\lstinline!shift\_grid = seq(-1, 1, by = 1)!}
when initializing the \passthrough{\lstinline!tmle3\_Spec!} object to communicate that we're interested
in assessing the mean counterfactual outcome over a grid of shifts -1, 0, 1 on the scale of the treatment \(A\).

\begin{lstlisting}[language=R]
# what's the grid of shifts we wish to consider?
delta_grid <- seq(from = -1, to = 1, by = 1)

# initialize a tmle specification
tmle_spec <- tmle_vimshift_delta(
  shift_grid = delta_grid,
  max_shifted_ratio = 2
)
\end{lstlisting}

\hypertarget{targeted-estimation-of-stochastic-intervention-effects}{%
\subsection{Targeted Estimation of Stochastic Intervention Effects}\label{targeted-estimation-of-stochastic-intervention-effects}}

One may walk through the step-by-step procedure for fitting the TML estimator
of the mean counterfactual outcome under each shift in the grid, using the
machinery exposed by the \href{https://tlverse.org/tmle3}{\passthrough{\lstinline!tmle3!} R package}, or
simply invoke the \passthrough{\lstinline!tmle3!} wrapper function to fit the series of TML estimators
(one for each parameter defined by the grid delta) in a single function call.
For convenience, we choose the latter:

\begin{lstlisting}[language=R]
tmle_fit <- tmle3(tmle_spec, data, node_list, learner_list)

Iter: 1 fn: 1340.1739    Pars:  0.59196 0.12955 0.27849
Iter: 2 fn: 1340.1739    Pars:  0.59196 0.12956 0.27848
solnp--> Completed in 2 iterations
tmle_fit
A tmle3_Fit that took 1 step(s)
         type          param init_est tmle_est        se   lower   upper
1:        TSM  E[Y_{A=NULL}]  0.61243  0.61414 0.0140803 0.58655 0.64174
2:        TSM  E[Y_{A=NULL}]  0.74078  0.73900 0.0138950 0.71177 0.76623
3:        TSM  E[Y_{A=NULL}]  0.85109  0.84900 0.0112007 0.82705 0.87096
4: MSM_linear MSM(intercept)  0.73477  0.73405 0.0125574 0.70944 0.75866
5: MSM_linear     MSM(slope)  0.11933  0.11743 0.0043886 0.10883 0.12603
   psi_transformed lower_transformed upper_transformed
1:         0.61414           0.58655           0.64174
2:         0.73900           0.71177           0.76623
3:         0.84900           0.82705           0.87096
4:         0.73405           0.70944           0.75866
5:         0.11743           0.10883           0.12603
\end{lstlisting}

\emph{Remark}: The \passthrough{\lstinline!print!} method of the resultant \passthrough{\lstinline!tmle\_fit!} object conveniently
displays the results from computing our TML estimator.

\hypertarget{inference-with-marginal-structural-models}{%
\subsection{Inference with Marginal Structural Models}\label{inference-with-marginal-structural-models}}

Since we consider estimating the mean counterfactual outcome \(\psi_n\) under
several values of the intervention \(\delta\), taken from the aforementioned
\(\delta\)-grid, one approach for obtaining inference on a single summary measure
of these estimated quantities involves leveraging working marginal structural
models (MSMs). Summarizing the estimates \(\psi_n\) through a working MSM allows
for inference on the \emph{trend} imposed by a \(\delta\)-grid to be evaluated via a
simple hypothesis test on a parameter of this working MSM. Letting
\(\psi_{\delta}(P_0)\) be the mean outcome under a shift \(\delta\) of the
treatment, we have \(\vec{\psi}_{\delta} = (\psi_{\delta}: \delta)\) with
corresponding estimators \(\vec{\psi}_{n, \delta} = (\psi_{n, \delta}: \delta)\).
Further, let \(\beta(\vec{\psi}_{\delta}) = \phi((\psi_{\delta}: \delta))\). By a
straightforward application of the delta method (discussed previously), we may
write the efficient influence function of the MSM parameter \(\beta\) in terms of
the EIFs of each of the corresponding point estimates. Based on this, inference
from a working MSM is rather straightforward. To wit, the limiting distribution
for \(m_{\beta}(\delta)\) may be expressed \[\sqrt{n}(\beta_n - \beta_0) \to N(0,
\Sigma),\] where \(\Sigma\) is the empirical covariance matrix of
\(\text{EIF}_{\beta}(O)\).

\begin{lstlisting}[language=R]
tmle_fit$summary[4:5, ]
         type          param init_est tmle_est        se   lower   upper
1: MSM_linear MSM(intercept)  0.73477  0.73405 0.0125574 0.70944 0.75866
2: MSM_linear     MSM(slope)  0.11933  0.11743 0.0043886 0.10883 0.12603
   psi_transformed lower_transformed upper_transformed
1:         0.73405           0.70944           0.75866
2:         0.11743           0.10883           0.12603
\end{lstlisting}

\hypertarget{directly-targeting-the-msm-parameter-beta}{%
\subsection{\texorpdfstring{Directly Targeting the MSM Parameter \(\beta\)}{Directly Targeting the MSM Parameter \textbackslash beta}}\label{directly-targeting-the-msm-parameter-beta}}

Note that in the above, a working MSM is fit to the individual TML estimates of
the mean counterfactual outcome under a given value of the shift \(\delta\) in
the supplied grid. The parameter of interest \(\beta\) of the MSM is
asymptotically linear (and, in fact, a TML estimator) as a consequence of its
construction from individual TML estimators. In smaller samples, it may be
prudent to perform a TML estimation procedure that targets the parameter
\(\beta\) directly, as opposed to constructing it from several independently
targeted TML estimates. An approach for constructing such an estimator is
proposed in the sequel.

Suppose a simple working MSM \(\mathbb{E}Y_{g^0_{\delta}} = \beta_0 + \beta_1 \delta\), then a TML estimator targeting \(\beta_0\) and \(\beta_1\) may be
constructed as
\[\overline{Q}_{n, \epsilon}(A,W) = \overline{Q}_n(A,W) + \epsilon (H_1(g),
H_2(g),\] for all \(\delta\), where \(H_1(g)\) is the auxiliary covariate for
\(\beta_0\) and \(H_2(g)\) is the auxiliary covariate for \(\beta_1\).

To construct a targeted maximum likelihood estimator that directly targets the
parameters of the working marginal structural model, we may use the
\passthrough{\lstinline!tmle\_vimshift\_msm!} Spec (instead of the \passthrough{\lstinline!tmle\_vimshift\_delta!} Spec that
appears above):

\begin{lstlisting}[language=R]
# initialize a tmle specification
tmle_msm_spec <- tmle_vimshift_msm(
  shift_grid = delta_grid,
  max_shifted_ratio = 2
)

# fit the TML estimator and examine the results
tmle_msm_fit <- tmle3(tmle_msm_spec, data, node_list, learner_list)

Iter: 1 fn: 1338.2186    Pars:  0.621115419 0.000009161 0.378875424
Iter: 2 fn: 1338.2186    Pars:  0.6211155417 0.0000003725 0.3788840858
solnp--> Completed in 2 iterations
tmle_msm_fit
A tmle3_Fit that took 100 step(s)
         type          param init_est tmle_est        se   lower   upper
1: MSM_linear MSM(intercept)  0.73511  0.73540 0.0125979 0.71071 0.76009
2: MSM_linear     MSM(slope)  0.11894  0.11876 0.0042977 0.11033 0.12718
   psi_transformed lower_transformed upper_transformed
1:         0.73540           0.71071           0.76009
2:         0.11876           0.11033           0.12718
\end{lstlisting}

\hypertarget{example-with-the-wash-benefits-data}{%
\subsection{Example with the WASH Benefits Data}\label{example-with-the-wash-benefits-data}}

To complete our walk through, let's turn to using stochastic interventions to
investigate the data from the WASH Benefits trial. To start, let's load the
data, convert all columns to be of class \passthrough{\lstinline!numeric!}, and take a quick look at it

\begin{lstlisting}[language=R]
washb_data <- fread("https://raw.githubusercontent.com/tlverse/tlverse-data/master/wash-benefits/washb_data_subset.csv", stringsAsFactors = TRUE)
washb_data <- washb_data[!is.na(momage) & !is.na(momheight), ]
head(washb_data, 3)
     whz          tr fracode month aged    sex momage         momedu momheight
1: -0.94 Handwashing  N06505     7  237   male     21 Primary (1-5y)    146.00
2: -1.13     Control  N06505     8  310 female     26   No education    148.90
3: -1.61     Control  N06524     3  162   male     25 Primary (1-5y)    153.75
       hfiacat Nlt18 Ncomp watmin elec floor walls roof asset_wardrobe
1: Food Secure     1    25      2    1     0     1    1              0
2: Food Secure     1     7      4    1     0     0    1              0
3: Food Secure     0    15      2    0     0     1    1              0
   asset_table asset_chair asset_khat asset_chouki asset_tv asset_refrig
1:           1           0          0            1        0            0
2:           1           1          0            1        0            0
3:           1           0          1            1        0            0
   asset_bike asset_moto asset_sewmach asset_mobile
1:          0          0             0            0
2:          0          0             0            1
3:          0          0             0            0
\end{lstlisting}

Next, we specify our NPSEM via the \passthrough{\lstinline!node\_list!} object. For our example analysis,
we'll consider the outcome to be the weight-for-height Z-score (as in previous
sections), the intervention of interest to be the mother's age at time of
child's birth, and take all other covariates to be potential confounders.

\begin{lstlisting}[language=R]
node_list <- list(
  W = names(washb_data)[!(names(washb_data) %in%
    c("whz", "momage"))],
  A = "momage", Y = "whz"
)
\end{lstlisting}

Were we to consider the counterfactual weight-for-height Z-score under shifts in
the age of the mother at child's birth, how would we interpret estimates of our
parameter?

To simplify our interpretation, consider a shift (up or down) of two years in
the mother's age (i.e., \(\delta = \{-2, 0, 2\}\)); in this setting, a stochastic
intervention would correspond to a policy advocating that potential mothers
defer or accelerate plans of having a child for two calendar years, possibly
implemented through the deployment of an encouragement design.

First, let's try a simple upward shift of just two years:

\begin{lstlisting}[language=R]
# initialize a tmle specification for just a single delta shift
washb_shift_spec <- tmle_shift(
  shift_val = 2,
  shift_fxn = shift_additive,
  shift_fxn_inv = shift_additive_inv
)
\end{lstlisting}

To examine the effect modification approach we looked at in previous chapters,
we'll estimate the effect of this shift \(\delta = 2\) while stratifying on the
mother's education level (\passthrough{\lstinline!momedu!}, a categorical variable with three levels).
For this, we augment our initialized \passthrough{\lstinline!tmle3\_Spec!} object like so

\begin{lstlisting}[language=R]
# initialize effect modification specification around previous specification
washb_shift_strat_spec <-  tmle_stratified(washb_shift_spec)
\end{lstlisting}

Prior to running our analysis, we'll modify the \passthrough{\lstinline!learner\_list!} object we had
created to include just one of the semiparametric location-scale conditional
density estimators, as fitting of these estimators is much faster than the more
computationally intensive approach implemented in the
\href{ihttps://CRAN.R-project.org/package=haldensify}{\passthrough{\lstinline!haldensify!} package}
\citep{hejazi2020haldensify}.

\begin{lstlisting}[language=R]
# learners used for conditional density regression (i.e., propensity score),
# but we need to turn on cross-validation for this conditional density learner
hose_learner_xgb_cv <- Lrnr_cv$new(
  learner = hose_learner_xgb,
  full_fit = TRUE
)

# modify learner list, using existing SL for Q fit
learner_list <- list(Y = Q_learner, A = hose_learner_xgb_cv)
\end{lstlisting}

Now we're ready to construct a TML estimate of the shift parameter at
\(\delta = 2\), stratified across levels of our variable of interest:

\begin{lstlisting}[language=R]
# fit stratified TMLE
strat_node_list <- copy(node_list)
strat_node_list$W <- setdiff(strat_node_list$W,"momedu")
strat_node_list$V <- "momedu"
washb_shift_strat_fit <- tmle3(washb_shift_strat_spec, washb_data, strat_node_list,
                               learner_list)
washb_shift_strat_fit
A tmle3_Fit that took 1 step(s)
             type                             param init_est tmle_est       se
1:            TSM                     E[Y_{A=NULL}] -0.57056 -0.59444 0.058920
2: stratified TSM  E[Y_{A=NULL}] | V=Primary (1-5y) -0.60763 -0.70816 0.075504
3: stratified TSM    E[Y_{A=NULL}] | V=No education -0.65272 -0.85506 0.125655
4: stratified TSM E[Y_{A=NULL}] | V=Secondary (>5y) -0.52368 -0.44815 0.094647
      lower    upper psi_transformed lower_transformed upper_transformed
1: -0.70992 -0.47896        -0.59444          -0.70992          -0.47896
2: -0.85614 -0.56017        -0.70816          -0.85614          -0.56017
3: -1.10134 -0.60878        -0.85506          -1.10134          -0.60878
4: -0.63365 -0.26264        -0.44815          -0.63365          -0.26264
\end{lstlisting}

For the next example, we'll use the variable importance strategy of considering
a grid of stochastic interventions to evaluate the weight-for-height Z-score
under a shift in the mother's age down by two years (\(\delta = -2\)) through up
by two years (\(\delta = 2\)), incrementing by a single year between the two. To
do this, we simply initialize a \passthrough{\lstinline!Spec!} \passthrough{\lstinline!tmle\_vimshift\_delta!} similar to how we
did in a previous example:

\begin{lstlisting}[language=R]
# initialize a tmle specification for the variable importance parameter
washb_vim_spec <- tmle_vimshift_delta(
  shift_grid = seq(from = -2, to = 2, by = 1),
  max_shifted_ratio = 2
)
\end{lstlisting}

Having made the above preparations, we're now ready to estimate the
counterfactual mean of the weight-for-height Z-score under a small grid of
shifts in the mother's age at child's birth. Just as before, we do this through
a simple call to our \passthrough{\lstinline!tmle3!} wrapper function:

\begin{lstlisting}[language=R]
washb_tmle_fit <- tmle3(washb_vim_spec, washb_data, node_list, learner_list)
washb_tmle_fit
A tmle3_Fit that took 1 step(s)
         type          param init_est   tmle_est        se      lower
1:        TSM  E[Y_{A=NULL}] -0.56197 -0.5606288 0.0459486 -0.6506865
2:        TSM  E[Y_{A=NULL}] -0.56388 -0.5604088 0.0464538 -0.6514565
3:        TSM  E[Y_{A=NULL}] -0.56579 -0.5652941 0.0466314 -0.6566901
4:        TSM  E[Y_{A=NULL}] -0.56770 -0.5688357 0.0468849 -0.6607283
5:        TSM  E[Y_{A=NULL}] -0.56961 -0.5653488 0.0479085 -0.6592478
6: MSM_linear MSM(intercept) -0.56579 -0.5641032 0.0466524 -0.6555403
7: MSM_linear     MSM(slope) -0.00191 -0.0017867 0.0015524 -0.0048293
        upper psi_transformed lower_transformed upper_transformed
1: -0.4705711      -0.5606288        -0.6506865        -0.4705711
2: -0.4693611      -0.5604088        -0.6514565        -0.4693611
3: -0.4738982      -0.5652941        -0.6566901        -0.4738982
4: -0.4769430      -0.5688357        -0.6607283        -0.4769430
5: -0.4714498      -0.5653488        -0.6592478        -0.4714498
6: -0.4726662      -0.5641032        -0.6555403        -0.4726662
7:  0.0012559      -0.0017867        -0.0048293         0.0012559
\end{lstlisting}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Set the \passthrough{\lstinline!sl3!} library of algorithms for the Super Learner to a simple,
  interpretable library and use this new library to estimate the counterfactual
  mean of mother's age at child's birth (\passthrough{\lstinline!momage!}) under a shift \(\delta = 0\).
  What does this counterfactual mean equate to in terms of the observed data?
\item
  Describe two (equivalent) ways in which the causal effects of stochastic
  interventions may be interpreted.
\item
  Using a grid of values of the shift parameter \(\delta\) (e.g., \(\{-1, 0, +1\}\)), repeat the analysis on the variable of interest (\passthrough{\lstinline!momage!}),
  summarizing the trend for this sequence of shifts using a marginal structural
  model.
\item
  For either the grid of shifts in the example preceding the exercises or that
  estimated in (3) above, plot the resultant estimates against their respective
  counterfactual shifts. Graphically add to the scatterplot a line with slope
  and intercept equivalent to the MSM fit through the individual TML estimates.
\item
  How does the marginal structural model we used to summarize the trend along
  the sequence of shifts previously help to contextualize the estimated effect
  for a single shift? That is, how does access to estimates across several
  shifts and the marginal structural model parameters allow us to more richly
  interpret our findings?
\end{enumerate}

\hypertarget{r6}{%
\chapter{\texorpdfstring{A Primer on the \texttt{R6} Class System}{A Primer on the R6 Class System}}\label{r6}}

A central goal of the Targeted Learning statistical paradigm is to estimate
scientifically relevant parameters in realistic (usually nonparametric) models.

The \passthrough{\lstinline!tlverse!} is designed using basic OOP principles and the \passthrough{\lstinline!R6!} OOP framework.
While we've tried to make it easy to use the \passthrough{\lstinline!tlverse!} packages without worrying
much about OOP, it is helpful to have some intuition about how the \passthrough{\lstinline!tlverse!} is
structured. Here, we briefly outline some key concepts from OOP. Readers
familiar with OOP basics are invited to skip this section.

\hypertarget{classes-fields-and-methods}{%
\section{Classes, Fields, and Methods}\label{classes-fields-and-methods}}

The key concept of OOP is that of an object, a collection of data and functions
that corresponds to some conceptual unit. Objects have two main types of
elements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{fields}, which can be thought of as nouns, are information about an object,
  and
\item
  \emph{methods}, which can be thought of as verbs, are actions an object can
  perform.
\end{enumerate}

Objects are members of classes, which define what those specific fields and
methods are. Classes can inherit elements from other classes (sometimes called
base classes) -- accordingly, classes that are similar, but not exactly the
same, can share some parts of their definitions.

Many different implementations of OOP exist, with variations in how these
concepts are implemented and used. R has several different implementations,
including \passthrough{\lstinline!S3!}, \passthrough{\lstinline!S4!}, reference classes, and \passthrough{\lstinline!R6!}. The \passthrough{\lstinline!tlverse!} uses the \passthrough{\lstinline!R6!}
implementation. In \passthrough{\lstinline!R6!}, methods and fields of a class object are accessed using
the \passthrough{\lstinline!$!} operator. For a more thorough introduction to \passthrough{\lstinline!R6!}, see \url{https://adv-r.hadley.nz/r6.html}, from Hadley Wickham's \emph{Advanced
R} \citep{wickham2014advanced}.

\hypertarget{object-oriented-programming-python-and-r}{%
\section{\texorpdfstring{Object Oriented Programming: \texttt{Python} and \texttt{R}}{Object Oriented Programming: Python and R}}\label{object-oriented-programming-python-and-r}}

OO concepts (classes with inherentence) were baked into Python from the first
published version (version 0.9 in 1991). In contrast, \passthrough{\lstinline!R!} gets its OO ``approach''
from its predecessor, \passthrough{\lstinline!S!}, first released in 1976. For the first 15 years, \passthrough{\lstinline!S!}
had no support for classes, then, suddenly, \passthrough{\lstinline!S!} got two OO frameworks bolted on
in rapid succession: informal classes with \passthrough{\lstinline!S3!} in 1991, and formal classes with
\passthrough{\lstinline!S4!} in 1998. This process continues, with new OO frameworks being periodically
released, to try to improve the lackluster OO support in \passthrough{\lstinline!R!}, with reference
classes (\passthrough{\lstinline!R5!}, 2010) and \passthrough{\lstinline!R6!} (2014). Of these, \passthrough{\lstinline!R6!} behaves most like Python
classes (and also most like OOP focused languages like C++ and Java), including
having method definitions be part of class definitions, and allowing objects to
be modified by reference.

  \bibliography{book.bib,packages.bib}

\backmatter
\printindex

\end{document}
